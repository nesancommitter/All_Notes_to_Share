Book Snowflake Definitive Guide
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Chapter 1. Getting Started

Cloud doesn’t require users to be directly or actively involved in the management of those computer system resources. Other benefits include access to unlimited storage capacity, automatic software updates, instant scalability, high speed, and cost reductions.

AWS Redshift, Google BigQuery, and Microsoft Azure DataWarehouse are Cloud Databases.

In addition tothe traditional data engineering and data warehouse workloads, Snowflake supports data lake, data collaboration, data analytics, data applications, data science, cyber-security, and Unistore workloads. Snowflake’s “Many DataWorkloads, One Platform” approach gives organizations away to quickly derive value from rapidly growing data sets in secure and governed ways that enable companies to meet compliance requirements.

It created more opportunities to democratize data analytics by allowing users at all levels within an organization to make data-driven decisions. Snowflake offers near-zero management capability to eliminate much of the administrative and management overhead associated with traditional data warehouses. Snowflake Secure Data Sharing enables virtually instantly secure governed data to be shared across your business ecosystem. It also opens up many possibilities for monetizing data assets.

Snowflake introduced the ability to manage multiple accounts with ease using Snowflake Organizations. Managing multiple accounts makes it possible to separately maintain different environments, such as development and production environments, and to adopt a multicloud strategy. It also means you can better manage costs since you can select which features you need for each separate account.

Snowpark is a developer framework that brings new data programmability to the cloud and makes it possible for developers, data scientists, and data engineers to use Java, Scala, or Python to deploy code in aserverless manner.

The Security Data Lake is an innovative workload that empowers cyber security and compliance teams to gain full visibility into security logs, at a massive scale, while reducing the costs of security information and event management (SIEM) systems. 

Two different Snowflake web user interfaces are available:the Classic Console and Snowsight. Snowsight was first introduced in 2021 and is now the default user interface in newly created Snowflake accounts. Snowsight

After four hours ofinactivity, the current session is terminated and you mustlog in again. The default session timeout policy of fourhours can be changed;
the minimum configurable idletimeout value for a session policy is five minutes.

Context Setting in snowsight worksheet is to setup the role, warehouse, Database and schema. Worksheets can be managed with in multiple folders.

SELECT CURRENT_ROLE();
SELECT CURRENT_WAREHOUSE();

Ctrl + Enter is ashortcut for the Run button in snowsight worksheets.

For object names, we’ll be using all uppercase letters. We could achieve the same results by using all lowercase letters or mixed case, because Snowflake converts objectnames to uppercase. If you want to have an object name with mixed-case or all lowercase letters, for example, you’ll need to enclose your object name in quotes.

different table types, including temporary tables and permanent tables. You’ll discover that temporary tables are session based;
thus, they are notbound by uniqueness requirements. Even though you can create a temporary table with the same name as another table, it is not a good practice to do so.

Snowflake controls access through roles, and specific rolesare responsible for creating certain types of objects. For example, the SYSADMIN role is used to create databasesand virtual warehouses.

Chapter 2. Creating and Managing the Snowflake Architecture

the three different Snowflake architecture layers are:the cloud services layer, query processing (virtual warehouse) compute layer, and centralized (hybridcolumnar) database storage layer.

four types of NoSQL databases: document stores, key-value(KV) stores, column family data stores, and graph databases.

A major limitation of NoSQL stores is that they perform poorly when do-ing calculations involving many records, such as aggregations, windowfunctions, and arbitrary ordering. Thus, NoSQL stores can be great whenyou need to quickly create, read, update, and delete (CRUD) individualentries in a table, but they aren’t recommended for ad hoc analysis.

The Snowflake Architecture

it allows multiple users to concurrently share live data. it addresses modern data problems and solve the long-standing scalability issue. The unique Snowflake design physically separates but logically inte-grates storage and compute along with providing services such as secu-rity and management. 

The Snowflake hybrid-model architecture is composed of three layers:the cloud services layer, the computelayer, and the data storage layer.

The Cloud Services Layer

All interactions with data in a Snowflake instance begin in the cloud services layer, also called the global services layer The Snowflake cloud services layer is a collection of services that coordinate activities such as authentication, access control, and encryption. It also includes management functions for handling infrastructure andmetadata, as well as performing query parsing and optimization, amongother features. The cloud services layer is sometimes referred to as the Snowflake brain because all the various service layer components work together to handle user requests that begin from the time a user requests to log in.

When a user submits a Snowflake query, the SQL querywill be sent to the cloud services layer optimizer before being sent to the compute layer for processing.




The cloud services layer is what enables the SQL client interface for Data Definition Language (DDL) and DataManipulation Language (DML) operations on data.

The cloud services layer manages data security, including the security for data sharing. The Snowflake cloud services layer runs across multiple availability zones in each cloud provider region and holds the result cache, a cached copy of the executed query results. The metadata required for query optimization or data filtering are also stored in the cloud services layer.

Just like the other Snowflake layers, the cloud services layer will scale independently of the other layers. Scaling of the cloud services layer is an automated process that doesn’t need to be directly manipulated by the Snowflake end user.

Billing for the Cloud Services Layer

Most cloud services consumption is already incorporated into Snowflakepricing. However, when customers occasionally exceed 10% of theirdaily compute credit usage, they are billed for the overage. Note that daily compute credit usage is calculated in the UTC time zone.

All queries use a small number of cloud services resources. DDL opera-tions are metadata operations, and as such, they use only cloud services

Increased usage of the cloud services layer will likely occur when usingseveral simple queries, especially queries accessing session informationor using session variables. Increased usage also occurs when using large,complex queries with many joins. Single row inserts, rather than bulk orbatch loading, will also result in higher cloud services consumption.

you’ll consume only cloud services resources if you use com-mands on the INFORMATION_SCHEMA tables or certain metadata-only commands such as the SHOW command.

Even though the cloud services cost for a particular use case is high, sometimes it makes sense economically and/or strategically to incur the cost. For example, taking advantage of the result cache for queries, especially for large or complex queries, will mean zero compute cost for that query.

The Query Processing (VirtualWarehouse) Compute Layer

A Snowflake compute cluster, most often referred to simply as a virtual warehouse, is a dynamic cluster of compute resources consisting of CPU memory and temporary storage. Creating virtual warehouses in Snowflake makes use of the compute clusters virtual machines in the cloud which are provisioned behind the scenes. The Snowflake compute resources are created and deployed on demand to the Snowflake user, to whom the process is transparent.

A running virtual warehouse is required for most SQL queries and all DML operations, including loading and unloading data into tables, aswell as updating rows in tables. Any virtual warehouse can access the same data as another, without any contention or impact on performance of the other warehouses. This is because each Snowflake virtual warehouse operates independently and does not share compute resources with other virtual warehouses.

A virtual warehouse is always consuming credits when it is running in asession. However, Snowflake virtual warehouses can be started and stopped at any time, and can be resized at any time, even while running.  Snowflake supports two different ways to scale warehouses. Virtualwarehouses can be scaled up by resizing a warehouse and can be scaled out by adding clusters to a warehouse. 

Snowflake predetermines the CPU, memory, and solid-state drive(SSD) configurations for each node in a virtual warehouse While these definitions are subject to change, they are consistent in configuration across all three cloud providers.

Virtual Warehouse Size

A Snowflake compute cluster is defined by its size, with size corresponding to the number of servers in the virtual warehouse cluster. Beyond 4X-Large, a different approach is used to determine the number of servers per cluster. However, the credits per hour do still increase by a factor of 2 for these extremely large virtual warehouses.

xs  s  m  l  xl  xxl  3xl  4xl 
1   2  4  8  16   32   64  128 

Because Snowflake utilizes per-second billing, it can often be cost-effective to runlarger virtual warehouses because you are able to suspend virtual warehouses when they aren’t being used.

Scaling Up a Virtual Warehouse to Process LargeData Volumes and Complex Queries

Many factors affect the performance of a virtual warehouse. The number of concurrent queries, the number of tables being queried, and the size and composition of the data are a few things that should be considered whensizing a Snowflake virtual warehouse.
Sizing appropriately matters. A lack of resources, due to the virtual warehouse being too small, could result in taking too long to complete the query. There could be a negative cost impact if the query is too small and the virtual warehouse is too large. Resizing a Snowflake virtual warehouse is a manual process and can be done even while queries are running because a virtual warehouse doesnot have to be stopped or suspended to be resized. However, when a Snowflake virtual warehouse is resized, only subsequent queries will make use of the new size. Any queries already running will finish running while any queued queries will run on the newly sized virtual warehouse.

It is recommended that you experiment with different types of queries and dif-ferent virtual warehouse sizes to determine the best way to manage your virtualwarehouses effectively and efficiently. The queries should be of a certain sizeand complexity that you would typically expect to complete within no more than5 to 10 minutes. Additionally, it is recommended that you start small and in-crease in size as you experiment. It is easier to identify an undersized virtualwarehouse than an underutilized one.

A multicluster virtual warehouse allows Snowflake to scale in and out automatically.

Scaling a virtual warehouse up or down can be achieved in the user in-terface or by using SQL statements in the worksheet. Auto Suspend and Auto Resume are enabled by default. With Auto Suspend enabled, Snowflake will automatically suspend the virtual warehouse if it is inactive for a specified period of time. The AutoSuspend functionality ensures that you don’t leave a virtual warehouse running when there are no incoming queries. 

The value of the Auto Resume and Auto Suspend times should equal or exceedany regular gaps in your query workload. For example, if you have regular gaps of four minutes between queries, it wouldn’t be advisable to set Auto Suspendfor less than four minutes. If you did, your virtual warehouse would be continually suspending and resuming, which could potentially result in higher costs since the minimum credit usage billed is 60 seconds.

Larger virtual warehouses do not necessarily result in better performance forquery processing or data loading. It is the query complexity, as part of query processing, that should be a consideration for choosing a virtual warehouse size because the time it takes for a server to execute a complex query will likely be greater than the time it takes to run a simple query. Specifically, the number of files being loaded and the size of each file are important factors for data loading performance. One exception to that general rule would be if you are bulk-loading hundreds or thousands of files concurrently.

Scaling Out with Multicluster Virtual Warehousesto Maximize Concurrency

The goal is to optimize the Snowflakesystem performance in terms of size and number of clusters. If a concurrency problem is due to many users, or connections, scalingup will not adequately address the problem. Instead, we’ll need to scaleout by adding clusters going from a Min Clusters value of 1 to a Max Clusters value of 3, for example. Multicluster virtual warehouses can be set to automatically scale if the number of users and/or queries tends to fluctuate.

Multicluster virtual warehouses are available on the Enterprise, BusinessCritical, and Virtual Private Snowflake editions.

Just like single-cluster virtual warehouses, multicluster virtual warehouses can be created through the web interface or by using SQL forSnowflake instances. Unlike single-cluster virtual warehouses where sizing is a manual process, scaling in or out for multicluster virtual warehouses is an automated process.

The two modes that can be selected for a multicluster virtual warehouseare Auto-scale and Maximized. The Snowflake scaling policy, designed to help control the usage credits in the Auto-scale mode, can be set to Standard or Economy.

Whenever a multicluster virtual warehouse is configured with the scaling policy set to Standard, the first virtual warehouse immediately starts when a query is queued, or if the Snowflake system detects that there is one more query than the currently running clusters can execute. Each successive virtual warehouse starts 20 seconds after the prior virtual warehouse has started.

If a multicluster virtual warehouse is configured with the scaling policy set to Economy, a virtual warehouse starts only if the Snowflake system estimates the query load can keep the virtual warehouse busy for atleast six minutes. The goal of the Economy scaling policy is to conserve credits by keeping virtual warehouses fully loaded. As a result, queries may end up being queued and could take longer to complete.

For example, if you set the Max Clusters at 10, keep in mind that you could experience a tenfold compute cost for the length of time all 10 clusters are running. A multi-cluster virtual warehouse is Maximized when the Min Clusters value is greater than 1 and both the Min Clusters and Max Clusters values are equal.

Compute can be scaled up, down, in, or out. In all cases, there is no effect on storage used.

Auto Suspend is the number of seconds that the virtual warehouse will wait if no queries need to be executed before going offline. Auto Resume will restart the virtual warehouse once there is an operation that requires compute resources. 

when you create a virtual warehouse using SQL, you’ll need to state the suspend time in total seconds. In the user interface, the time is entered in minutes.

USE ROLE SYSADMIN;

CREATE WAREHOUSE CH2_WH WITH WAREHOUSE_SIZE=MEDIUM 
AUTO_SUSPEND=300 AUTO_RESUME=true INITIALLY_SUSPENDED=true;

It is a best practice to create a new virtual warehouse in a suspended state. Unless the Snowflake virtual warehouse is created initially in a suspended state,the initial creation of a Snowflake virtual warehouse could take time to provision compute resources.

USE ROLE SYSADMIN;
ALTER WAREHOUSE CH2_WH SET WAREHOUSE_SIZE=LARGE;

USE WAREHOUSE CH2_WH;

If you are working in a Snowflake Standard Edition org, you won’t see the multicluster virtual warehouse option. For the Enterprise Edition, Business Critical Edition, and Virtual Private Snowflake Edition, multi-cluster virtual warehouses are enabled.

To scale out, the number of clusters needs to be two or greater. When you increase the number of clusters to two or more, be sure to evaluate the mode. You’ll most likely want to change the mode to Auto-scale from the default Maximized mode.

To create a Snowflake multicluster virtual warehouse via SQL, you’ll need to specify the scaling policy as well as the minimum and maximum number of clusters.

A multicluster virtual warehouse is said to be Maximized when the minimum number of clusters and maximum number of clusters are the same. Additionally,value(s) must be more than one. An example of a Maximized multicluster virtual warehouse is when MIN_CLUSTER_COUNT=3 and MAX_CLUSTER_COUNT=3.

Separation of Workloads and Workload Management

Snowflake estimates the resources needed for each query, and as the workload approaches 100%, each new query is suspended in a queue until there are sufficient resources to execute them. Several options exist to efficiently handle workloads of varying size. One way is to separate the workloads by assigning different virtual warehouses to different users or groups of users. Different groups of users can be assigned to different Snowflake virtualwarehouses of varying size.

For anautomatically scaling multicluster virtual warehouse, we will still needto define the virtual warehouse size and the minimum and maximumnumber of clusters. 

USE ROLE SYSADMIN;
CREATE OR REPLACE WAREHOUSE ACCOUNTING_WH WITH Warehouse_Size=MEDIUM 
MIN_CLUSTER_COUNT=1 MAX_CLUSTER_COUNT=6 SCALING_POLICY='STANDARD';

Billing for the Virtual Warehouse Layer

Consumption charges for Snowflake virtual warehouses are calculated based on the warehouse size, as determined by the number of servers per cluster, the number of clusters if there are multicluster virtual ware-houses, and the amount of time each cluster server runs. Snowflake utilizes per-second billing with a 60-second minimum each time a virtual warehouse starts or is resized. 

When a virtual warehouse is scaled up,credits are billed for one minute of the additional resources that are provisioned. All billing, even though calculated in seconds, is reported infractions of hours. When using the ACCOUNTADMIN role, you can view the virtual warehouse credit usage for your account by clicking Admin -> Cost management in the UI. You can also query the Account Usage view in the SNOWFLAKEshared database to obtain the information.

Centralized (Hybrid Columnar)Database Storage Layer

Snowflake’s centralized database storage layer holds all data, including structured and semi-structured data. As data is loaded into Snowflake, it is optimally reorganized into a compressed, columnar format and stored and maintained in Snowflake databases. Each Snowflake database consists of one or more schemas, which are logical groupings of database objects such as tables and views.

Data stored in Snowflake databases is always compressed and encrypted. Snowflake takes care of managing every aspect of how the data is stored. Snowflake automatically organizes stored data into micro-partitions, an optimized, immutable, compressed columnar format which is encrypted using AES-256 encryption. Snowflake optimizes and compresses data to make metadata extraction and query processing easierand more efficient. whenever a user submits a Snowflake query, that query will be sent to the cloud services optimizer before being sent to the compute layer for processing.

Snowflake’s data storage layer is sometimes referred to as the remote disk layer. The underlying file system is implemented on Amazon, Microsoft, or Google Cloud. The specific provider used for data storage is the one you selected when you created your Snowflake account. Snowflake doesn’t place limits on the amount of data you can store or the number of databases or database objects you can create. Snowflake tables can easily store petabytes of data. There is no effect on virtual warehouse size as the storage increases or decreases in a Snowflake account. The two are scaled independently from each other and from the cloud services layer.

two unique features in the storage layer architecture: TimeTravel and zero-copy cloning.

Introduction to Zero-Copy Cloning

Zero-copy cloning offers the user a way to snapshot a Snowflake database, schema, or table along with its associated data. There is no additional storage charge until changes are made to the cloned object, because zero-copy data cloning is a metadata-only operation. For example,if you clone a database and then add a new table or delete some rows from a cloned table, at that point storage charges would be assessed. Most often, zero-copy clones will be used to support development and test environments.

Introduction to Time Travel

Time Travel allows you to restore a previous version of a database, table,or schema. This is an incredibly helpful feature that gives you an opportunity to fix previous edits that were done incorrectly or restore items deleted in error. With Time Travel, you can also back up data from different points in the past by combining the Time Travel feature with the clone feature, or you can perform a simple query of a database object that no longer exists. How far back you can go into the past depends on a few different factors. 


Billing for the Storage Layer
Snowflake data storage costs are calculated based on the daily average size of compressed rather than uncompressed data. Storage costs include the cost of persistent data stored in permanent tables and files staged for bulk data loading and unloading. Fail-safe data and the data retained for data recovery using Time Travel are also considered in the calculation of data storage costs. Clones of tables referencing data that has been deleted are similarly considered.

Snowflake Caching
When you submit a query, Snowflake checks to see whether that query has been previously run and, if so, whether the results are still cached. Snowflake will use the cached result set if it is still available rather than executing the query you just submitted. In addition to retrieving the pre-vious query results from a cache, Snowflake supports other caching techniques. There are three Snowflake caching types: the query result cache, the virtual warehouse cache, and the metadata cache.

query result cache
the fastest way to retrieve data from Snowflake is by using the query result cache. The results of a Snowflake query are cached, or persisted, for 24 hours and then purged. This contrasts with how the virtual warehouse cache and metadata cache work. Neither of those two caches is purged based on a timeline. Even though the result cache only persists for 24 hours, the clock is reset each time the query is reexecuted, up to a maximum of 31 days from the date and time when the query was first executed. After 31 days, or sooner if the underlying data changes, a new result is generated and cached when the query is submitted again.

The result cache is fully managed by the Snowflake global cloud services(GCS) layer, as shown in Figure 2-18, and is available across all virtual warehouses since virtual warehouses have access to all data. The process for retrieving cached results is managed by GCS. However, once the size of the results exceeds a certain threshold, the results are stored in and retrieved from cloud storage.

Query results returned to one user are also available to any user who has the necessary access privileges and who executes the same query. Therefore, any user can run a query against the result cache with no running virtual warehouse needed, assuming the query is cached and the underlying data has not changed.

Another unique feature of the query result cache is that it is the only cache that can be disabled by a parameter. Navigate to the Chapter2 worksheet and execute the following SQL statement:

ALTER SESSION SET USE_CACHED_RESULT=FALSE;

Disabling the result cache is necessary before performing A/B testing,and it is important to enable query result caching once the testing is complete.

Metadata Cache

The metadata cache is fully managed in the global services layer (as shown in Figure 2-19) where the user does have some control over the metadata but no control over the cache. Snowflake collects and manages metadata about tables, micro-partitions, and even clustering. For tables, Snowflake stores row count, table size in bytes, file references, and table versions. Thus, a running virtual warehouse will not be needed, because the count statistics are kept in the metadata cache when running a 

SELECT COUNT(*) on a table.

The Snowflake metadata repository includes table definitions and references to the micro-partition files for that table. The range of values in terms of MIN and MAX, the NULL count, and the number of distinct values are captured from micro-partitions and stored in Snowflake. As a result, some queries don’t require a running virtual warehouse to return results. For example, the MIN of zip code, an integer data type column, wouldn’t require virtual compute-only cloud services. Snowflake also stores the total number of micro-partitions and the depth of overlapping micro-partitions to provide information about clustering.

The information stored in the metadata cache is used to build the query execution plan. 

Virtual Warehouse Local Disk Cache

The traditional Snowflake data cache is specific to the virtual warehouse used to process the query. Running virtual warehouses use SSD storage to store the micro-partitions that are pulled from the centralized database storage layer when a query is processed. The size of the virtual warehouse’s SSD cache is determined by the size of the virtualwarehouse’s compute resources. The virtual warehouse data cache is limited in size and uses the LRU (LeastRecently Used) algorithm.

Whenever a virtual warehouse receives a query to execute, that warehouse will scan the SSD cache first before accessing the Snowflake remote disk storage. Reading from SSD is faster than from the database storage layer but still requires the use of a running virtual warehouse.

Although the virtual warehouse cache is implemented in the virtual warehouse layer where each virtual warehouse operates independently,the global services layer handles overall system data freshness. It does so via the query optimizer, which checks the freshness of each data segment of the assigned virtual warehouse and then builds a query plan to update any segment by replacing it with data from the remote disk storage.

Note that the virtual warehouse cache is sometimes referred to as the raw data cache, the SSD cache, or the data cache. This cache is dropped once the virtual warehouse is suspended, so you’ll want to consider the trade-off between the credits that will be consumed by keeping a virtual warehouse running and the value from maintaining the cache of data from previous queries to improve performance. By default, Snowflake will automatically suspend a virtual warehouse after 10 minutes of idletime, but this can be changed.

Whenever possible, and where it makes sense, assign the same virtual ware-house to users who will be accessing the same data for their queries. This increases the likelihood that they will benefit from the virtual warehouse localdisk cache.

USE ROLE SYSADMIN;
DROP WAREHOUSE CH2_WH;
DROP WAREHOUSE ACCOUNTING_WH;

Chapter 3. Creating and Managing Snowflake Securable Database Objects

Within Snowflake, all data is stored in database tables. Snowflake database tables are logically structured into collections of rows and columns. A Snowflake securable object is an entity for which you grant access to specific roles. Roles, which have been granted access privileges, are assigned to users. A Snowflake user can be either an individual person or an application. 

Creating and Managing Snowflake Databases

In Snowflake, the database logically groups the data while the schema organizes it. Together, the database and schema comprise the namespace. whenever we work with database objects we’ll need to specify a namespace, unless the schema and database we want to use are the active context in the workspace.

two main types of databases: permanent (persistent) and transient. At the time we create a database, the default will be a permanent database, if we don’t specify.

Snowflake is designed so that your data is accessible and recoverable at every stage within the data lifecycle. This is achieved through Continuous DataProtection (CDP). The important Snowflake CDP features introduced in this chapter are Time Travel and fail-safe.

Transient databases have a maximum one-day data retention period, aka Time Travel period, and do not have a fail-safe period. The Snowflake Time Travel period is the time during which table data within the database can be queried at a historical point in time. This also enables databases and database objects to be cloned or undropped and historical data to be restored. The default Time Travel period is one day but can be up to 90 days for permanent databases;
or a user could set the Time Travel period to zero days if no Time Travel period is desired. Enterprise Edition or higher is necessary to take advantage of the 90-day Time Travel period.

Snowflake’s fail-safe data recovery service provides a seven-day period during which data from permanent databases and database objects maybe recoverable by Snowflake. The fail-safe data recovery period is the seven-day period after the data retention period ends. Unlike Time Travel data, which is accessible by Snowflake users, fail-safe data is recoverable only by Snowflake employees.

It is important to note that data storage costs are incurred for those sevendays. That is one consideration when deciding what database type you want to create. Another consideration is the ability to recover data from other sources if the data stored in the database is lost after a single data Time Travel period is up.

CREATE DATABASE is the command used to create a new database, clone an existing database, create a database from a share provided by another Snowflake account, or create a replica of an existing primary database (i.e., a secondary database). We can create databases from the UI or by using SQL code in the Snowflake worksheet.

It is a best practice to usethe SYSADMIN role, rather than the ACCOUNTADMIN or SECURITYADMIN role, to create most Snowflake objects.

create a new transient database:
USE ROLE SYSADMIN;
USE WAREHOUSE COMPUTE_WH;

CREATE OR REPLACE TRANSIENT DATABASE DEMO3B_DB Comment='Transient Database for Chapter 3 Exercises';
CREATE TRANSIENT DATABASE DEMO3B_DB IF NOT EXISTS Comment='Transient Database for Chapter 3 Exercises';
 -- Better to use always

Dont Use the CREATE OR REPLACE statement in production so as not to over-write an existing database. As an alternative, if you don’t want to have an error but also don’t want to overwrite an existing database, use the CREATE DATABASE DEMO3B_DB IF NOT EXISTS statement.

Whenever you create a new database, that database is automatically setas the active database for the current session. It’s the equivalent of using the USE DATABASE command. 


USE ROLE ACCOUNTADMIN;
SHOW DATABASES;

Data retention time (in days) is the same as the Time Travel number of days and specifies the number of days for which the underlying data is retained after deletion, and for which the CLONE and UNDROP commands can be performed on the database.

We can change the data retention time for a permanent database but not for a transient one. We can change the retention time to up to 90 days for permanent databases, assuming we have an Enterprise or higher Snowflake edition.

USE ROLE SYSADMIN;
ALTER DATABASE DEMO3A_DB SET DATA_RETENTION_TIME_IN_DAYS=10;

USE ROLE SYSADMIN;

ALTER DATABASE DEMO3B_DB SET DATA_RETENTION_TIME_IN_DAYS=10;


You could change the retention time to zero days, but then you wouldn’t be able to CLONE or UNDROP that database.

Permanent databases are not limited to the different types of objects that can be stored within them. For example, you can store transient tables within a permanent database but you can-not store permanent tables within a transient database. As a reminder, transient tables are designed to hold transitory data that doesn’t need the same level of protection and recovery as permanent tables but does need to be maintained beyond a session.

Here is an example of creating a table in our transient database:

USE ROLE SYSADMIN;

CREATE OR REPLACE TABLE DEMO3B_DB.PUBLIC.SUMMARY (CASH_AMT number, RECEIVABLES_AMT number, CUSTOMER_AMT number);

Notice that we didn’t specify the type of table as either permanent ortransient. By default, Snowflake creates a permanent table in a database unless you indicate otherwise when you are creating the table. The exception would be when you are creating a table within a transient database. In that case, the table would also have to be transient.

By default, all tables created in a transient schema are transient. SHOW TABLES command has Kind column which shows transient parameter of tables.

There is no limit to the number of database objects, schemas, and databases thatcan be created within a Snowflake account.

The SNOWFLAKE database is owned by Snowflake Inc. and is a system-defined, read-only, shared database which provides object metadata and usage metrics about your account.

While using the SYSADMIN role, we are not able to see the SNOWFLAKE database because, by default, the SNOWFLAKE database is shown only to those using the ACCOUNTADMIN role. However, that privilege can be granted to other roles.

However, the sample database is actually one that has been shared from the Snowflake SFC_SAMPLES account and the database is read-only in your account, which means that no Data Definition Language (DDL) commands can be executed on that database. In addition, no Data Manipulation Language (DML) commands for actions such as cloning can be performed on the tables.

Creating and Managing Snowflake Schemas
when we create a schema, we need to let Snowflake know which database we want to use. If we don’t specify a particular database, Snowflake will use the one that is active.

USE ROLE SYSADMIN;

USE DATABASE DEMO3A_DB;

CREATE OR REPLACE SCHEMA BANKING;


In this example, we simply use the fully qualified schema name: 

USE ROLE SYSADMIN;

CREATE OR REPLACE SCHEMA DEMO3A_DB.BANKING;


new schema also has a retention time of 10 days, just like the database in which it was created.
However, we can always change the retention time to one day for the schema:

USE ROLE SYSADMIN;

ALTER SCHEMA DEMO3A_DB.BANKING SET DATA_RETENTION_TIME_IN_DAYS=1;

assume we’ve now decided we want that table to exist in a different schema.

USE ROLE SYSADMIN;
CREATE OR REPLACE SCHEMA DEMO3B_DB.BANKING;
ALTER TABLE DEMO3B_DB.PUBLIC.SUMMARY RENAME TO DEMO3B_DB.BANKING.SUMMARY;

Just like databases, schemas can be either permanent or transient, with the default being permanent. However, for schemas we have something unique, called a managed access schema. In a managed access schema, the schema owner manages grants on the objects within a schema, such as tables and views, but doesn’t have any of the USAGE, SELECT, or DROP privileges on the objects.

create a schema with managedaccess:

USE ROLE SYSADMIN;
USE DATABASE DEMO3A_DB;

CREATE OR REPLACE SCHEMA MSCHEMA WITH MANAGED ACCESS;

or regular schemas, the object owner role can grant object access to other roles and can also grant those roles the ability to manage grants for the object. However, in managed access schemas, object owners are unable to issue grant privileges. Instead, only the schema owner or a role with the MANAGE GRANTS privilege assigned to it can manage the grant privileges.

The SECURITYADMIN and ACCOUNTADMIN roles inherently have the MANAGE GRANTS privilege. Thus, both roles can manage the grant privileges on all managed schemas. 

Two database schemas, as shown in Figure 3-14, are included in every database that is created: INFORMATION_SCHEMA and PUBLIC. ThePUBLIC schema is the default schema and can be used to create any otherobjects, whereas the INFORMATION_SCHEMA is a special schema for the system that contains views and table functions which provide access to the metadata for the database and account.

The INFORMATION_SCHEMA, also known as the Data Dictionary, includes metadata information about the objects within the database as well as account-level objects such as roles. More than 20 system-defined views are included in every INFORMATION_SCHEMA. These views can be divided into two categories :account views and database views.

SELECT * FROM SNOWFLAKE_SAMPLE_DATA.INFORMATION_SCHEMA.SCHEMATA;

SHOW SCHEMAS IN DATABASE SNOWFLAKE_SAMPLE_DATA;


ACCOUNT_USAGE Schema
The SNOWFLAKE database, viewable by the ACCOUNTADMIN by default,includes an ACCOUNT_USAGE schema that is very similar to the INFORMATION_SCHEMA, but with three differences:

 - The SNOWFLAKE database ACCOUNT_USAGE schema includes recordsfor dropped objects whereas the INFORMATION_SCHEMA does not.
 - The ACCOUNT_USAGE schema has a longer retention time for histori-cal usage data. Where as the INFORMATION_SCHEMA has data available ranging from seven days to six months, the ACCOUNT_USAGE view retains historical data for one year.
 - Most views in the INFORMATION_SCHEMA have no latency, but the latency time for ACCOUNT_USAGE could range from 45 minutes to threehours. Specifically, for the INFORMATION_SCHEMA, there may be a one-to two-hour delay in updating storage-related statistics for ACTIVE_BYTES, TIME_TRAVEL_BYTES, FAILSAFE_BYTES, and RETAINED_FOR_CLONE_BYTES.

One of the common uses for the ACCOUNT_USAGE schema is to keep track of credits used over time by each virtual warehouse in your account(month to date). Change your role to ACCOUNTADMIN and execute the following


USE ROLE ACCOUNTADMIN;

USE DATABASE SNOWFLAKE;

USE SCHEMA ACCOUNT_USAGE;

USE WAREHOUSE COMPUTE_WH;


SELECT start_time::date AS USAGE_DATE, WAREHOUSE_NAME, SUM (credits_used) AS TOTAL_CREDITS_CONSUMED FROM warehouse_metering_history WHERE start_time>= date_trunc (Month, current_date) GROUP BY 1, 2 ORDER BY 1,2;


+------------+---------------------+------------------------+
| USAGE_DATE | WAREHOUSE_NAME      | TOTAL_CREDITS_CONSUMED |
|------------+---------------------+------------------------|
| 2024-11-18 | CLOUD_SERVICES_ONLY |            0.000102222 |
| 2024-11-18 | COMPUTE_WH          |            0.000011111 |
| 2024-11-19 | CLOUD_SERVICES_ONLY |            0.000005556 |
| 2024-11-19 | COMPUTE_WH          |            0.000007500 |
| 2024-11-22 | CLOUD_SERVICES_ONLY |            0.000187224 |
| 2024-11-22 | COMPUTE_WH          |            2.200158894 |
| 2024-11-23 | COMPUTE_WH          |            4.389136117 |
| 2024-11-24 | CLOUD_SERVICES_ONLY |            0.000007222 |
| 2024-11-24 | COMPUTE_WH          |            1.862475836 |
| 2024-11-25 | CLOUD_SERVICES_ONLY |            0.000074721 |
| 2024-11-25 | COMPUTE_WH          |            3.612968886 |
| 2024-11-26 | CLOUD_SERVICES_ONLY |            0.000023889 |
| 2024-11-26 | COMPUTE_WH          |            3.729749723 |
| 2024-11-27 | CLOUD_SERVICES_ONLY |            0.000025278 |
| 2024-11-27 | COMPUTE_WH          |            1.743578333 |
| 2024-11-28 | CLOUD_SERVICES_ONLY |            0.000032222 |
| 2024-11-28 | COMPUTE_WH          |            0.322506943 |
| 2024-11-29 | CLOUD_SERVICES_ONLY |            0.000014444 |
| 2024-11-29 | COMPUTE_WH          |            0.174034444 |
+------------+---------------------+------------------------+

ACCOUNT_USAGE schema, is only available to the ACCOUNTADMIN role, unless the ACCOUNTADMIN grants imported privileges from the underlying share to another role. 

Many objects exist within a Snowflake schema object, including tables,views, stages, policies, stored procedures, UDFs, and more.

Introduction to Snowflake Tables

All Snowflake data is stored in tables. In addition to permanent and transient tables, it is also possible to create hybrid, temporary, and external tables, Snowflake hybrid tables support the new Unistore workload. Snowflake temporary tables only exist within the session in which they were created and are frequently used for storing transitory data such as ETL data. Snowflake external tables give you the ability to directly process or query your data that exists else where with-out ingesting it into Snowflake, including data that lives in a data lake.

One new way you can work with your external data is by integrating Apache Hive metastores with Snowflake. You can use the new Hive Metastore connector to connect to your Hadoop environments. The Snowflake support is also available if you are using newer technologies, such as Delta Lake or Apache Iceberg. Delta Lake is a table format on a data lake Spark-based platform. A Snowflake external table can be created which will reference your DeltaLake cloud storage locations.

Apache Iceberg tables have addressed many of the challenges associated with object stores, which has made this a popular choice as a data lake. Where as Hive keeps track of data at the folder level, Iceberg keeps track of a complete list of all files within a table using a persistent tree structure. Keeping track of the data at the folder level can lead to performance problems, and there is the potential for data to appear as if it were missing when file list operations are performed at the folder level. The Apache Iceberg table format is used by many leading technology companies, in-cluding Apple, AWS, Expedia, LinkedIn, and Netflix.

In June 2022, Snowflake announced a new table type, dynamic tables (previously referred to as materialized tables). Dynamic tables allowusers to declaratively specify the pipelines where transformations can occur. Snowflake then handles the incremental refresh to materialize the data. In this way, Snowflake dynamic tables automatically refresh as new data streams in. Dynamic tables have some of the characteristics of materialized views and tasks and can be thought of as the logical progression of streams. 

Like databases and schemas, we can use the CREATE, ALTER, DROP, and SHOW TABLES commands. In addition, we’ll need to use INSERT INTO or COPY INTO to place data into a table. For Snowflake tables we create, we can also use the TRUNCATE or DELETE command to remove data from a table but not remove the table object itself.

TRUNCATE and DELETE are different in that TRUNCATE also clears table load history metadata while DELETE retains the load history metadata.
	
Snowflake assumes it should create a permanent table if the table type is not specified, unless the table is created within a transient database. Transient tables are unique to Snowflake and have characteristics of both permanent and temporary tables. Transient tables are designed for transitory data that needs to be maintained beyond a session but doesn’t need the same level of data recovery as permanent tables. As a result, the datastorage costs for a transient table would be less than for a permanent table. One of the biggest differences between transient tables and permanent tables is that the fail-safe service is not provided for transient tables.

It isn’t possible to change a permanent table to a transient table by using the ALTER command, because the TRANSIENT property is set at table creation time. Likewise, a transient table cannot be converted to a permanent table. If you would like to make a change to a transient or permanent table type, you’ll need to create a new table, use the COPY GRANTS clause, and then copy the data. Using the COPY GRANTS clause will ensure that the table will inherit any explicit access privileges.

If it makes sense to have new tables automatically created as a transient type by default, you can first create a transient database or schema. As we saw in “Creating and Managing Snowflake Databases”, all tables created afterward will be transient rather than permanent. Transient tables can be accessed by other users who have the necessary permissions. However, temporary tables exist only within the session in which they are created. This means they are not available to other users and cannot be cloned. Temporary tables have many uses, including being used for ETL data and for session-specific data needs.

A temporary table, as well as the data within it, is no longer accessible once the session ends. During the time a temporary table exists, it does count toward storage costs;
therefore, it is a good practice to drop a temporary table once you no longer need it.

Characteristics	Permanent table		Transient table		Temporary table		External table
Persistence		Until explicitly    Until explicitly	Remainder of		Until explicitly
                dropped	 			dropped	 			session	 			dropped
Time Travel
retention(days)	0–90				0 or 1				0 or 1				0
Fail-safe 
period(days)	7					0					0					0
Cloning 
possible?		Yes					Yes					Yes					No
Create views 
possible?		Yes					Yes					Yes					Yes


Interestingly, you can create a temporary table that has the same name as an existing table in the same schema since the temporary table is session based. No errors or warnings will be given. Thus, it is a best practice to give temporary tables unique names to avoid unexpected problems given that the temporary table takes precedence.

USE ROLE SYSADMIN;
USE DATABASE DEMO3A_DB;
CREATE OR REPLACE SCHEMA BANKING;

CREATE OR REPLACE TABLE CUSTOMER_ACCT (Customer_Account int, Amount int, transaction_ts timestamp);
CREATE OR REPLACE TABLE CASH (Customer_Account int, Amount int, transaction_ts timestamp);

CREATE OR REPLACE TABLE RECEIVABLES (Customer_Account int, Amount int, transaction_ts timestamp);

USE ROLE SYSADMIN;

CREATE OR REPLACE TABLE NEWTABLE(Customer_Account int, Amount int, transaction_ts timestamp);

views are the primary objects maintained in database schemas. Views are of two types: materialized and nonmaterialized. Whenever the term view is mentioned and the type is not specified, it is understood that it is a nonmaterialized view.

I’ve mentioned the importance of using fully qualified names for tables. Using a fully qualified name for the table is even more important when creating views,because the connected reference will be invalid if the namespace of the base table is not used and this table or the view is moved to a different schema or database later.

A view is considered to be a virtual table created by a query expression;
something like a window into a database.

USE ROLE SYSADMIN;

CREATE OR REPLACE VIEW DEMO3B_DB.PUBLIC.NEWVIEW AS 
SELECT CC_NAME FROM (SELECT * FROM SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.CALL_CENTER);

One purpose of views is to display selected rows and columns from one or more tables. This is a way to provide some security by only exposing certain data to specific users. Views can provide even more security bycreating a specific secure view of either a nonmaterialized or materialized view.

It’s important to remember that creating materialized views requiresSnowflake Enterprise Edition. 

USE ROLE SYSADMIN;
CREATE OR REPLACE MATERIALIZED VIEW DEMO3B_DB.PUBLIC.NEWVIEW_MVW
 AS SELECT CC_NAME FROM (SELECT * FROM SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.CALL_CENTER);
 
You can run a SHOW VIEWS command and both views will be returned inthe results. If you run a SHOW MATERIALIZED VIEWS command, only the materialized view result will be returned.

USE SCHEMA DEMO3B_DB.PUBLIC;
SHOW VIEWS;

Unlike a regular view, a materialized viewobject gets periodically refreshed with the data from the base table. Materialized views are generally used to aggregate as well as filter data so that the results of resource-intensive operations can be stored in a materialized view for improved data performance. The performance improvement is especially good when that same query is used frequently.
Snowflake uses a background service to automatically update materialized views. As a result, data accessed through materialized views is always current, regardless of the amount of DML that has been performed on the base table. Snowflake will update the materialized view or use the up-to-date portions of the materialized view and retrieve newer data from the base table if a query is run before the materialized view is upto date.


CREATE OR REPLACE MATERIALIZED VIEW DEMO3B_DB.BANKING.SUMMARY_MVW 
AS SELECT * FROM (SELECT * FROM DEMO3B_DB.BANKING.SUMMARY);


Let’s also create a nonmaterialized view, for comparison: 

CREATE OR REPLACE VIEW DEMO3B_DB.BANKING.SUMMARY_VW 
AS SELECT * FROM (SELECT * FROM DEMO3B_DB.BANKING.SUMMARY);

As you would expect, views are read-only. Thus, it isn’t possible to use the INSERT, UPDATE, or DELETE command on views. Further, Snowflake doesn’t allow users to truncate views. While it is not possible to execute DML commands on a view, you can use a subquery within a DML statement to update the underlying base table. An example might be something like this:

DELETE FROM <Base_Table> WHERE <Column_Name>> (SELECT AVG <Column> FROM <View_Name>);

Another thing to be aware of is that a view definition cannot be updatedwith the ALTER VIEW command. However, the ALTER MATERIALIZEDVIEW command can be used to rename a materialized view, to add or replace comments, to modify the view to be a secure view, and much more. The SHOW and DESCRIBE commands are also available for views. If you wanted to change something structural about the view, you would have to re-create it with a new definition. 

Changes to a source table’s structure do not automatically propagate to views. For example, dropping a table column won’t drop the column in the view.

One consideration beyond whether to create aregular view or a materialized view is whether to use ETL to materialize the data set in a table. 

As a general rule, it is best to use a nonmaterialized view when the results of the view change frequently and the query isn’t so complex and expensive to rerun. Regular views do incur compute costs but not storage costs. The compute cost to refresh the view and the storage cost will need to be weighed against the benefits of a materialized view when the results of a view change often.

Generally, it is beneficial to use a materialized view when the query consumes a lot of resources, as well as when the results of the view are used often and the underlying table doesn’t change frequently. Also, if a table needs to be clustered in multiple ways, a materialized view can be used with a cluster key that is different from the cluster key of the base table.

There are some limitations for materialized views. For example, a materialized view can query only a single table, and joins are not supported. It is recommended that you consult the Snowflake documentation for more detail on materialized view limitations.

One thing to remember is that we are using the SYSADMIN role currently and we’re creating the views using that role. Someone who doesn’t have the SYSDAMIN role will need to have assigned to them the privileges on the schema,the database objects in the underlying table(s), and the view itself if they are to work with the materialized view.

Introduction to Snowflake Stages: FileFormat Included

There are two types of Snowflake stages: internal and external. Stages are Snowflake objects that point to a storage location, either internal to Snowflake or on external cloud storage. Internal stage objects can be either named stages or a user or table stage. The temporary keyword can be used to create a session-based named stage object.

In most cases, the storage is permanent while the stage object, a pointer to the storage location, may be temporary or dropped at any time. Snowflake stages are often used as an intermediate step to load files to Snowflake tables or to unload data from Snowflake tables into files.

Snowflake permanent and internal temporary stages are used to store data files internally, on cloud storage managed by Snowflake, where as external stages reference data files that are stored in a location outside of Snowflake. Outside locations, whether private/protected or public, like Amazon S3 buckets, Google Cloud Storage buckets, and Microsoft Azure containers, are supported by Snowflake and can be used in external stages.

Each Snowflake user has a stage for storing files which is accessible onlyby that user. The User stage can be referenced by @~.Likewise, each Snowflake table has a stage allocated for storing files, and each tablestage can be referenced by using
@%<name of table>.

Table stages are useful if multiple users need to access the files and those files only need to be copied into a single table, where as a user stage is best when the files only need to be accessed by one user but will need to be copied into multipletables.

User and table stages cannot be altered or dropped, and neither of these stages supports setting the file format. But you can specify the format and copy options at the time the COPY INTO command is issued. Additionally, table stages do not support transforming the data while loading it. A table stage is tied to the table itself and is not a separate database object. To perform actions on the table stage, you must have been granted the table ownership role.

The command to list a user stage is ls @~;
or LIST @~;

The command to list a user stage is ls @%<tablename>;
or LIST @%<tablename>;

User stages and table stages, both of which are types of internal stages,are automatically provided for each Snowflake account. In addition touser and table stages, internal named stages can also be created. Internal named stages are database objects, which meansthey can be used not just by one user but by any user who has beengranted a role with the appropriate privileges.

The objects hierarchy for Snowflake stages Internal named stages and external stages can be created as either a permanent or a temporary stage. When a temporary external stage is dropped, no data files are removed, because those files are stored external to Snowflake. Only the stage object is dropped. For a temporary internal stage, however, the data and stage are both dropped and the files are not recoverable. It is important to note that the behavior just described is not limited to temporary stage objects. Both temporary and permanent stages, internal or external, have the same characteristics described.

When using stages, we can use file formats to store all the format information we need for loading data from files to tables. The default file format is CSV. However, you can create file formats for other formats such as JSON, Avro, ORC, Parquet, and XML. There are also optional parameters that can be included when you create a file format. Here we are creatinga very basic file format, without any optional parameters, for loading JSON data into a stage:

USE
ROLE
SYSADMIN
;
USE DATABASE DEMO3B_DB;

CREATE OR REPLACE FILE FORMAT FF_JSON TYPE=JSON;

Once we create the file format, we can make use of it when creating an internal named stage:

USE DATABASE DEMO3B_DB;

USE SCHEMA BANKING;

CREATE OR REPLACE TEMPORARY STAGE BANKING_STG FILE_FORMAT=FF_JSON;

The data is always in an encrypted state, whether it is in flight to an internalnamed stage or at rest while stored in a Snowflake database table.

Both stored procedures and UDFs encapsulate and return a single value (scalar). User-defined table functions (UDTFs) can return multiple values (tabular) where as stored procedures can return only a single value. You can create Snowflake stored procedures natively using JavaScript or with Snowflake scripting. Additionally, by using Snowpark, you can create Snowflake stored procedures using Java, Python, or Scala. UDFs can be created natively by using SQL, JavaScript, Python, and Java languages. It is possible to create secure UDFs

The return value for stored procedures is scalar, but procedures can return multiple values if the return type is a variant.

Both stored procedures and UDFs extend SQL capabilities, but there aremany differences between the two. One of the most important differencesis how they are used.

If you need to perform a database operation such as SELECT, DELETE, or CREATE, you’ll need to use a stored procedure. If you want to use a function as part of the SQL statement or expression, or if your output needs to include a value for every input row, you’ll want to use a UDF.
A UDF is called as part of the SQL statement, but a stored procedure cannot be called within a SQL statement. Instead, a stored procedure is called as an independent statement using the CALL statement. The CALL statement can call only one stored procedure per statement.
A UDF is required to return a value, and you can use the UDF return value inside your SQL statements. Although not required, a stored procedure is allowed to return a value, but the CALL command doesn’t providea place to store the returned value. The stored procedure also doesn’t provide a way to pass it to another operation.

User-Defined Function (UDF): Task Included

UDFs allow you to perform some operations that are not available through the built-in, system-defined functions provided by Snowflake. Snowflake supports four languages for UDFs. Python UDFs are limited to returning scalar results. SQL, JavaScript, and Java UDFs can return either scalar or tabular results.

A SQL UDF evaluates SQL statements and can refer to other UDFs, although a SQL UDF cannot refer to itself either directly or indirectly.

A JavaScript UDF is useful for manipulating variant and JSON data. A JavaScript UDF expression can refer to itself recursively, although it can-not refer to other UDFs. JavaScript UDFs also have size and depth limitations that don’t apply to SQL UDFs.

JavaScript UDFs have access to the basic standard JavaScript library needed to create arrays, variables, and simple objects. You cannot use math functions or use error handling, because Snowflake does not let you import external libraries. The properties that are available for both JavaScript UDFs and JavaScript procedures can be found by using the following commands:

USE ROLE SYSADMIN;

CREATE OR REPLACE DATABASE DEMO3C_DB;

CREATE OR REPLACE FUNCTION JS_PROPERTIES() 
RETURNS string 
LANGUAGE JAVASCRIPT AS 
$$ 
   return Object.getOwnPropertyNames(this);

$$;

SELECT JS_PROPERTIES();

a simple JavaScript UDF which returns a scalar result.

USE ROLE SYSADMIN;
USE DATABASE DEMO3C_DB;
CREATE OR REPLACE FUNCTION FACTORIAL (n variant) 
RETURNS variant 
LANGUAGE JAVASCRIPT 
AS 
'var f=n;

for(i=n-1;
i>0;
i--) 
  {
  f=f*i
  } 
return f';

SELECT FACTORIAL(5);

If you use a number greater than 33, you’ll receive an error message.

Try finding the result of FACTORIAL(50) and see what happens. If youuse the number 50 in the factorial function, you’ll receive an error mes-sage telling you that the numeric value is out of range.

By default,the PUBLIC schema will be used if you don’t specify a schema.

Secure UDFs are the same as nonsecure UDFs, except that they hide the DDL from the consumer of the UDF. Secure UDFs do have limitations on performance functionality due to some optimizations being by passed. Thus, data privacy versus performance is the consideration because only secure UDFs can be shared. Secure SQL UDFs and secure JavaScript UDFs are both shareable but operate differently and are generally used for different purposes. Secure JavaScript UDFs are often used for data cleansing, address matching, or other data manipulation operations.
Unlike secure JavaScript UDFs, a secure SQL UDF can run queries. For a secure shared UDF, the queries can only be run against the provider’s data. When a provider shares a secure UDF with a customer, the cost of data storage for any tables being accessed by the function is paid by the provider and the compute cost is paid by the consumer.

Secure SQL UDTF That Returns Tabular Value(Market Basket Analysis Example)

Market basket analysis is a common use of secure SQL user-defined tablefunctions (UDTFs). We wouldn’t want the consumer account to have access to our raw sales data, so we’ll wrap the SQL statement in a secure UDF and create an input parameter. When the secure UDF is used with an input parameter, the consumer still gets the same results as running the SQL statement directly on the underlying raw data.

USE ROLE SYSADMIN;

CREATE OR REPLACE DATABASE DEMO3D_DB;

CREATE OR REPLACE TABLE DEMO3D_DB.PUBLIC.SALES 
AS 
(SELECT * FROM SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.WEB_SALES) LIMIT 100000;

SELECT 1 AS INPUT_ITEM, WS_WEB_SITE_SK AS BASKET_ITEM, COUNT(DISTINCT WS_ORDER_NUMBER) BASKETS 
FROM DEMO3D_DB.PUBLIC.SALES 
WHERE WS_ORDER_NUMBER IN
  (SELECT WS_ORDER_NUMBER FROM DEMO3D_DB.PUBLIC.SALES WHERE WS_WEB_SITE_SK=1) 
GROUP BY WS_WEB_SITE_SK ORDER BY 3 DESC, 2;

The results of your query will probably vary slightly from what is shown here, for a few different reasons. First, the underlying sample data set could change. And second, because we’ve limited the previous results to 10,000 records, your limited data set could be different even if the underlying data set is still the same.

We might be willing to share those kinds of related sales details with the manufacturers, but we wouldn’t want to allow access to the underlyingsales data.


USE ROLE SYSADMIN;
CREATE OR REPLACE SECURE FUNCTION DEMO3D_DB.PUBLIC.GET_MKTBASKET
(INPUT_WEB_SITE_SK number (38)) 
RETURNS TABLE 
(INPUT_ITEM NUMBER (38,0), BASKET_ITEM NUMBER (38,0), BASKETS NUMBER (38,0)) 
AS 
'SELECT input_web_site_sk, WS_WEB_SITE_SK as BASKET_ITEM, COUNT(DISTINCT WS_ORDER_NUMBER) BASKETS 
FROM DEMO3D_DB.PUBLIC.SALES 
WHERE WS_ORDER_NUMBER IN 
 (SELECT WS_ORDER_NUMBER FROM DEMO3D_DB.PUBLIC.SALES WHERE WS_WEB_SITE_SK=input_web_site_sk) 
GROUP BY ws_web_site_sk ORDER BY 3 DESC, 2';

SELECT * FROM TABLE (DEMO3D_DB.PUBLIC.GET_MKTBASKET(1));

Secure UDFs should be used for instances in which data privacy is of concern and you want to limit access to sensitive data. It is important to consider the purpose and necessity of creating a secure UDF and weigh that against the decreased query performance that is likely to result from using a secure UDF.

Stored Procedures

Stored procedures are similar to functions in that they are created once and can be executed many times. Stored procedures allow you to extend Snowflake SQL by combining it with JavaScript in order to include branching and looping, as well as error handling. Stored procedures, which must be written in JavaScript, can be used to automate tasks that require multiple SQL statements performed frequently. Although stored procedures can only be written in JavaScript at this time, SQL stored procedures are now in private preview.

While you can use SELECT statements inside a stored procedure, the results must be used within the stored procedure. If not, only a single value result can be returned. Stored procedures are great for batch actions because a stored procedure runs by itself and, similar to a trigger, can be conditionally tied to database events.

USE ROLE SYSADMIN;
CREATE OR REPLACE DATABASE DEMO3E_DB;
CREATE OR REPLACE PROCEDURE STOREDPROC1 
(ARGUMENT1 VARCHAR) 
RETURNS string not null 
language javascript 
AS 
$$ 
   var INPUT_ARGUMENT1=ARGUMENT1;

   var result=`${INPUT_ARGUMENT1 }`;
   return result;

$$;

CALL STOREDPROC1 ('I really love Snowflake*');

SELECT * FROM DEMO3E_DB.INFORMATION_SCHEMA.PROCEDURES;

USE ROLE SYSADMIN;
USE DATABASE DEMO3A_DB;
USE SCHEMA BANKING;
CREATE OR REPLACE PROCEDURE deposit(PARAM_ACCT FLOAT,PARAM_AMT FLOAT) 
returns STRING LANGUAGE 
javascript 
AS 
$$ 
   var ret_val="";

   var cmd_debit="";

   var cmd_credit="";

   // INSERT data into tables 
   cmd_debit="INSERT INTO DEMO3A_DB.BANKING.CASH VALUES (" + PARAM_ACCT + "," + PARAM_AMT + ",current_timestamp());";
cmd_credit="INSERT INTO DEMO3A_DB.BANKING.CUSTOMER_ACCT VALUES (" + PARAM_ACCT + "," + PARAM_AMT + ",current_timestamp());";
   // BEGIN transaction 
   snowflake.execute({ sqlText:cmd_debit });
   snowflake.execute({ sqlText:cmd_credit });

   ret_val="Deposit Transaction Succeeded";

   return ret_val;

$$;

USE ROLE SYSADMIN;
USE DATABASE DEMO3A_DB;
USE SCHEMA BANKING;
CREATE OR REPLACE PROCEDURE withdrawal(PARAM_ACCT FLOAT, PARAM_AMT FLOAT) 
returns STRING 
LANGUAGE javascript 
AS 
$$ 
    var ret_val="";

	var cmd_debit="";

	var cmd_credit="";

	// INSERT data into tables 
	cmd_debit="INSERT INTO DEMO3A_DB.BANKING.CUSTOMER_ACCT VALUES (" + PARAM_ACCT + "," + (- PARAM_AMT) + ",current_timestamp());";

	cmd_credit="INSERT INTO DEMO3A_DB.BANKING.CASH VALUES (" + PARAM_ACCT + "," + (- PARAM_AMT) + ",current_timestamp());";

	// BEGIN transaction 
	snowflake.execute ({sqlText:cmd_debit });

	snowflake.execute ({ sqlText:cmd_credit });

	ret_val="Withdrawal Transaction Succeeded";

	return ret_val;

$$;

USE ROLE SYSADMIN;

USE DATABASE DEMO3A_DB;

USE SCHEMA BANKING;

CREATE OR REPLACE PROCEDURE loan_payment (PARAM_ACCT FLOAT, PARAM_AMT FLOAT) 
returns STRING 
LANGUAGE javascript 
AS 
$$ 
   var ret_val="";

   var cmd_debit="";

   var cmd_credit="";

   // INSERT data into the tables 
   cmd_debit="INSERT INTO DEMO3A_DB.BANKING.CASH VALUES (" + PARAM_ACCT + "," + PARAM_AMT + ",current_timestamp());";

   cmd_credit="INSERT INTO DEMO3A_DB.BANKING.RECEIVABLES VALUES (" + PARAM_ACCT + "," + (- PARAM_AMT) + ",current_timestamp());";

   // BEGIN transaction 
   snowflake.execute ({sqlText:cmd_debit});

   snowflake.execute ({sqlText:cmd_credit});

   ret_val="Loan Payment Transaction Succeeded";

   return ret_val;

$$;


CALL withdrawal (21, 100);
CALL loan_payment (21, 100);
CALL deposit (21, 100);

SELECT CUSTOMER_ACCOUNT, AMOUNT FROM DEMO3A_DB.BANKING.CASH;

So, now we can truncate the tables, leaving the tables intact but removing the data:

USE ROLE SYSADMIN;

USE DATABASE DEMO3A_DB;

USE SCHEMA BANKING;

TRUNCATE TABLE DEMO3A_DB.BANKING.CUSTOMER_ACCT;

TRUNCATE TABLE DEMO3A_DB.BANKING.CASH;

TRUNCATE TABLE DEMO3A_DB.BANKING.RECEIVABLES;

USE ROLE SYSADMIN;

CALL deposit (21, 10000);

CALL deposit (21, 400);

CALL loan_payment (14, 1000);

CALL withdrawal (21, 500);

CALL deposit (72, 4000);

CALL withdrawal (21, 250);


USE ROLE SYSADMIN;

USE DATABASE DEMO3B_DB;

USE SCHEMA BANKING;

create OR REPLACE PROCEDURE Transactions_Summary () 
returns STRING 
LANGUAGE javascript 
AS 
$$ 
   var cmd_truncate=` TRUNCATE TABLE IF EXISTS DEMO3B_DB.BANKING.SUMMARY;

   var sql=snowflake.createStatement ({sqlText:cmd_truncate});

   // Summarize Cash Amount 
   var cmd_cash='Insert into DEMO3B_DB.BANKING.SUMMARY (CASH_AMT) select sum (AMOUNT) from DEMO3A_DB.BANKING.CASH;' 
   var sql=snowflake.createStatement ({sqlText:cmd_cash});

   // Summarize Receivables Amount 
   var cmd_receivables='Insert into DEMO3B_DB.BANKING.SUMMARY (RECEIVABLES_AMT) select sum (AMOUNT) from DEMO3A_DB.BANKING.RECEIVABLES'
   var sql=snowflake.createStatement ({sqlText:cmd_receivables});

   // Summarize Customer Account Amount 
   var cmd_customer='Insert into DEMO3B_DB.BANKING.SUMMARY (CUSTOMER_AMT select sum (AMOUNT) from DEMO3A_DB.BANKING.CUSTOMER_ACCT;' 
   var sql=snowflake.createStatement ({sqlText:cmd_customer});

   // BEGIN transaction 
   snowflake.execute ({sqlText:cmd_truncate});

   snowflake.execute ({sqlText:cmd_cash});

   snowflake.execute ({sqlText:cmd_receivables});

   snowflake.execute ({sqlText:cmd_customer});

   ret_val="Transactions Successfully Summarized";

   return ret_val;

$$;

CALL Transactions_Summary ();

SELECT * FROM DEMO3B_DB.BANKING.SUMMARY;

USE ROLE SYSADMIN;

USE DATABASE DEMO3B_DB;

USE SCHEMA BANKING;

SELECT * FROM DEMO3B_DB.BANKING.SUMMARY_MVW;

Interestingly, the materialized view has been kept updated with the infor-mation from the SUMMARY base table

USE ROLE SYSADMIN;

USE DATABASE DEMO3B_DB;

USE SCHEMA BANKING;

SELECT * FROM DEMO3B_DB.BANKING.SUMMARY_VW;

we are going to add a task to execute this stored procedure.

tasks can be combined with table streams. a task can call a stored procedure, or it can execute a single SQL statement.

We will be creating a stored procedure that will delete a database. Thus,it’s important that we create the stored procedure in a different database than the one we’ll want to delete using the stored procedure:

USE ROLE SYSADMIN;

USE DATABASE DEMO3E_DB;

CREATE OR REPLACE PROCEDURE drop_db () 
RETURNS STRING NOT NULL 
LANGUAGE javascript 
AS 
$$ 
    var cmd=` DROP DATABASE DEMO3A_DB;
` 
	var sql=snowflake.createStatement ({ sqlText:cmd });

	var result=sql.execute ();

	return 'Database has been successfully dropped';

$$;

CALL drop_db();

Refresh the worksheet and you’ll see that the DEMO3A_DB database has been dropped. we’ll modify it so that it will drop a different database, and we’ll add a task so that the database will be dropped 15 minutes later.

USE ROLE SYSADMIN;

CREATE OR REPLACE PROCEDURE drop_db () 
RETURNS STRING NOT NULL 
LANGUAGE javascript 
AS 
$$ 
   var cmd=`DROP DATABASE "DEMO3B_DB";` 
   var sql=snowflake.createStatement ({ sqlText:cmd });

   var result=sql.execute ();

   return 'Database has been successfully dropped';

$$;

USE ROLE SYSADMIN;

USE DATABASE DEMO3E_DB;

CREATE OR REPLACE TASK tsk_wait_15 WAREHOUSE=COMPUTE_WH SCHEDULE='15 MINUTE' 
AS 
  CALL drop_db ();

The SYSADMIN role is going to need some privileges to execute the task,so be sure to use the ACCOUNTADMIN role for this command. Even though the SYSADMIN role is the task owner, an elevated account-level privilege to execute tasks is required. Make sure you set the role to ACCOUNTADMIN:

USE ROLE ACCOUNTADMIN;

GRANT EXECUTE TASK ON ACCOUNT TO ROLE SYSADMIN;

You can now set the role to SYSADMIN. Because tasks are always createdin a suspended state, they’ll need to be resumed:

USE ROLE SYSADMIN;

ALTER TASK IF EXISTS tsk_wait_15 RESUME;

Now our task is in a scheduled state. We’ll be able to see that by using this query against the INFORMATION_SCHEMA.TASK_HISTORY table function:

SELECT * FROM table (information_schema.task_history (task_name => 'tsk_wait_15', scheduled_time_range_start => dateadd ('hour', - 1, current_timestamp ())));

USE ROLE SYSADMIN;

ALTER TASK IF EXISTS tsk_15 SUSPEND;

Introduction to Pipes, Streams, andSequences

Pipes are objects that contain a COPY command that is used by Snowpipe. Snowpipe is used for continuous, serverless loading of data into a Snowflake target table. Snowflake streams, also known as change data capture (CDC), keep track of certain changes made to a table including inserts, updates, and deletes. Snowflake streams have many useful purposes, including recording changes made in a staging table which areused to update another table. 

A sequence object is used to generate unique numbers. Often, sequencesare used as surrogate keys for primary key values.

USE ROLE SYSADMIN;
USE DATABASE DEMO3E_DB;
CREATE OR REPLACE SEQUENCE SEQ_01 START=1 INCREMENT=1;
CREATE OR REPLACE TABLE SEQUENCE_TEST (i integer);

You can use the SELECT command three or four times to see how the NEXTVAL increments. Each time you execute the statement, you’ll noticethe NEXTVAL increases by one: 

SELECT SEQ_01.NEXTVAL;

Now let’s create a new sequence with a different increment:

USE ROLE SYSADMIN;

USE DATABASE DEMO3E_DB;

CREATE OR REPLACE SEQUENCE SEQ_02 START=1 INCREMENT=2;

CREATE OR REPLACE TABLE SEQUENCE_TEST (i integer);

SELECT SEQ_02.NEXTVAL a, SEQ_02.NEXTVAL b, SEQ_02.NEXTVAL c, SEQ_02.NEXTVAL d;

You should see that the value of A is 1, the value of B is 3, the value of C is 5, and the value of D is 7.

Some important things to remember are that the first value in a sequence cannot be changed after the sequence is created, and sequence values, although unique, are not necessarily free of gaps. This is because sequences are user-defined database objects;
thus, the sequence value can be shared by multiple tables because sequences are not tied to any specific table. Analternative to creating a sequence is to use identity columns in which you would generate auto-incrementing numbers, often used as a primary key,in one specific table.

A consideration when using sequences is that they may not be appropriate for sit-uations such as a secure UDF case. In some instances, a consumer may be able touse the difference between sequence numbers to infer information about the number of records. In that case, you can exclude the sequence column from the consumer results or use a unique string ID instead of a sequence.

how to combine the power of streams and tasks.

Snowflake Streams (Deep Dive)

A Snowflake stream works like a pointer that keeps track of the status of a DML operation on a defined table. A Snowflake table stream creates a change table that shows what has changed, at a row level, between two transactional points in time. Streams are like processing queues and can be queried just like a table. Table streams make it easy to grab the new data in a table so that one can have more efficient processing. Streams do that by taking a snapshot of all the rows in a table at a point in time and only storing an offset for the source table. In that way, a stream can return the CDC records by leveraging the versioning history.

table for a bank branch that stores the branch ID, city, and associated dollar amount of deposits for a particular day.

USE ROLE SYSADMIN;

CREATE OR REPLACE DATABASE DEMO3F_DB;

CREATE OR REPLACE SCHEMA BANKING;

CREATE OR REPLACE TABLE BRANCH 
(ID varchar, City varchar, Amount number (20 

INSERT INTO BRANCH 
(ID, City, Amount) 
values 
(12001, 'Abilene', 5387.97), 
(12002, 'Barstow', 34478.10), 
(12003, 'Cadbury', 8994.63);

SELECT * FROM BRANCH;

CREATE OR REPLACE STREAM STREAM_A ON TABLE BRANCH;

CREATE OR REPLACE STREAM STREAM_B ON TABLE BRANCH;

SHOW STREAMS;


INSERT INTO BRANCH (ID, City, Amount) 
values 
(12004, 'Denton', 41242.93), 
(12005, 'Everett', 6175.22), 
(12006, 'Fargo', 443689.75);

SELECT * FROM BRANCH;

SELECT * FROM STREAM_A;

SELECT * FROM STREAM_B;

You should see that there are now six records in the BRANCH table andthree records each in the streams. If we add another stream, STREAM_C, we will expect that there will be no records in that stream:

 CREATE OR REPLACE STREAM STREAM_C ON TABLE BRANCH;
 
Now let’s go ahead and insert some records a third time:

INSERT INTO BRANCH (ID, City, Amount) 
values 
(12007, 'Galveston', 351247.79), 
(12008, 'Houston', 917011.27);
 
If you run SELECT * statements on the table and the three streams, you’ll see that the table has eight records, STREAM_A and STREAM_B each have five records, and STREAM_C has two records.

Let’s do one more thing. Let’s re-create STREAM_B to see what happens inthe next section:

CREATE OR REPLACE STREAM STREAM_B ON TABLE BRANCH;

At this point, STREAM_B will have zero records.

DELETE FROM BRANCH WHERE ID=12001;

DELETE FROM BRANCH WHERE ID=12004;

DELETE FROM BRANCH WHERE ID=12007;

If we run SELECT * on the table and then on all three streams, we shouldsee five records in the BRANCH table (as shown in Figure 3-32), fourrecords in STREAM_A, three records in STREAM_B, and three records inSTREAM_C.

After STREAM_A was created, three records were entered: 12004, 12005,and 12006. Then two more records were entered: 12007 and 12008. When records 12004 and 12007 were deleted, they were removed from STREAM_A. When record 12001 was deleted, this showed up as a new entry in STREAM_A because record 12001 didn’t previously exist.

When records 12001, 12004, and 12007 were deleted, they all appeared in STREAM_B because they had not appeared there previously.

After STREAM_C was created, two records were entered: 12007 and 12008. When record 12007 was deleted, it was removed from STREAM_C. When records 12001 and 12004 were deleted, they showed up as new entries in STREAM_C because those records didn’t previously exist.

Our examples demonstrate the impact of inserting and deleting records, and we can see the results in the METADATA$ACTION columns. But what happens when we update a record? Let’s update the city to Fayetteville where the BRANCH ID equals 12006:

UPDATE BRANCH SET City='Fayetteville' WHERE ID=12006;

SELECT * FROM BRANCH;

Record 12006 already existed in STREAM_A and, thus, no new entry was needed. The value was simply updated in the stream 

SELECT * query on STREAM_A;

Record 12006 did not previously exist in STREAM_B. Therefore, we see that there is an entry for the deletion of record 12006 with the city of Fargo and then an entry for the insertion of the new 12006 record with the value of Fayetteville. You can see that those two new entries show as having a value of TRUE in the METADATA$ISUPDATE column. 

Record 12006 did not previously exist in STREAM_C. Therefore, we see that there is an entry for the deletion of record 12006 with the city of Fargo and then an entry for the insertion of the new 12006 record with the value of Fayetteville. You can see that those two entries show as having a value of TRUE in the METADATA$ISUPDATE column.

To summarize the metadata, let’s review the stream metadata columns. The METADATA$ACTION column tells us whether the row was inserted or deleted. If the row is updated, the METADATA$ISUPDATE column will be TRUE. And lastly, there is a unique hash key for the METADATA$ROW_ID column.

As you can see in the examples presented, Snowflake streams are a powerful way to handle changing data sets. In Snowflake, one of the most important reasons for using table streams is to keep the staging table and production table in sync. Using a staging table, along with streams, helps to protect undesired changes from being made to the production table. Snowflake table streams are also often used in conjunction with other features, such as Snowflake pipelines or Snowflake tasks.

Snowflake Tasks (Deep Dive)

Snowflake tasks are frequently used with Snowflake streams, demonstrate how Snowflake tasks can be used with streams.

One way Snowflake tasks can be completed is by using compute resources that are managed by users. We used this method earlier, where we designated a virtual warehouse for the task to use. Alternatively, the completion of tasks can be managed by Snowflake via a serverless compute model. In a serverless compute model, the compute resources are automatically resized and scaled up or down by Snowflake as the workload requirements change.

The option to enable the serverless compute model must be specified when the task is created. Tasks run on a schedule which is defined at the time a task is created. Alternatively, you can establish task dependencies where by a task can be triggered by a predecessor task. There is no event source that can trigger a task;
a task that is not triggered by a predecessor task must be run on a schedule.

tasks can be either scheduled or triggered by a predecessor task. You’ll notice that we elected to include a small virtual warehouse as the initial warehouse size for all the tasks, but we could have selected different sizes for different tasks.

You can use the cron expression in a task definition, rather than defining the number of minutes, to support specifying a time zone when scheduling tasks. Toavoid unexpected task executions due to daylight saving time, consider scheduling tasks at a time other than 1 a.m. to 3 a.m. daily or on days of the week that include Sundays.

The EXECUTE TASK command manually triggers a single run of a scheduled task that is either a standalone task or the root task in a task tree. If it is necessary to recursively resume all dependent tasks tied to a roottask, be sure to use the SYSTEM$TASK_DEPENDENTS_ENABLE function, rather than resuming each task individually by using the ALTER TASK RESUME command.

By default, Snowflake ensures that only one instance of a particular tree of tasks is allowed to run at one time. As a result, at least one run of the task tree will be skipped if the cumulative time required to run all tasks in the tree exceeds the explicit scheduled time set in the definition of the root task. When read/write SQLoperations executed by overlapping runs of a task tree don’t produce incorrect or duplicate data, overlapping runs may not be problematic. In this situation, if it makes sense to do so, the default behavior can be changed by setting the ALLOW_OVERLAPPING_EXECUTION parameter in the root task to TRUE.

If it would be problematic to have overlapping runs of a task tree, you have several options. You could choose a more appropriate warehouse size or use Snowflake-managed compute resources so that the task tree will finish before the next scheduled run of the root task. Alternatively, you could analyze the SQL statements or stored procedures executed by each task to determine whether the code could be optimized.

Three Snowflake task functions can be used to retrieve information about tasks:

SYSTEM$CURRENT_USER_TASK_NAME
	This function returns the name of the task currently executing when invokedfrom the statement or stored procedure defined by the task.
TASK_HISTORY
	This function returns task activity within the last seven days or the nextscheduled execution within the next eight days.
TASK_DEPENDENTS
	This is a table function that returns the list of child tasks for a given root task.

Because many access control privileges are needed to create and managetasks, it is recommended that a custom TASKADMIN role be created so that a role can alter its own tasks. Having a custom TASKADMIN role also allows for a SECURITYADMIN role, or any role with MANAGE GRANTS privileges, to manage the TASKADMIN role. Otherwise, only an ACCOUNTADMIN can revoke any task privileges assigned to the task owner.

The following example demonstrates how you would create a TASKADMIN role:

USE ROLE SECURITYADMIN;

CREATE ROLE TASKADMIN;


USE ROLE ACCOUNTADMIN;

GRANT EXECUTE TASK, EXECUTE MANAGED TASK ON ACCOUNT TO ROLE TASKADMIN;


USE ROLE SECURITYADMIN;

GRANT ROLE TASKADMIN TO ROLE <USER_ROLE>;

GRANT ROLE TASKADMIN TO ROLE SYSADMIN;

In practice, you’d need to make sure the USER_ROLE to which the TASKADMIN role is assigned has all the necessary access to the database, schema, and tables.

USE ROLE ACCOUNTADMIN;

USE WAREHOUSE COMPUTE_WH;


USE DATABASE DEMO3F_DB;

CREATE OR REPLACE SCHEMA TASKSDEMO;

CREATE OR REPLACE TABLE DEMO3F_DB.TASKSDEMO.PRODUCT (
Prod_ID int, 
Prod_Desc varchar (), 
Category varchar (30), 
Segment varchar (20), 
Mfg_ID int, 
Mfg_Name varchar (50)
);

INSERT INTO DEMO3F_DB.TASKSDEMO.PRODUCT values 
(1201, 'Product 1201', 'Category 1201', 'Segment 1201', '1201', 'Mfg 1201');


INSERT INTO DEMO3F_DB.TASKSDEMO.PRODUCT values 
(1202, 'Product 1202', 'Category 1202', 'Segment 1202', '1202', 'Mfg 1202');


INSERT INTO DEMO3F_DB.TASKSDEMO.PRODUCT values 
(1203, 'Product 1203', 'Category 1203', 'Segment 1203', '1203', 'Mfg 1203');


INSERT INTO DEMO3F_DB.TASKSDEMO.PRODUCT values 
(1204, 'Product 1204', 'Category 1204', 'Segment 1204', '1204', 'Mfg 1204');

INSERT INTO DEMO3F_DB.TASKSDEMO.PRODUCT values 
(1205, 'Product 1205', 'Category 1205', 'Segment 1205', '1205', 'Mfg 1205');

INSERT INTO DEMO3F_DB.TASKSDEMO.PRODUCT values 
(1206, 'Product 1206', 'Category 1206', 'Segment 1206', '1206', 'Mfg 1206');

CREATE OR REPLACE TABLE DEMO3F_DB.TASKSDEMO.SALES 
(Prod_ID int, 
Customer varchar (), 
Zip varchar (), 
Qty int, 
Revenue decimal (10, 2));

CREATE OR REPLACE STREAM DEMO3F_DB.TASKSDEMO.SALES_STREAM ON TABLE DEMO3F_DB.TASKSDEMO.SALES APPEND_ONLY=TRUE;

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES VALUES (1201, 'Amy Johnson', 45466, 45, 2345.67);

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES VALUES (1201, 'Harold Robinson', 89701, 45, 2345.67);

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES VALUES (1203, 'Chad Norton', 33236, 45, 2345.67);

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES VALUES (1206, 'Horatio Simon', 75148, 45, 2345.67);

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES VALUES (1205, 'Misty Crawford', 10001, 45, 2345.67);

SELECT * FROM DEMO3F_DB.TASKSDEMO.SALES_STREAM;

CREATE OR REPLACE TABLE DEMO3F_DB.TASKSDEMO.SALES_TRANSACT 
(
Prod_ID int, 
Prod_Desc varchar (), 
Category varchar (30), 
Segment varchar (20), 
Mfg_ID int, 
Mfg_Name varchar (50), 
Customer varchar (), 
Zip varchar (), 
Qty int, 
Revenue decimal (10, 2), 
TS timestamp);

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES_TRANSACT 
(Prod_ID, Prod_Desc, Category, Segment, Mfg_Id, Mfg_Name, Customer, Zip, Qty, Revenue, TS) 
SELECT s.Prod_ID, p.Prod_Desc, p.Category, p.Segment, p.Mfg_ID, p.Mfg_Name, s.Customer, s.Zip, s.Qty, s.Revenue, current_timestamp 
FROM DEMO3F_DB.TASKSDEMO.SALES_STREAM s 
JOIN DEMO3F_DB.TASKSDEMO.PRODUCT p 
ON s.Prod_ID=p.Prod_ID;

SELECT * FROM DEMO3F_DB.TASKSDEMO.SALES_TRANSACT;

CREATE OR REPLACE TASK DEMO3F_DB.TASKSDEMO.SALES_TASK 
  WAREHOUSE=compute_wh 
  SCHEDULE='1 minute' 
  WHEN system$stream_has_data ('DEMO3F_DB.TASKSDEMO.SALES_STREAM') 
  AS 
     INSERT INTO DEMO3F_DB.TASKSDEMO.SALES_TRANSACT (Prod_ID, Prod_Desc, Category, Segment, Mfg_Id, Mfg_Name, Customer, Zip, Qty, Revenue, TS) 
	 SELECT s.Prod_ID, p.Prod_Desc, p.Category, p.Segment, p.Mfg_ID, p.Mfg_Name, s.Customer, s.Zip, s.Qty, s.Revenue, current_timestamp FROM DEMO3F_DB.TASKSDEMO.SALES_STREAM s 
	 JOIN DEMO3F_DB.TASKSDEMO.PRODUCT p 
	 ON s.Prod_ID=p.Prod_ID;
	 
ALTER TASK DEMO3F_DB.TASKSDEMO.SALES_TASK RESUME;

Created tasks are suspended by default. Therefore, you’ll need to execute the ALTER TASK…RESUME statement to allow the task to run. Alternatively, for nonroot tree tasks, use the SYSTEM$TASK_DEPENDENTS_ENABLE function.

Note that it is possible for the root task to execute something on a stream event. However, there is a limitation that it can be scheduled at the lowest granulation to one minute. The previous example was used to demonstrate how you would use SYSTEM$STREAM_HAS_DATA to accomplish this.

Now let’s see what happens when we insert values into the sales table. Once we insert values into the sales table, the SALES_STREAM should reflect the newly inserted records. Then the task should insert the new sales records after joining with the product table and generating a timestamp. Let’s see what happens:

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES 
VALUES (1201, 'Edward Jameson', 45466, 45, 2345.67);

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES 
VALUES (1201, 'Margaret Volt', 89701, 45, 2345.67);

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES 
VALUES (1203, 'Antoine Lancaster', 33236, 45, 2345.67);

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES 
VALUES (1204, 'Esther Baker', 75148, 45, 2345.67);

INSERT INTO DEMO3F_DB.TASKSDEMO.SALES 
VALUES (1206, 'Quintin Anderson', 10001, 45, 2345.67);

SELECT * FROM DEMO3F_DB.TASKSDEMO.SALES_STREAM;

SELECT * FROM DEMO3F_DB.TASKSDEMO.SALES_TRANSACT;

ALTER TASK DEMO3F_DB.TASKSDEMO.SALES_TASK SUSPEND;

The costs associated with using tasks depend on whether you choose the virtual warehouse dedicated to the task or whether you opt to use the USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE command. The latter is aserverless feature.

Note that there is no need to drop objects in the hierarchy below the data-base before dropping the databases.

DROP DATABASE DEMO3C_DB;

DROP DATABASE DEMO3D_DB;

DROP DATABASE DEMO3E_DB;

DROP DATABASE DEMO3F_DB;

Chapter 4. Exploring Snowflake SQLCommands, Data Types, and Functions

Snowflake was built to store data in an optimized, compressed, columnar format within a relational database. Snowflake supports SQL:ANSI, the most common standardized version of SQL. Snowflake offers native support for semi-structured data formats such as JSON and XML. Snowflake also supports unstructured data.

Other than using worksheets in the Snowflake web UI, it is possible to use a Snowflake-native command-line client, known as SnowSQL, to create and execute SQL commands. Besides connecting to Snowflake via the web UI or SnowSQL, you can use ODBC and JDBC drivers to access Snowflake data through external applications such as Tableau and Looker.

Working with SQL Commands in Snowflake

Five different language command types. To create Snowflake objects, Data Definition Language (DDL) commands. Giving access to those objects Data Control Language(DCL). Data Manipulation Language (DML) to manipulate the data into and out of Snowflake. Transaction Control Language (TCL) to manage transaction blocks. Data Query Language (DQL) used to actually querythe data.

DDL commands:
CREATE
ALTER
TRUNCATE
RENAME
DROP
DESCRIBE
SHOW
USE
SET/UNSET
COMMENT

DCL commands:
GRANT
REVOKE

DML commands:
INSERT
MERGE
UPDATE
DELETE
COPY INTO
PUT
GET
LIST
VALIDATE
REMOVE

TCL commands:
BEGIN
COMMIT
ROLLBACK
CREATE

DQL command:
SELECT

DDL Commands
used to define the database schema. can be used to perform account-level sessionoperations, such as setting parameters, Snowflake DDL commands manipulate objects such as databases, virtual warehouses, schemas, tables, and views;
however, they do not manipulate data.

DCL Commands
used to enable access control.Examples of DCL commands include GRANT and REVOKE. how to secure Snowflake objects.

DML Commands
DML commands used to manipulate the data. 
The traditional DML commands INSERT, MERGE, UPDATE, and DELETE for general data manipulation.
COPY INTO <table> and COPY INTO <location> commands For data loadingand unloading.
Additionally, Snowflake’s DML commands in-clude some commands that do not perform any actual data manipulation but are used to stage and manage files stored in Snowflake locations. Some examples include VALIDATE, PUT, GET, LIST, and REMOVE.

TCL Commands
used to manage transactionblocks within Snowflake. BEGIN, COMMIT, and ROLLBACK can be used for multistatement transactions in a session. A Snowflake transaction is a set of read and write SQL statements that are processed together as one unit.

DQL Command
either a statement or a clause to retrieve data that meets the criteria specified in the SELECT command. The Snowflake SELECT command works on external tables and can be used to query historical data. In certain situations, using the SELECT statement will not require a running virtual warehouse to return results;
this is because of Snowflake caching.

Executable SnowSQL can be run as an interactive shell or inbatch mode.

SELECT command is evaluated in the following order:
FROM
WHERE
GROUP BY
HAVING
WINDOW
QUALIFY
ORDER BY
LIMIT

Table 4.1

Note that QUALIFY is evaluated after a window function;
QUALIFY workswith window functions in much the same way as HAVING does with theaggregate functions and GROUP BY clauses.

A subquery is a query within another query and can be used to compute values that are returned in a SELECT list, grouped in a GROUP BY clause,or compared with other expressions in the WHERE or HAVING clause.

A Snowflake subquery is a nested SELECT statement supported as a blockin one or more of the following Snowflake SQL statements:

CREATE TABLE AS
SELECT
INSERT
INSERT INTO
UPDATE
DELETE

USE ROLE SYSADMIN;

USE WAREHOUSE COMPUTE_WH;

CREATE OR REPLACE DATABASE DEMO4_DB;

CREATE OR REPLACE SCHEMA SUBQUERIES;

CREATE OR REPLACE TABLE DEMO4_DB.SUBQUERIES.DERIVED (ID integer, AMT integer, Total integer);

INSERT INTO DERIVED 
(ID, AMT, Total) 
VALUES 
(1, 1000, 4000),
(2, 2000, 3500),
(3, 3000, 9900),
(4, 4000, 3000), 
(5, 5000, 3700),
(6, 6000, 2222);


SELECT * FROM DEMO4_DB.SUBQUERIES.DERIVED;

CREATE OR REPLACE TABLE DEMO4_DB.SUBQUERIES.TABLE2 
(ID integer, AMT integer, Total integer);

INSERT INTO TABLE2 (ID, AMT, Total) 
VALUES 
(1, 1000, 8300),
(2, 1001, 1900),
(3, 3000, 4400),
(4, 1010, 3535), 
(5, 1200, 3232),
(6, 1000, 2222);

 
SELECT * FROM DEMO4_DB.SUBQUERIES.TABLE2;

SELECT ID, AMT FROM DEMO4_DB.SUBQUERIES.DERIVED WHERE AMT=(SELECT MAX (AMT) FROM DEMO4_DB.SUBQUERIES.TABLE2);

uncorrelated subquery is an independent query, onein which the value returned doesn’t depend on any columns of the outerquery. An uncorrelated subquery returns a single result that is used bythe outer query only once. A correlated subquery ref-erences one or more external columns. A correlated subquery is evalu-ated on each row of the outer query table and returns one result per row that is evaluated.

SELECT ID, AMT FROM DEMO4_DB.SUBQUERIES.DERIVED 
WHERE AMT=(SELECT AMT FROM DEMO4_DB.SUBQUERIES.TABLE2 WHERE ID=ID);

We receive an error message telling us that a single-row subquery re-turns more than one row.

Logically, we know there is only one row per ID;
so, the subquery won’tbe returning more than one row in the result set. However, the servercan’t know that. We must use a MIN, MAX, or AVG function so that theserver can know for certain that only one row will be returned each timethe subquery is executed.

Let’s go ahead and add MAX to the statement to see for ourselves how this works:

SELECT ID, AMT FROM DEMO4_DB.SUBQUERIES.DERIVED WHERE AMT=(SELECT MAX (AMT) FROM DEMO4_DB.SUBQUERIES.TABLE2 WHERE ID=ID);
SELECT ID, AMT FROM DEMO4_DB.SUBQUERIES.DERIVED WHERE AMT> (SELECT MAX (AMT) FROM DEMO4_DB.SUBQUERIES.TABLE2 WHERE ID=ID);
SELECT ID, AMT FROM DEMO4_DB.SUBQUERIES.DERIVED WHERE AMT> (SELECT AVG (AMT) FROM DEMO4_DB.SUBQUERIES.TABLE2 WHERE ID=ID);

Correlated subqueries are used infrequently because they result in onequery per row, which is probably not the best scalable approach for most use cases. 

Subqueries can be used for multiple purposes, one of which is to calculateor derive values that are then used in a variety of different ways. Derived columns can also be used in Snowflake to calculate another derived column, can be consumed by the outer SELECT query, or can be used as partof the WITH clause. These derived column values, sometimes called com-puted column values or virtual column values, are not physically stored ina table but are instead recalculated each time they are referenced in a query.

how a derived column can be used inSnowflake to calculate another derived column. Let’s create a derived column, AMT1, from the AMT column and then directly use the first derived column to create the second derived column, AMT2:

SELECT ID, AMT, AMT * 10 as AMT1, AMT1 + 20 as AMT2 FROM DEMO4_DB.SUBQUERIES.DERIVED;

The subquery inour example is a Snowflake uncorrelated scalar subquery.

SELECT sub.ID, sub.AMT, sub.AMT1 + 20 as AMT2 FROM (SELECT ID, AMT, AMT * 10 as AMT1 FROM DEMO4_DB.SUBQUERIES.DERIVED) AS sub;

you’ll notice that we’ve included a CTE subquery whichcould help increase modularity and simplify maintenance. The CTE de-
fines a temporary view name, which is CTE1 in our example.

WITH CTE1 AS (SELECT ID, AMT, AMT * 10 as AMT2 FROM DEMO4_DB.SUBQUERIES.DERIVED) 
SELECT a.ID, b.AMT, b.AMT2 + 20 as AMT2 FROM DEMO4_DB.SUBQUERIES.DERIVED a 
JOIN CTE1 b ON (a.ID=b.ID);

A major benefit of using a CTE is that it can make your code more read-able. With a CTE, you can define a temporary table once and refer to itwhenever you need it instead of having to declare the same subquery ev-ery place you need it. A recursive CTE can join a table to itself many times to process hierarchical data

Whenever the same names exist for a CTE and a table or view, the CTE will takeprecedence. Therefore, it is recommended to always choose a unique name foryour CTEs.

Caution about multirow inserts

There is one important thing to be aware of regarding multirow inserts. When inserting multiple rows of data into a
VARCHAR data type, each datatype being inserted into VARCHAR columns must be the same or else the insert will fail. A VARCHAR data type can accept data values such as the word one or the number 1, but never both types of values in the same INSERT statement.

USE ROLE SYSADMIN;

CREATE OR REPLACE SCHEMA DEMO4_DB.TEST;

CREATE OR REPLACE TABLE DEMO4_DB.TEST.TEST1 (ID integer, DEPT Varchar);

INSERT INTO TEST1 (ID, DEPT) 
VALUES (1, 'one');

SELECT * FROM DEMO4_DB.TEST.TEST1;

USE ROLE SYSADMIN;

CREATE OR REPLACE SCHEMA DEMO4_DB.TEST;

CREATE OR REPLACE TABLE DEMO4_DB.TEST.TEST1 (ID integer, DEPT Varchar);

INSERT INTO TEST1 (ID, DEPT) 
VALUES (1, 1);

SELECT * FROM DEMO4_DB.TEST.TEST1;

USE ROLE SYSADMIN;

CREATE OR REPLACE SCHEMA DEMO4_DB.TEST;

CREATE OR REPLACE TABLE DEMO4_DB.TEST.TEST1 (ID integer, DEPT Varchar);

INSERT INTO TEST1 (ID, DEPT) 
VALUES (1, 'one'), 
(2, 2);

SELECT * FROM DEMO4_DB.TEST.TEST1;

When we try to insert two different data types into the VARCHAR columnat the same time, we experience an error,
We’re also successful if we insert two numerical values into the VARCHAR column:

USE ROLE SYSADMIN;

CREATE OR REPLACE SCHEMA DEMO4_DB.TEST;

CREATE OR REPLACE TABLE DEMO4_DB.TEST.TEST1 (ID integer, DEPT Varchar);

INSERT INTO TEST1 (ID, DEPT) 
VALUES (1, 1), (2, 2);

SELECT * FROM DEMO4_DB.TEST.TEST1;

You’ll notice that we are able to successfully load two different data types into the VARCHAR column, but not at the same time. And once we have two different data types in the VARCHAR column, we can still add additional values:

INSERT INTO TEST1 (ID, DEPT) VALUES (5, 'five');

SELECT * FROM DEMO4_DB.TEST.TEST1;

Query Operators

There are several different types of query operators, including arithmetic, comparison, logical, subquery, and set operators. Arithmetic operators, including +, –, *, /, and %, produce a numeric output from one or more inputs. The scale and precision of the output depends on the scale and precision of the input(s). Note that subtraction is the only arithmetic operation allowed on DATE expressions. Comparison operators, typically appearing in a WHERE clause, are used to test the equality of two inputs.

Equal (=)
Not equal (!= or <>)
Less than (<)
Less than or equal (<=)
Greater than (>)
Greater than or equal (>=)

Remember that TIMESTAMP_TZ values are compared based on their times in UTC,which does not account for daylight saving time. This is important because, at themoment of creation, TIMESTAMP_TZ stores the offset of a given time zone, not theactual time zone.

Logical operators can only be used in the WHERE clause. The order ofprecedence of these operators is NOT then AND then OR.Subquery opera-tors include[NOT] EXISTS, ANY or ALL, and[NOT] IN.Queries can becombined when using set operators such as INTERSECT, MINUS or EXCEPT, UNION, and UNION ALL.The default set operator order of preference is INTERSECT as the highestprecedence, followed by EXCEPT, MINUS, and UNION, and finally UNIONALL as the lowest precedence. Of course, you can always use parentheses to override the default. Note that the UNION set operation is costly because it needs to sort the records to eliminate duplicate rows.

When using set operators, make sure each query selects the same number of col-umns and the data type of each column is consistent, although an explicit typecast can be used if the data types are inconsistent.

Long-Running Queries, and Query Performanceand Optimization

The Snowflake system will cancel long-running queries. The default duration for long-running queries is two days, but the STATEMENT_TIMEOUT_IN_SECONDS duration value can always be set at anaccount, session, object, or virtual warehouse level. During the Snowflake SQL query process, one of the things that happens is the optimization engines find the most efficient execution plan for aspecific query. 

Snowflake Query Limits

SQL statements submitted through Snowflake clients have a query textsize limit of 1 MB. Included in that limit are literals, including both stringand binary literals. The query text size limit applies to the compressedsize of the query. However, because the compression ratio for data varieswidely, it is recommended to keep the uncompressed query text size be-low 1 MB. 

Additionally, Snowflake limits the number of expressions allowed in aquery to 16,384.

Another type of query limit error occurs when using a SELECT statement with an IN clause that has more than 16,384 values.

SELECT <column_1> FROM <table_1> WHERE <column_2> IN (1, 2, 3, 4, 5,...);

One solution would be to use a JOIN or UNION command after placingthose values in a second table.

SELECT <column_1> FROM <table_1> a 
JOIN <table_2> b ON a.<column_2>=b.<column_2>;

Introduction to Data Types Supportedby Snowflake

Snowflake supports the basic SQL data types including geospatial datatypes, and a Boolean logical data type which provides for ternary logic.Snowflake’s BOOLEAN data type can have an unknown value, or a TRUE or FALSE value. If the Boolean is used in an expression, such as a SELECT statement, an unknown value returns a NULL.If the Boolean is used as apredicate, such as in a WHERE clause, the unknown results will evaluateto FALSE.There are a few data types not supported by Snowflake, such asLarge Object (LOB), including BLOB and CLOB, as well as ENUM and user-defined data types. 

Snowflake offers native support for geospatial features such as points,lines, and polygons on the earth’s surface. The Snowflake GEOGRAPHY datatype follows the WGS standard. Points on the earth are represented as de-grees of longitude and latitude. Altitude is not currently supported.

If you have geospatial data such as longitude and latitude, WKT, WKB, or GeoJSON, it is recommended that you convert and store this data in GEOGRAPHYcolumns rather than keeping the data in their original formats in VARCHAR,VARIANT, or NUMBER columns. This could significantly improve the performanceof queries that use geospatial functionality.

Numeric Data Types

fixed-point numbers and floating-point numbers, Precision, thetotal number of digits, impacts storage, whereas scale, the number of digits following the decimal point, does not. However, processing numericdata values with a larger scale could cause slower processing.

It is a known issue that DOUBLE, DOUBLE PRECISION, and REAL columns are stored as DOUBLE but displayed as FLOAT.

Fixed-point numbers are exact numeric values and, as such, are often used for natural numbers and exact decimal values such as monetary amounts. In contrast, floating-point data types are used most often for mathematics and science.

USE ROLE SYSADMIN;

CREATE OR REPLACE SCHEMA DEMO4_DB.DATATYPES;

CREATE OR REPLACE TABLE NUMFIXED 
(NUM NUMBER, NUM12 NUMBER (12, 0), DECIMAL DECIMAL (10, 2), INT INT, INTEGER INTEGER);

DESC TABLE NUMFIXED;

USE ROLE SYSADMIN;

USE SCHEMA DEMO4_DB.DATATYPES;

CREATE OR REPLACE TABLE NUMFLOAT (FLOAT FLOAT, DOUBLE DOUBLE, DP DOUBLE PRECISION, REAL REAL);

DESC TABLE NUMFLOAT;

It is impor-tant to consider that integer values can be stored in a compressed formatin Snowflake, whereas float data types cannot. This results in less storagespace and less cost for integers. Querying rows for an integer table type also takes significantly less time.

Because of the inexact nature of floating-point data types, floating-point operations could have small rounding errors and those errors can accumulate, especially when using aggregate functions to process a large number of rows.

Snowflake’s numeric data types are supported by numeric constants.Constants, also referred to as literals, represent fixed data values. Numeric digits 0 through 9 can be prefaced by a positive or negative sign.Exponents, indicated by e or E, are also supported in Snowflake numeric constants.

String and Binary Data Types

supports both text and binary string data types, 

CHAR, CHARACTERS - Synonymous with VARCHAR;length is CHAR(1) if not specified
VARCHAR          - Holds Unicode characters;
noperformance difference between using full-length VARCHAR (16,777,216) or a smaller length
BINARY           - Has no notion of Unicode characters, so length is always measured in bytes;
if length is not specified,
					the default is 8 MB (themaximum length)

USE ROLE SYSADMIN;

USE SCHEMA DEMO4_DB.DATATYPES;

CREATE OR REPLACE TABLE TEXTSTRING 
(VARCHAR VARCHAR, V100 VARCHAR (100), CHAR CHAR, C100 CHAR (100), STRING STRING, S100 STRING (100), TEXT TEXT, T100 TEXT (100));


DESC TABLE TEXTSTRING;


string constants, which are always enclosed between delimiters, either single quotes or dollar signs. Using dollar sign symbols as delimiters is especially useful whenthe string contains many quote characters.

Date and Time Input/Output Data Types Snowflake

uses the Gregorian calendar, rather than the Julian calendar,for all dates and timestamps.

DATETIME 		- Alias for TIMESTAMP_NTZ
TIMESTAMP_NTZ	- Internally wall clock time;
TIMESTAMP without time zone

Snowflake’s data and time data types are supported by interval constants as well as date and time constants. Interval constants can be used to addor subtract a specific period of time to or from a date, time, or timestamp.The interval is not a data type;
it can be used only in date, time, or time-stamp arithmetic and will represent seconds if the date or time portion isnot specified.

The order of interval increments is important because increments are added or subtracted in the order in which they are listed. This could be important for calculations affected by leap years.

Semi-Structured Data Types

whereas semi-structured data, such as XML data, is not schema dependent, which makes it more difficult to store in a database. In some situations, however, semi-structured datacan be stored in a relational database. Snowflake supports data types for importing and operating on semi-structured data such as JSON, Avro, ORC, Parquet, and XML data. through its universal data type VARIANT, a special column type which allows you to store semi-structured data.

Note that it is possible for a VARIANT value to be missing, which is considered to be dif-ferent from a true null value.

VARIANT	- Stores values of any other type,up to a maximum of 16 MB uncompressed;
internally stored in compressed columnar binary representation
OBJECT 	- Represents collections of key-value pairs with the key as anonempty string and the valueof VARIANT type
ARRAY	- Represents arrays of arbitrary size whose index is a non-negative integer and values have VARIANT type

When loaded into a VARIANT column, non-native values such as dates and time-stamps are stored as strings. Storing values in this way will likely cause operations to be slower and to consume more space as compared to storing date and timestamp values in a relational column with the corresponding data type.

learn how to use the FLATTEN function to produce a lateral view of the semi-structured data.

USE ROLE SYSADMIN;

USE SCHEMA SNOWFLAKE_SAMPLE_DATA.WEATHER;

SELECT * FROM DAILY_16_TOTAL LIMIT 5;

SELECT v:city FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL LIMIT 10;

SELECT v:city:id, v:city:name, v:city:country, v:city:coord FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL LIMIT 10;

SELECT v:city:id AS ID, v:city:name AS CITY, v:city:country AS COUNTRY, v:city:coord:lat AS LATITUDE, v:city:coord:lon AS LONGITUDE FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL LIMIT 10;

In the next ex-ample, we’ll cast the city and country VARIANT data to a VARCHAR datatype, and we’ll assign meaningful labels to the columns:

SELECT v:city:id AS ID, v:city:name :: varchar AS city, v:city.country :: varchar AS country, v:city:coord:lon AS longitude, v:city:coord:lat AS latitude FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL LIMIT 10;

DESC RESULT LAST_QUERY_ID ();
-- asking Snowflake to describe the results of our last query:

SELECT v:data FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL LIMIT 10;

You’ll notice that there is one array in this column relating to the DATA information. Because the DATA information is stored as an array, we can look at a particular element in the array.

SELECT v:data[5] FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL LIMIT 10;

SELECT v:city:name AS city, v:city:country AS country, v:data[0]: humidity AS HUMIDITY FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL LIMIT 10;

SELECT v:data[0]: dt :: timestamp AS TIME, v:data[0]: humidity AS HUMIDITY0, v:data[0]: temp:day AS DAY_TEMP0, v:data[1]: humidity AS HUMIDITY1, v:data[1]: temp:day AS DAY_TEMP1, v:data AS DATA FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL LIMIT 100;

Let’s see how we can leverage the FLATTEN table function. The FLATTEN function produces a lateral view of a VARIANT, OBJECT, or ARRAY col-umn. We’ll demonstrate how FLATTEN works on the DATA array in thesample weather data table:

SELECT d.value:dt :: timestamp AS TIME, v:city:name AS CITY, v:city:country AS COUNTRY, d.path AS PATH, d.value:humidity AS HUMIDITY, d.value:temp:day AS DAY_TEMP, v:data AS DATA FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL, LATERAL FLATTEN (input => daily_16_total.v:data) d LIMIT 100;

SELECT d.value:dt :: timestamp AS TIME, t.key, v:city:name AS CITY, v:city:country AS COUNTRY, d.path AS PATH, d.value:humidity AS HUMIDITY, d.value:temp:day AS DAY_TEMP, d.value:temp:night AS NIGHT_TEMP, v:data AS data FROM SNOWFLAKE_SAMPLE_DATA.WEATHER.DAILY_16_TOTAL, LATERAL FLATTEN (input => daily_16_total.v:data) d, LATERAL FLATTEN (input => d.value:temp) t WHERE v:city:id=1274693 LIMIT 100;

As we’ve just seen, the Snowflake FLATTEN function is used to convert semi-structured data to a relational representation

It is possible to combine a LATERAL JOIN with a FLATTEN function to separate events into individual JSON objects while preserving the global data .

Unstructured Data Types

Unstructured data is often qualitative, but it can also be quantitative datalacking rows, columns, or delimiters, the case with a PDF file that contains quantitative data. can be used for analytical purposes and for the purpose of sentiment analysis. Storing and governing unstructured data is noteasy. Unstructured data is not organized in a predefined manner, which means it is not well suited for relational databases.

To improve searchability of unstructured data, Snowflake recentlylaunched built-in directory tables. Using a tabular file catalog forsearches of unstructured data is now as simple as using a SELECT * command on the directory table. Users can also build a table stream on top ofa directory table, which makes it possible to create pipelines for processing unstructured data. Additionally, Snowflake users can create secureviews on directory tables and, thus, are also able to share those secureviews with others.

How Snowflake Supports Unstructured Data Use
include video, audio, or image files, logfiles, sensor data, and social media posts. use cases could include deriving insights like sentiment analysis from call center recordings;
extracting text for analytics by using optical character recognition processes on insurance cards or prescription pills;
using machine learning on DICOM medical images;
or extracting key-value pairs from stored PDF documents.

The first consideration in using unstructured data is how and where tostore the unstructured files. there are two ways to do this: internal stages and external stages. We’d use an internal stage if we wanted to store data internally on Snowflake;
especially if we were looking for a simple, easy-to-manage solution. That is because Snowflake automatically manages the scalability, encryption, data compression, andother aspects of storage. We’d alternatively use an external stage, known asbring your own storage, if we have legacy data stored else where across the cloud as there is no need to move all your data into Snowflake.

VARIANT column type, it is usually not recommended because there is a file storage limitation of 16 MB. If we instead use a stage,there are no size limitations other than those imposed by the major cloudproviders upon which your Snowflake instance is built: 5 TB of data for AWS and GCP or 256 GB of data for Azure.

Whether you use internal or external Snowflake stages, control access todata is easily achieved through role-based access controls. By using GRANT and REVOKE statements, privileges can be given to Snowflake re-sources like stages by granting permissions to roles which are thengranted to individuals.

Using Snowflake, storing and granting access to unstructured data can bedone in three different ways: stage file URLs, scoped URLs, or presigned URLs.

Stage file URL access

A stage file URL is used to create a permanent URL to a file on a Snowflake stage and is used most frequently for custom applications. Access to a file URL is through a GET request to the REST API endpointalong with the authorization token. Stage file URLs have a unique feature in that theycan be listed in a Snowflake directory table.

The ability to create a directory table, like a file catalog, which you caneasily search to retrieve file URLs to access the staged files as well asother metadata, is a unique feature that Snowflake provides for unstructured data. Snowflake roles that have been granted privileges can query adirectory table to retrieve URLs to access staged files.

Whether you want to, for example, sort by file size or by last modifieddate, or only take the top 100 files or the largest files, it is possible to do so with Snowflake directory tables.

Using tablestreams, for example, you can easily find all the new files that were re-cently added. Because a directory table is a table, you can perform fine-grain select and search operations. Search operations in regular blobstores are extremely difficult because they don’t have the catalog information in a tabular format.

A Snowflake directory table is a built-in read-only table. As such, you can-not add more columns or modify the columns in a directory table. What you can do is use Snowflake streams and tasks to calculate values and putthem into a new table with a column containing the results of the calculation. You’ll then be able to join that table with the directory table by creating a view. You can also add tags, if desired.

Scoped URL access

frequently used for custom applications;
especially in situations where access to the data will be given to other accounts using thedata share functionality or when ad hoc analysis is performed internallyusing Snowsight. Sharing unstructured data securely in the cloud is easywith Snowflake. No privileges are required on the stage. Instead, you’d create a secure view, and using the scoped URL, you would share the contents of the secure view. The scoped URL is encoded, so it is not possible to determine the account, database, schema, or other storage details fromthe URL.

Access to files in a stage using scoped URL access is achieved in one oftwo ways. One way is for a Snowflake user to click a scoped URL in the re-sults table in Snowsight. The other way is to send the scoped URL in a request which results in Snowflake authenticating the user, verifying thescoped URL has not expired, and then redirecting the user to the staged file in the cloud storage service. Remember, the location of the staged filein the cloud storage is encoded, so the user is unable to determine the lo-cation. The scoped URL in the output from the API call is valid for 24hours, the current length of time the result cache exists.

For security reasons, it is impossible to share a scoped URL that has been shared with you. If you were to share the link with someone else who does not have simi-lar access granted to them, the message access denied would appear.

Presigned URL access

A presigned URL is most often used for business intelligence applications or reporting tools that need to display unstructured file contents for openfiles. Because the presigned URLs are already authenticated, a user or application can directly access or download files without the need to passan authorization token.

The GET_PRESIGNED_URL function generates the presigned URL to a stage file using the stage name and relative file path as inputs. Access to files in a stage using a presigned URL can be accomplished in three differentways: use the presigned URL in a web browser to directly navigate to the file, click a presigned URL in the results table in Snowsight, or send the presigned URL in a REST API call request.

Processing unstructured data with Java functions and external functions

The ability to run processes on the unstructured data inside files is one ofthe most exciting features offered by Snowflake. Currently, there are twoways to process unstructured data using Snowflake: Java functions andexternal functions.

Note that Java UDFs are executed directly in Snowflake, using a Snowflake virtual warehouse. As such, Java UDFs do not make any API calls outside theboundaries of Snowflake. Everything is tightly secured and managedwithin the Snowflake environment.

If there are external API services such as machine learning models,geocoders, or other custom code that you want to utilize, external func-tions can be used. External functions make it possible to use existing machine learning services to extract text from images, or to process PDF filesto extract key-value pairs. In an external function, you can use any of the AWS, Azure, or GCP functionalities, including AWS Rekognition or AzureCognitive Services. External functions executed on unstructured data,whether stored within internal or external stages, can be used to eliminate the need to export and reimport data.

Snowflake SQL Functions and Session Variables 

Snowflake offers users the ability to create UDFs and to use external func-tions, as well as to access many different built-in functions. Session vari-ables also extend Snowflake SQL capabilities.

Using System-Defined (Built-In) Functions 

Examples of Snowflake built-in functions include scalar, aggregate, window, table, and system functions.

Scalar functions accept a single row or value as an input and then returnone value as a result, whereas aggregate functions also return a singlevalue but accept multiple rows or values as inputs.

Scalar functions

Some scalar functions operate on a string or binary input value.Examples include CONCAT, LEN, SPLIT, TRIM, UPPER and LOWER caseconversion, and REPLACE.Other scalar file functions, such as GET_STAGE_LOCATION, enable you to access files staged in Snowflakecloud storage.

Construct/deconstruct (extract) using month, day, and year components.
Truncate or “round up” dates to a higher level.
Parse and format dates using strings.
Add/subtract to find and use date differences.
Generate system dates or a table of dates.

Aggregate functions

A Snowflake aggregate function will always return one row even whenthe input contains no rows. The returned row from an aggregate functionwhere the input contains zero rows could be a zero, an empty string, orsome other value. Aggregate functions can be of a general nature, such as MIN, MAX, MEDIAN, MODE, and SUM.Aggregate functions also include lin- ear regression, statistics and probability, frequency estimation, percentileestimation, and much more.

Snowflake window functions are a special type of aggregate function thatcan operate on a subset of rows. This subset of related rows is called a window.Unlike aggregate functions which return a single value for agroup of rows, a window function will return an output row for each in-put row. The output depends not only on the individual row passed to thefunction but also on the values of the other rows in the window passed tothe function.

Window functions are commonly used for finding a year-over-year percentage change, a moving average, and a running or cumulative total, aswell as for ranking rows by groupings or custom criteria.

SELECT LETTER, SUM (LOCATION) as AGGREGATE 
FROM 
(
   SELECT 'A' as LETTER, 1 as LOCATION UNION ALL 
  (SELECT 'A' as LETTER, 1 as LOCATION) UNION ALL 
  (SELECT 'E' as LETTER, 5 as LOCATION)
) 
as AGG_TABLE GROUP BY LETTER;

SELECT LETTER, SUM (LOCATION) OVER (PARTITION BY LETTER) as WINDOW_FUNCTIon
 FROM 
(
    SELECT 'A' as LETTER, 1 as LOCATION UNION ALL 
	(SELECT 'A' as LETTER, 1 as LOCATION) UNION ALL 
	(SELECT 'E' as LETTER, 5 as LOCATION)
) as WINDOW_TABLE;

Table functions

Table functions, often called tabular functions, return results in a tabularformat with one or more columns and none, one, or many rows. Most Snowflake table functions are 1-to- N functions where each input row gen-erates N output rows, but there exist some M-to-N table functions where agroup of M input rows produces a group of N output rows. Table functions can be system defined or user defined. Some examples of system-de-fined table functions include VALIDATE, GENERATOR, FLATTEN, RESULT_SCAN, LOGIN_HISTORY, and TASK_HISTORY.

System functions

Built-in system functions return system-level information or query infor-mation, or perform control operations. One oft-used system information function is SYSTEM$CLUSTERING_INFORMATION, which returns clustering information, including the average clustering depth, about one or more columns in a table.
System control functions allow you to execute actions in the system. One example of a control function is SYSTEM$CANCEL_ALL_QUERIES and requires the session ID. You can obtain the session ID by logging in as the ACCOUNTADMIN.

SELECT SYSTEM$CANCEL_ALL_QUERIES (<session_id>);

If you need to cancel queries for a specific virtual warehouse or user rather than the session, you’ll want to use the
ALTER command along with ABORT ALL QUERIES instead of a system control function.

Creating SQL and JavaScript UDFs and Using Session Variables

SQL functionality can be extended by SQL UDFs, Java UDFs, Python UDFs, and session variables.

Snowflake supports SQL variables declared by the user, using the SET command. These session variables exist while a Snowflake session is ac-tive. Variables are distinguished in a Snowflake SQL statement by a $ prefix and can also contain identifier names when used with objects. You must wrap a variable inside the identifier, such as IDENTIFIER($Variable), to use a variable as an identifier. Alternatively, you can wrap the variable inside an object in the context of a FROM clause. To see all the variables defined in the current session, use the SHOW VARIABLES command.

Some examples of session variable functions include the following:

SYS_CONTEXT and SET_SYS_CONTEXT
SESSION_CONTEXT and SET_SESSION_CONTEXT
GETVARIABLE and SETVARIABLE

All variables created during a session are dropped when a Snowflake session is closed. If you want to destroy a variable during a session, you canuse the UNSET command.

External Functions

An external function is a type of UDF that calls code which is stored andexecuted outside of Snowflake. Snowflake supports scalar external func-tions, which means the remote service must return exactly one row foreach row received. Within Snowflake, the external function is stored as adatabase object that Snowflake uses to call the remote service.

It is important to note that rather than calling a remote service directly,Snowflake most often calls a proxy service to relay the data to the remoteservice. The Amazon API Gateway and Microsoft Azure API managementservice are two examples of proxy services that can be used. A remoteservice can be implemented as an AWS Lambda function, a MicrosoftAzure function, or an HTTPS server (e.g., Node.js) running on an EC2instance.

Any charges by providers of remote services will be billed separately.Snowflake charges normal costs associated with data transfer and virtual warehouse usage when using external functions.

There are many advantages of using external functions. External func-tions can be created to be called from other software programs in addition to being called from within Snowflake. Also, the code for the remoteservices can be written in languages such as Go or C#—languages that cannot be used within other UDFs supported by Snowflake. One of thebiggest advantages is that the remote services for Snowflake externalfunctions can be interfaced with commercially available third-party libraries, such as machine learning scoring libraries.

DROP DATABASE DEMO4_DB;

Chapter 5. Leveraging SnowflakeAccess Controls

necessary data must be made available tojust the right users at just the right time without sacrificing security.

Creating multiple layers of security that take advantage of Snowflake’sbuilt-in security options is among the best practices for managing security. The outer security layer relies on network policies, key pair authentication, multifactorauthentication (MFA), and secure private networks as options to ensurethat only authenticated users gain access to a Snowflake account.

Snowflake provides data security protection through data encryption.

next layer of security is user management andobject security. This security type uses role-based access control and dis-cretionary access control to open access to certain data for users 

Snowflake’s hybrid access control provides for access at a granular level.A combination of discretionary access control and role-based access con-trol approaches determines who can access what object and perform op-erations on those objects, as well as who can create or alter the accesscontrol policies themselves.
Discretionary access control (DAC) is a secu-rity model in which each object has an owner who has control over that object. Role-based access control (RBAC) is an approach in which accessprivileges are assigned to roles and roles are then assigned to one or more users.

In the Snowflake hybrid approach, all securable objects are owned by arole rather than a user. Further, each securable object is owned by onlyone role, which is usually the role used to create the object. Note that be-cause a role can be assigned to more than one user, every user grantedthe same role also has inherent privileges in a shared, controlled fashion.

Object ownership can be transferred from one role to another.

Securable object - Entity such as a database or table. Access to a securable object is denied unlessspecifically granted.
Role - Receives privileges to access and perform operations on an object or to create oralter the access control policies themselves. Roles are assigned to users. Rolescan also be assigned to other roles, creating a role hierarchy.
Privileges - Inherent, assigned, or inherited access to an object.
User - A person, service account, or program that Snowflake recognizes.

As explained in Chapter 3, every securable database object resides in a logical container within a hierarchy that includes a schema and a database, with the account as the top container for all securable objects.

Resource monitors are always owned by the ACCOUNTADMIN role. Only the ACCOUNTADMIN can create or drop this object. The ACCOUNTADMIN cannot grant this privilege to any other role, but can grant another role the ability to modify a resource monitor already created.

Creating Snowflake Objects

USE ROLE ACCOUNTADMIN;
GRANT ALL ON WAREHOUSE compute_wh TO ROLE SYSADMIN;

USE ROLE SYSADMIN;
USE WAREHOUSE COMPUTE_WH;
CREATE OR REPLACE WAREHOUSE VW1_WH WITH WAREHOUSE_SIZE='X-SMALL' INITIALLY_SUSPENDED=true;
CREATE OR REPLACE WAREHOUSE VW2_WH WITH WAREHOUSE_SIZE='X-SMALL' INITIALLY_SUSPENDED=true;
CREATE OR REPLACE WAREHOUSE VW3_WH WITH WAREHOUSE_SIZE='X-SMALL' INITIALLY_SUSPENDED=true;
SHOW WAREHOUSES;

show databases and show warehouses return only the databases or warehouse to which the role has access.

Three databases are included and made available to every role, including the PUBLIC role, at the time when a Snowflake account is created.

USE ROLE SYSADMIN;
 SHOW DATABASES;
 

USE ROLE PUBLIC;
 SHOW DATABASES;


USE ROLE ACCOUNTADMIN;
 SHOW DATABASES;
 -- have access to snowflake_sample_data database

USE ROLE SYSADMIN;
 
CREATE OR REPLACE DATABASE DB1;
CREATE OR REPLACE DATABASE DB2;
CREATE OR REPLACE SCHEMA DB2_SCHEMA1;
CREATE OR REPLACE SCHEMA DB2_SCHEMA2;
CREATE OR REPLACE SCHEMA DB1.DB1_SCHEMA1;
    -- Fully qualified name
SHOW DATABASES;


USE ROLE ACCOUNTADMIN;
 
CREATE OR REPLACE RESOURCE MONITOR MONITOR1_RM WITH CREDIT_QUOTA=10000 TRIGGERS ON 75 PERCENT DO NOTIFY ON 98 PERCENT DO SUSPEND ON 105 PERCENT DO SUSPEND_IMMEDIATE;
 
SHOW RESOURCE MONITORS;

USE ROLE USERADMIN;
 
CREATE OR REPLACE USER USER1 LOGIN_NAME=ARNOLD;
CREATE OR REPLACE USER USER2 LOGIN_NAME=BEATRICE;
CREATE OR REPLACE USER USER3 LOGIN_NAME=COLLIN;
CREATE OR REPLACE USER USER4 LOGIN_NAME=DIEDRE;

USE ROLE USERADMIN;
SHOW USERS;


The listing of all users can be viewed by using the role in the hierarchy just above the USERADMIN role,

USE ROLE SECURITYADMIN;
SHOW USERS;


Snowflake System-Defined Roles

privileges can be assigned to role and roles are assigned to users. Assigned privileges do not come with the GRANT option, the ability to grant the assigned privilege to another role, unless specifically assigned. Roles can also be granted to other roles, which creates a role hierarchy. With this role hierarchy structure, privileges arealso inherited by all roles above a particular role in the hierarchy.

A usercan have multiple roles and can switch between roles, though only onerole can be active in one point of time. The exception would beif the USE SECONDARY ROLES option is used in an active session.

the account administrator (ACCOUNTADMIN) isat the top level of system-defined account roles and can view and operateon all objects in the account, with one exception. The account administra-tor will have no access to an object when the object is created by a customrole without an assigned system-defined role.

The ACCOUNTADMIN role can stop any running SQL statements and canview and manage Snowflake billing. Privileges for resource monitors areunique to the ACCOUNTADMIN role. None of the inherent privileges thatcome with the resource monitor privileges come with the GRANT option,but the ACCOUNTADMIN can assign the ALTER RESOURCE MONITOR privi-lege to another role. It is a best practice to limit the ACCOUNTADMIN roleto the minimum number of users required to maintain control of theSnowflake account, but to no fewer than two users.

Snowflake has no concept of a super user or a super role.All access to securableobjects, even by the account administrator, requires access privileges granted ex-plicitly when the ACCOUNTADMIN creates objects itself, or implicitly by being in ahigher hierarchy role. As such, if a custom role is created without assignment toanother role in a hierarchy that ultimately leads to the ACCOUNTADMIN role, anysecurable objects created by that role would be inaccessible by the ACCOUNTADMIN role.

In addition to ACCOUNTADMIN, other system-defined roles can managesecurity for objects in the system by using the inherent privileges as-signed to them. Those privileges can be granted to other roles inSnowflake, including custom roles, to assign the responsibility for manag-ing security in the system.	

The security administrator (SECURITYADMIN) is inherently given the MANAGE GRANTS privilege and also inherits all the privileges of theUSERADMIN role. The user administrator (USERADMIN) is responsible forcreating and managing users and roles.

The system administrator (SYSADMIN) role is a system-defined role withprivileges to create virtual warehouses, databases, and other objects inthe Snowflake account. The most common roles created by theUSERADMIN are assigned to the SYSADMIN role, thus enabling the systemor account administrator to manage any of the objects created by customroles.

It is important to remember that any privileges or object access provided to thePUBLIC role is available to all roles and all users in the account.

Creating and managing custom roles is one of the most important func-tions in Snowflake. Before creating custom roles, planning should be un-dertaken to design a custom role architecture that will secure and protectsensitive data but will not be unnecessarily restrictive. The approach I’ve taken is to divide the custom roles into functional-level cus-tom roles, which include business and IT roles, and system-level custom roles, which include service account roles and object access roles.

USE ROLE USERADMIN;
SHOW ROLES;

USE ROLE SECURITYADMIN;
SHOW ROLES;

Functional-Level Business and IT Roles

use suffixes todifferentiate between levels, such as Sr for senior analyst and Jr for ju-nior analyst. A decision was also made to use prefixes for roles in whichdifferent environments might be needed, such as SBX for Sandbox, DEV for Development, and PRD for Production.

USE ROLE USERADMIN;
CREATE OR REPLACE ROLE DATA_SCIENTIST;
CREATE OR REPLACE ROLE ANALYST_SR;
CREATE OR REPLACE ROLE ANALYST_JR;
CREATE OR REPLACE ROLE DATA_EXCHANGE_ASST;
CREATE OR REPLACE ROLE ACCOUNTANT_SR;
CREATE OR REPLACE ROLE ACCOUNTANT_JR;
CREATE OR REPLACE ROLE PRD_DBA;
CREATE OR REPLACE ROLE DATA_ENGINEER;
CREATE OR REPLACE ROLE DEVELOPER_SR;
CREATE OR REPLACE ROLE DEVELOPER_JR;
SHOW ROLES;

By using the SHOW command, you’ll find that none of the custom roleshas been granted to other roles. It is otherwise important to have all cus-tom roles assigned to another role in the hierarchy with the top customrole being assigned to either the SYSADMIN role or the ACCOUNTADMIN role, unless there is a business need to isolate a custom role. Later in this chapter we’ll complete the hierarchy of custom roles by assigning the cus-tom roles to a system-defined roles.

System-Level Service Account and Object AccessRoles

I’ve created two differenttypes of roles. The service account roles are typically roles used for load-ing data or connecting to visualization tools. The object access roles areroles for which data access privileges, such as the ability to view the datain a particular schema or to insert data into a table, will be granted. Theseobject access roles then will be assigned to other roles higher in thehierarchy.

USE ROLE USERADMIN;
CREATE OR REPLACE ROLE LOADER;
CREATE OR REPLACE ROLE VISUALIZER;
CREATE OR REPLACE ROLE REPORTING;
CREATE OR REPLACE ROLE MONITORING;

Next, we’ll create the system object access roles:

USE ROLE USERADMIN;
CREATE OR REPLACE ROLE DB1_SCHEMA1_READONLY;
CREATE OR REPLACE ROLE DB1_SCHEMA1_ALL;
CREATE OR REPLACE ROLE DB2_SCHEMA1_READONLY;
CREATE OR REPLACE ROLE DB2_SCHEMA1_ALL;
CREATE OR REPLACE ROLE DB2_SCHEMA2_READONLY;
CREATE OR REPLACE ROLE DB2_SCHEMA2_ALL;
CREATE OR REPLACE ROLE RM1_MODIFY;
CREATE OR REPLACE ROLE WH1_USAGE;
CREATE OR REPLACE ROLE WH2_USAGE;
CREATE OR REPLACE ROLE WH3_USAGE;
CREATE OR REPLACE ROLE DB1_MONITOR;
CREATE OR REPLACE ROLE DB2_MONITOR;
CREATE OR REPLACE ROLE WH1_MONITOR;
CREATE OR REPLACE ROLE WH2_MONITOR;
CREATE OR REPLACE ROLE WH3_MONITOR;
CREATE OR REPLACE ROLE RM1_MONITOR;

Role Hierarchy Assignments: AssigningRoles to Other Roles

complete the system-level role hierarchy assignments

USE ROLE USERADMIN;
GRANT ROLE RM1_MONITOR TO ROLE MONITORING;
GRANT ROLE WH1_MONITOR TO ROLE MONITORING;
GRANT ROLE WH2_MONITOR TO ROLE MONITORING;
GRANT ROLE WH3_MONITOR TO ROLE MONITORING;
GRANT ROLE DB1_MONITOR TO ROLE MONITORING;
GRANT ROLE DB2_MONITOR TO ROLE MONITORING;
GRANT ROLE WH3_USAGE TO ROLE MONITORING;
GRANT ROLE DB1_SCHEMA1_ALL TO ROLE LOADER;
GRANT ROLE DB2_SCHEMA1_ALL TO ROLE LOADER;
GRANT ROLE DB2_SCHEMA2_ALL TO ROLE LOADER;
GRANT ROLE WH3_USAGE TO ROLE LOADER;
GRANT ROLE DB2_SCHEMA1_READONLY TO ROLE VISUALIZER;
GRANT ROLE DB2_SCHEMA2_READONLY TO ROLE VISUALIZER;
GRANT ROLE WH3_USAGE TO ROLE VISUALIZER;
GRANT ROLE DB1_SCHEMA1_READONLY TO ROLE REPORTING;
GRANT ROLE DB2_SCHEMA1_READONLY TO ROLE REPORTING;
GRANT ROLE DB2_SCHEMA2_READONLY TO ROLE REPORTING;
GRANT ROLE WH3_USAGE TO ROLE REPORTING;
GRANT ROLE MONITORING TO ROLE ACCOUNTANT_SR;
GRANT ROLE LOADER TO ROLE DEVELOPER_SR;
GRANT ROLE VISUALIZER TO ROLE ANALYST_JR;
GRANT ROLE REPORTING TO ROLE ACCOUNTANT_JR;
GRANT ROLE RM1_MODIFY TO ROLE ACCOUNTANT_SR;

Completing the functional-level role hierarchy assignments means we’llalso want to assign the top-level custom role to either the SYSADMIN or ACCOUNTADMIN role as a final step in the hierarchy assignment process.

USE ROLE USERADMIN;
GRANT ROLE ACCOUNTANT_JR TO ROLE ACCOUNTANT_SR;
GRANT ROLE ANALYST_JR TO ROLE ANALYST_SR;
GRANT ROLE ANALYST_SR TO ROLE DATA_SCIENTIST;
GRANT ROLE DEVELOPER_JR TO ROLE DEVELOPER_SR;
GRANT ROLE DEVELOPER_SR TO ROLE DATA_ENGINEER;
GRANT ROLE DATA_ENGINEER TO ROLE PRD_DBA;
GRANT ROLE ACCOUNTANT_SR TO ROLE ACCOUNTADMIN;
GRANT ROLE DATA_EXCHANGE_ASST TO ROLE ACCOUNTADMIN;
GRANT ROLE DATA_SCIENTIST TO ROLE SYSADMIN;
GRANT ROLE PRD_DBA TO ROLE SYSADMIN;

Next, be sure to grant usage of virtual warehouse VW2_WH directly to ITroles and usage of virtual warehouse VW2_WH to business roles

GRANT ROLE WH1_USAGE TO ROLE DEVELOPER_JR;
GRANT ROLE WH1_USAGE TO ROLE DEVELOPER_SR;
GRANT ROLE WH1_USAGE TO ROLE DATA_ENGINEER;
GRANT ROLE WH1_USAGE TO ROLE PRD_DBA;
GRANT ROLE WH2_USAGE TO ROLE ACCOUNTANT_JR;
GRANT ROLE WH2_USAGE TO ROLE ACCOUNTANT_SR;
GRANT ROLE WH2_USAGE TO ROLE DATA_EXCHANGE_ASST;
GRANT ROLE WH2_USAGE TO ROLE ANALYST_JR;
GRANT ROLE WH2_USAGE TO ROLE ANALYST_SR;
GRANT ROLE WH2_USAGE TO ROLE DATA_SCIENTIST;

Granting Privileges to Roles

Snowflake privileges are a defined level of access to a securable object.The granularity of access can be granted by using different distinct privi-leges. Privileges can also be revoked if necessary. Each securable objecthas a specific set of privileges which can be granted on it. For existing ob-jects, these privileges must be granted on the individual object. To make grant management more flexible and simpler, future grants can be as-signed on objects created in a schema.

The ACCOUNTADMIN role is required to grant direct global privileges tothe functional roles. We also will need to use the ACCOUNTADMIN role togrant privileges to custom roles that only the ACCOUNTADMIN can grant

USE ROLE ACCOUNTADMIN;
GRANT CREATE DATA EXCHANGE LISTING ON ACCOUNT TO ROLE DATA_EXCHANGE_ASST;
GRANT IMPORT SHARE ON ACCOUNT TO ROLE DATA_EXCHANGE_ASST;
GRANT CREATE SHARE ON ACCOUNT TO ROLE DATA_EXCHANGE_ASST;
GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE MONITORING;
GRANT MONITOR ON RESOURCE MONITOR MONITOR1_RM TO ROLE MONITORING;
GRANT MONITOR USAGE ON ACCOUNT TO ROLE ACCOUNTANT_JR;
GRANT APPLY MASKING POLICY ON ACCOUNT TO ROLE ACCOUNTANT_SR;
GRANT MONITOR EXECUTION ON ACCOUNT TO ROLE ACCOUNTANT_SR;
GRANT MODIFY ON RESOURCE MONITOR MONITOR1_RM TO ROLE ACCOUNTANT_SR;

A number of different custom roles need privileges to interact with datain objects and need the ability to use a virtual warehouse to make that in-teraction. For the ability to view data in a table, a role needs privileges touse the database and schema in which the table resides as well as theability to use the SELECT command on the table.

The privileges will be as-signed for any existing objects in the schema when these privileges aregranted. We’ll also want to consider assigning FUTURE GRANT privilegesso that the role can access tables created in the future. Future grants can only be assigned by the ACCOUNTADMIN;
therefore, we’ll have to assignfuture grant access in a later step.

object monitoring privi-lege is set at the database level;
thus, the role will be able to monitor data-bases we created as well as all objects below the databases in thehierarchy:

USE ROLE SYSADMIN;
GRANT USAGE ON DATABASE DB1 TO ROLE DB1_SCHEMA1_READONLY;
GRANT USAGE ON DATABASE DB2 TO ROLE DB2_SCHEMA1_READONLY;
GRANT USAGE ON DATABASE DB2 TO ROLE DB2_SCHEMA2_READONLY;
GRANT USAGE ON SCHEMA DB1.DB1_SCHEMA1 TO ROLE DB1_SCHEMA1_READONLY;
GRANT USAGE ON SCHEMA DB2.DB2_SCHEMA1 TO ROLE DB2_SCHEMA1_READONLY;
GRANT USAGE ON SCHEMA DB2.DB2_SCHEMA2 TO ROLE DB2_SCHEMA2_READONLY;
GRANT SELECT ON ALL TABLES IN SCHEMA DB1.DB1_SCHEMA1 TO ROLE DB1_SCHEMA1_READONLY;
GRANT SELECT ON ALL TABLES IN SCHEMA DB2.DB2_SCHEMA1 TO ROLE DB2_SCHEMA1_READONLY;
GRANT SELECT ON ALL TABLES IN SCHEMA DB2.DB2_SCHEMA2 TO ROLE DB1_SCHEMA1_READONLY;
GRANT ALL ON SCHEMA DB1.DB1_SCHEMA1 TO ROLE DB1_SCHEMA1_ALL;
GRANT ALL ON SCHEMA DB2.DB2_SCHEMA1 TO ROLE DB2_SCHEMA1_ALL;
GRANT ALL ON SCHEMA DB2.DB2_SCHEMA2 TO ROLE DB2_SCHEMA2_ALL;
GRANT MONITOR ON DATABASE DB1 TO ROLE DB1_MONITOR;
GRANT MONITOR ON DATABASE DB2 TO ROLE DB2_MONITOR;
GRANT MONITOR ON WAREHOUSE VW1_WH TO ROLE WH1_MONITOR;
GRANT MONITOR ON WAREHOUSE VW2_WH TO ROLE WH2_MONITOR;
GRANT MONITOR ON WAREHOUSE VW3_WH TO ROLE WH3_MONITOR;
GRANT USAGE ON WAREHOUSE VW1_WH TO WH1_USAGE;
GRANT USAGE ON WAREHOUSE VW2_WH TO WH2_USAGE;
GRANT USAGE ON WAREHOUSE VW3_WH TO WH3_USAGE;

Use the ACCOUNTADMIN role to grant FUTURE direct assigned privileges

USE ROLE ACCOUNTADMIN;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DB1.DB1_SCHEMA1 TO ROLE DB1_SCHEMA1_READONLY;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DB2.DB2_SCHEMA1 TO ROLE DB2_SCHEMA1_READONLY;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DB2.DB2_SCHEMA2 TO ROLE DB2_SCHEMA2_READONLY;

GRANT SELECT ON FUTURE TABLES IN SCHEMA DB1.DB1_SCHEMA1 TO ROLE DB1_SCHEMA1;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DB2.DB2_SCHEMA1 TO ROLE DB2_SCHEMA1;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DB2.DB2_SCHEMA2 TO ROLE DB2_SCHEMA2;

Assigning Roles to Users

let’s assign roles to eachof those four users. It is possible to assign more than one role to eachuser, but only one role can be used at any given time by the user. Remember that for any role that is as-signed for which there are roles below it in the hierarchy, the user has al-ready inherited that role. For example, the Data Scientist role inherits theAnalyst Sr and Analyst Jr roles and will see those roles in their account.Accordingly, it would be redundant to assign either of those two roles to auser who is assigned the Data Scientist role:

USE ROLE USERADMIN;
GRANT ROLE DATA_EXCHANGE_ASST TO USER USER1;
GRANT ROLE DATA_SCIENTIST TO USER USER2;
GRANT ROLE ACCOUNTANT_SR TO USER USER3;
GRANT ROLE PRD_DBA TO USER USER4;

When running any of the queries using any of the custom roles, you’ll need tohave a running virtual warehouse to complete the queries. If, at any time, you re-ceive an error message that there is no running virtual warehouse, you can al-ways use the SHOW command to find a list of available virtual warehouses for thatrole and the USE command to get the virtual warehouse running.

USE ROLE ACCOUNTANT_JR;

SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE QUERY_TYPE like 'GRA%';

You may want to refresh the Snowsight web UI screen or log out and log back inbefore attempting to test the custom roles.

USE ROLE ACCOUNTANT_SR;
USE WAREHOUSE VW2_WH;

SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE QUERY_TYPE like 'GRAN%';

However, we used a future grants option earlier to grant access to any fu-ture objects, like tables, that we created in the three schemas. Therefore,no action is needed to assign privileges on a newly created table.

USE ROLE SYSADMIN;
SHOW DATABASES;

USE SCHEMA DB1_SCHEMA1;
SHOW TABLES;

CREATE OR REPLACE TABLE DB1.DB1_SCHEMA1.TABLE1 ( a varchar );

INSERT INTO TABLE1 VALUES ( 'A' );

SHOW TABLES;

USE ROLE REPORTING;
USE WAREHOUSE VW3_WH;
SELECT * FROM DB1.DB1_SCHEMA1.TABLE1;

USE ROLE VISUALIZER;

SELECT * FROM DB1.DB1_SCHEMA1.TABLE1;
  -- Gives error cannot find table1

USE ROLE ACCOUNTANT_SR;
USE WAREHOUSE VW3_WH;
SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.GRANTS_TO_USERS;
SHOW GRANTS ON ACCOUNT;
SHOW GRANTS ON DATABASE DB1;
SHOW GRANTS OF ROLE ANALYST_SR;
SHOW FUTURE GRANTS IN DATABASE DB1;
SHOW FUTURE GRANTS IN SCHEMA DB1.DB1_SCHEMA1;

Any of the global privileges, privileges for account objects, and privilegesfor schemas can be revoked from a role. As an example, we’ll have the ac-count administrator grant a role to the junior analyst and then the USERADMIN will revoke that role:

USE ROLE ACCOUNTADMIN;
GRANT MONITOR USAGE ON ACCOUNT TO ROLE ANALYST_JR;
USE ROLE USERADMIN;
REVOKE MONITOR USAGE ON ACCOUNT FROM ROLE ANALYST_JR;

User Management

A User object in Snowflake stores all the information about a user, includ-ing their login name, password, and defaults. A Snowflake user can be aperson or a program. From previous discussions, we know thatSnowflake users are created and managed by the USERADMIN system-de-fined role. The user name is required and should be unique when creat-ing a user.
At a minimum, you should include some basic details and assign an initial password which you require the user to change at the next login.

USE ROLE USERADMIN;
CREATE OR REPLACE USER USER10 PASSWORD='123' LOGIN_NAME=ABARNETT DISPLAY_NAME=AMY FIRST_NAME=AMY LAST_NAME=BARNETT EMAIL='ABARNETT@COMPANY.COM' MUST_CHANGE_PASSWORD=TRUE;

USE ROLE USERADMIN; 
ALTER USER USER10 SET DAYS_TO_EXPIRY=30;


Adding a default virtual warehouse for the user does not verify that the virtualwarehouse exists. We never created a WAREHOUSE52, yet the code to assign thatvirtual warehouse as a default will be executed successfully with no errors or warning:

USE ROLE USERADMIN;

ALTER USER USER10 SET DEFAULT_WAREHOUSE=WAREHOUSE52_WH;

We also can attempt to assign to a user a default role that does exist:

USE ROLE USERADMIN;

ALTER USER USER10 SET DEFAULT_ROLE=IMAGINARY_ROLE;

In other words, setting a default role does not assign the role tothe user. The same thing applies for access to a database. When we attempt to assign defaults that have not previously been assigned to a user, no warning is given, but the command is not actually executed successfully.

When you create a new password and attempt to log in, you’ll receive anerror message because there is a default role set for ABARNETT thathasn’t been granted to the user. As such, login will fail.

USE ROLE USERADMIN;
GRANT ROLE ACCOUNTANT_SR TO USER USER10;
ALTER USER USER10 SET DEFAULT_NAMESPACE=SNOWFLAKE.ACCOUNT_USAGE;
ALTER USER USER10 SET DEFAULT_WAREHOUSE=VW2_WH;
ALTER USER USER10 SET DEFAULT_ROLE=ACCOUNTANT_SR;
ALTER USER USER10 UNSET DEFAULT_WAREHOUSE;

A common user management problem is users who are unable to log in totheir account. The Snowflake system will lock out a user who failed to login successfully after five consecutive attempts. The Snowflake system willautomatically clear the lock after 15 minutes. If the user cannot wait, thetimer can immediately be reset:

USE ROLE USERADMIN;

ALTER USER USER10 SET MINS_TO_UNLOCK=0;

USE ROLE USERADMIN;

ALTER USER USER10 SET PASSWORD='123' MUST_CHANGE_PASSWORD=TRUE;
 -- 123 is not accepted password

USE ROLE USERADMIN;

ALTER USER USER10 SET PASSWORD='123456Aa' MUST_CHANGE_PASSWORD=TRUE;

USE ROLE SECURITYADMIN;

ALTER USER USER10 SET PASSWORD = '123456Bb' MUST_CHANGE_PASSWORD = TRUE;

Keep in mind that because the USERADMIN role rolls up to the SECURITYADMIN role, which itself rolls up to the ACCOUNTADMIN role,the two higher roles can also run any command that the USERADMIN role can run.

There may be a time when it is necessary to abort a user’s currently run-ning queries and prevent the person from running any new queries. Toaccomplish this and to immediately lock the user out of Snowflake,

USE ROLE USERADMIN;

ALTER USER USER10 SET DISABLED = TRUE;

To describe an i ndividual user and get a listing of all the user’s propertyvalues and default values, use the DESCRIBE command

USE ROLE USERADMIN;

DESC USER USER10;

USE ROLE USERADMIN;
ALTER USER USER10 SET DEFAULT_WAREHOUSE = DEFAULT;
USE ROLE USERADMIN;
DESC USER USER10;

The SNOWFLAKE user which you see as securityadmin is a special userthat is only used by Snowflake support with the permission of the ACCOUNTADMIN when there is a need to troubleshoot account issues.

It is possible to delete the SNOWFLAKE user, but once it’s deleted, you can’t simply create another SNOWFLAKE user that can be used for support. Therefore, it is highly recommended that you do not delete this user.

Wildcards are supported when using the SHOW command. You can usethe LIKE command with an underscore to match any single characterand with a percent sign to match any sequence of zero or more charac-ters.

USE ROLE SECURITYADMIN;

SHOW USERS LIKE 'USER%';

USE ROLE USERADMIN;

DROP USER USER10;

However, ownership of those objects tables or views created by the dropped user (for which he is owner) are transferred to the ACCOUNTADMIN role automatically.

The ACCOUNTADMIN has access to a list of all users by querying theSNOWFLAKE.ACCOUNT_USAGE.USERS table, including those users who havebeen deleted.

Role Management

Previously, it was mentioned that a default role can be designated for anyuser. When a user first logs in, their primary role is whatever default rolewas assigned to them. A user’s current role in a session is considered their primary role. There are only primary and secondary roles: any role not being used as the primary is a secondary role.

Secondary roles can be handled in different ways. One way is to keep allroles separate so the user can only use one role at a time. Another way isto grant a secondary role to a primary role, in effect creating a persistent access layering of a specific secondary role to a primary role for everyonewho uses the primary role.

If,instead, some secondary roles should be granted only to select users, thenit is possible to implement a session-based access layering of all secondaryroles by employing the USE SECONDARY ROLES statement.

Snowflake Multi-Account Strategy

There are many times, however, when it makes sense to have amulti-account strategy to take advantage of the separation of accounts. One use case for needing multiple accounts is to use different accountsfor different environments, such as development, testing, and production.While it is certainly possible to use a single account to accomplish separa-tion of environments, choosing a multi-account approach means differentenvironments can have different security features selected. This makes itpossible to balance differently how productivity versus security risk ishandled for each account. There is also the ability to save money by usinga higher-grade feature set only for production and not for developmentor testing. This is possible when using a multi-account strategy.

Sometimes a company faces regulatory constraints that prevent it fromusing a specific cloud provider or region in one subsidiary. In that case, anew Snowflake account can be created for that particular subsidiary whereby an acceptable cloud provider and region can be selected. Othertimes, multi-account choices are made for many different strategic rea-sons. Many of today’s organizations are global and need their data locatedgeographically in certain regions. Sometimes organizations opt for a mul-ticloud approach to reduce dependency on a single vendor or to make iteasier to merge with an acquired company that operates on a differentcloud provider.

Snowflake makes it easy for companies to adopt a multi-account strategywith the Snowflake organization object and the ORGADMIN role. ASnowflake organization simplifies account management and billing, data-base replication and failover/failback, Secure Data Sharing, and other ad-ministrative tasks.

An ORGADMIN can create a new Snowflake account and can view account properties. However, an ORGADMIN does not have access to the account data by default.

It takes approximately 30 seconds for the DNS changes to propagate be-fore you can access a newly created Snowflake account. The maximumnumber of accounts in a Snowflake organization cannot exceed 25 by de-fault; however, you can contact Snowflake support to have the limitraised.

Managing Users and Groups with SCIM

Snowflake supports SCIM 2.0 for integration with identity providers Oktaand Microsoft Azure AD as well as with other identity providers which re-quire customization. 

The specific Snowflake SCIM role must own any users and roles that are im-ported from the identity provider. If the Snowflake SCIM role does not own theimported users or roles, updates in the identity provider will not be synced toSnowflake. Also, ensure that you do not make user and group changes inSnowflake if using an identity provider, because those changes made directly inSnowflake will not synchronize back to the identity provider.

USE ROLE SYSADMIN;
DROP DATABASE DB1;
DROP DATABASE DB2;
SHOW DATABASES;
DROP WAREHOUSE VW1_WH;
DROP WAREHOUSE VW2_WH;
DROP WAREHOUSE VW3_WH;
SHOW WAREHOUSES;

USE ROLE ACCOUNTADMIN;
DROP RESOURCE MONITOR MONITOR1_RM;
SHOW RESOURCE MONITORS;

USE ROLE USERADMIN;
DROP ROLE DATA_SCIENTIST;
DROP ROLE ANALYST_SR;
DROP ROLE ANALYST_JR;
DROP ROLE DATA_EXCHANGE_ASST;
DROP ROLE ACCOUNTANT_SR;
DROP ROLE ACCOUNTANT DROP ROLE PRD_DBA;
DROP ROLE DATA_ENGINEER;
DROP ROLE DEVELOPER_SR;
DROP ROLE DEVELOPER_JR;
DROP ROLE LOADER;
DROP ROLE VISUALIZER;
DROP ROLE REPORT DROP ROLE MONITORING;
DROP ROLE RM1_MODIFY;
DROP ROLE WH1_USAGE;
DROP ROLE WH2_USAGE;
DROP ROLE WH3_USAGE;
DROP ROLE DB1_MONITOR;
DROP ROLE DB2_MONITOR;
DROP ROLE WH1_MONITOR;
DROP ROLE WH2_MONITOR;
DROP ROLE WH3_MONITOR;
DROP ROLE RM1_MONITOR;
DROP ROLE DB1_SCHEMA1_READONLY;
DROP RO DB1_SCHEMA1_ALL;
DROP ROLE DB2_SCHEMA1_READONLY;
DROP ROLE DB2_SCHEMA1_ALL;
DROP ROLE DB2_SCHEMA2_READONLY;
DROP ROLE DB2_SCHEMA2_ALL;
SHOW ROLES;

USE ROLE USERADMIN;
DROP USER USER1;
DROP USER USER2;
DROP USER USER3;
DROP USER USER4;

USE ROLE SECURITYADMIN;
SHOW USERS;

Chapter 6. Data Loading and Unloading

some alternatives to dataloading, such as creating materialized views on external stages and ac-cessing shared data.

USE ROLE SYSADMIN;
USE WAREHOUSE COMPUTE_WH;
CREATE OR REPLACE DATABASE DEMO6_DB COMMENT = "Database for all Chapter 6 Examples";
CREATE OR REPLACE SCHEMA WS COMMENT = "Schema for Worksheet Insert Examples CREATE OR REPLACE SCHEMA UI COMMENT = "Schema for Web UI Uploads";
CREATE OR REPLACE SCHEMA SNOW COMMENT = "Schema for SnowSQL Loads";
CREATE OR REPLACE WAREHOUSE LOAD_WH COMMENT = "Warehouse for CH 6 Load Examples";

Basics of Data Loading and Unloading

Snowflake provides flexible schema data types for loading semi-struc-tured data. No transformations are needed prior to loading semi-struc-tured data because Snowflake automatically converts semi-structureddata to allow for SQL querying of the data in a fully relational manner.

VARIANT datatype can store the values of any other Snowflake data type, including OBJECT and ARRAY.Thus, VARIANT is considered a universal data type.

The VARIANT data type has a size limit of 16 MB of compressed data. Therefore, itis recommended that you flatten your object and key data into separate relationalcolumns if your semi-structured data stored in a VARIANT data type column in-cludes arrays, numbers with strings, or dates and timestamps. This is especiallyimportant for dates and timestamps loaded into a VARIANT data type, becausethose data types end up being stored as strings in a VARIANT column.

The Snowflake VARIANT data type is a good choice if you are not yet surewhat types of operations you want to perform on the semi-structureddata you load into Snowflake. The Snowflake OBJECT data type represents collections of key-valuepairs, where the key is a nonempty string and the values are of a VARIANT data type. The Snowflake ARRAY data type represents dense orsparse arrays of an arbitrary size, where the index is a non-negative inte-ger and the values are of a VARIANT data type.

File Formats

File formats are Snowflake objects that can be used with COPY com-mands. A file format defines the data file with parameters such as com-pression and file type. It also defines format options such as trim spaceand field delimiter for CSV files, and strip outer array for JSON files.

Snowflake-supported file format types for data unloading include JSONand Parquet for semi-structured data, and delimited file formats such asCSV and TSV for structured data. Data loading file format types supportedby Snowflake include the same format types for unloading, plus XML,Avro, and ORC.

When loading data into Snowflake from delimited files such as CSV orTSV files, the default character set is UTF-8; other character sets are ac-cepted for delimited files, but they are converted by Snowflake to UTF-8before being stored in tables. For all other supported file formats, UTF-8 is the only supported character set when loading data. For unloading data,UTF-8 is the only supported character set no matter the file format type.

Snowflake supports both the Newline delimited JSON (NDJSON) standard format,and the comma-separated JSON format for loading data into Snowflake tables.When unloading to files, Snowflake outputs only to the NDJSON format.

Creating file formats for Snowflake data loading is optional. It is recom-mended that you create file format objects if you foresee that you’ll beable to reuse them with COPY commands for loading similarly structuredfiles .

Data File Compression

You can load compressed and uncompressed data into Snowflake. Alldata stored in Snowflake by default is compressed using gzip, without youhaving to select the columns or choose a compression algorithm. You cancompress your data files prior to loading the data into Snowflake; it is rec-ommended that you compress your files if they are large in size.Snowflake supports the GZIP, BZIP2, DEFLATE, RAW_DEFLATE, BROTLI,and ZSTANDARD (ZSTD) compression methods, and can automatically de-tect all of them except for BROTLI and ZSTANDARD (ZSTD). For those com-pression methods, you’ll need to specify the compression method at thetime of data load.

Snowflake also supports the Lempei-Ziv-Oberhumer (LZO) and SNAPPY compres-sion methods for the Parquet file format type.

Frequency of Data Processing

Batch processing : 

data freshness is not mission critical, and anytime youare working with complex algorithms on large data sets may be good usecases for batch processing. It’s less expensive because compute resources are only usedwhen the processing is executed, and since batch processes execute lessfrequently than near–real-time processing, the overall cost is less.

Streaming, continuous loading, and micro-batch processing

The terms streaming, stream processing, continuous loading, near–real-time processing, and micro-batch processing are often used interchange-ably because in practice they achieve similar results. Continuous loading,near–real-time processing, and micro-batch processing are synonymous;however, there are differences between those three terms and the terms streaming and stream processing .

For micro-batch processing implementations, batch processes are exe-cuted on small amounts of data which can be processed in less than 60seconds. If the data routinely takes longer than 60 seconds to load, youmay want to consider reducing the size of each batch. The time betweenloading each micro-batch could be greater than 60 seconds but is usuallyno more than a few minutes between loads. This achieves near–real-timeresults.

In contrast, pure-play stream processing solutions would use tools such asa Kafka Streams API or Confluent’s KSQL in situations in which actualreal-time data is critical. security situations in which data applications provide immediatefraud detection capabilities, and operational situations in which it is im-portant to have a real-time view of ecommerce data or Internet of Things(IoT) data, such as security cameras or patient medical devices.

Snowflake Stage References

Snowflake stages are temporary storage spaces used as an intermediatestep to lead files to Snowflake tables or to unload data from Snowflake ta-bles into files. There are two main types of stages: internal and external.

external stages, the files are stored in an external location such as anS3 bucket, and they are referenced by the external stage. Access to these external locations was previously managed through cloud identity andaccess management (IAM) roles and access control lists (ACLs). Today,however, the best practice is to create a storage integration. A storage in-tegration is a Snowflake object that stores a generated IAM entity for ex-ternal cloud storage and optionally includes allowed or blocked storagelocations for Amazon S3, Google Cloud Storage, or Microsoft Azure.

Internal stage types include internal named stages, user stages, and tablestages. 

Note that all stage types in Snowflake are referred to using the @symbol. SQL statements in which a named stage is referenced will need the @ symbol along with the name of the stage.

Named stages : Internal named stages are database objects; thus, they can be used by anyuser who has been granted a role with the appropriate privileges.  To list files in named stages, you can run the

LIST @<stage name>;

User stages : Each Snowflake user has a stage for storing files which is accessible onlyby that user. A user stage is not a separate database object, and it cannotbe altered or dropped. SQL statements in whicha user stage is referenced will need the @~ symbols.

LIST @~;

Table stages : A table stage is not a separate database object; rather, it is an implicitstage tied to the table itself. Just like user stages, a table stage cannot be altered or dropped. Additionally, the data in table stages is accessible onlyto those Snowflake roles which have been granted the privileges to readfrom the table. SQL statements in which a table stage is referenced willneed the @% symbols as well as the name of the table. 

LIST @%<name oftable>;

Data Sources

Data can be loaded into Snowflake from a local file system, Amazon S3 bucket, Azure container, or GCP bucket. you can load data into Snowflake from virtually any data source, including files, APIs, enterprise applica-tions, and databases.

Data Loading Tools

five different ways to load data into Snowflake. 

SQL worksheets
Web UI Snowsight
SQL command line
data pipelines
third-party ETL tools

Snowflake Worksheet SQL Using INSERT INTO and INSERT ALL Commands

when you have just a few rows of data. 

USE WAREHOUSE LOAD_WH;
USE DATABASE DEMO6_DB;
USE SCHEMA WS;

Single-row inserts for structured and semi-structured data

CREATE OR REPLACE TABLE TABLE1 ( id integer, f_name string, l_name string, city string ) COMMENT = "Single-Row Insert for Structured Data using Explicitly Specified Values";

INSERT INTO TABLE1 ( id, f_name, l_name, city ) 
VALUES ( 1, 'Anthony', 'Robinson', 'Atlanta' ); 
SELECT * FROM TABLE1;

INSERT INTO TABLE1 ( id, f_name, l_name, city ) 
VALUES ( 2, 'Peggy', 'Mathison', 'Birmingham' ); 
SELECT * FROM TABLE1;

+----+---------+----------+------------+
| ID | F_NAME  | L_NAME   | CITY       |
|----+---------+----------+------------|
|  1 | Anthony | Robinson | Atlanta    |
|  2 | Peggy   | Mathison | Birmingham |
+----+---------+----------+------------+

Semi-structured data

CREATE OR REPLACE TABLE TABLE2 ( id integer, variant1 variant ) COMMENT = "Single-Row Insert for Semi-Structured JSON Data";

INSERT INTO TABLE2 ( id, variant1 ) 
SELECT 1, parse_json ( '{"f_name": "Anthony", "l_name": "Robinson", "city": "Atlanta" }' ); 
SELECT * FROM TABLE2;

INSERT INTO TABLE2 ( id, variant1 ) 
SELECT 2 ,parse_json ( ' {"f_name": "Peggy", "l_name": "Mathison", "city": "Birmingham" } ' ); 
SELECT * FROM TABLE2;

+----+-------------------------+
| ID | VARIANT1                |
|----+-------------------------|
|  1 | {                       |
|    |   "city": "Atlanta",    |
|    |   "f_name": "Anthony",  |
|    |   "l_name": "Robinson"  |
|    | }                       |
|  2 | {                       |
|    |   "city": "Birmingham", |
|    |   "f_name": "Peggy",    |
|    |   "l_name": "Mathison"  |
|    | }                       |
+----+-------------------------+

Multirow inserts for structured and semi-structured data

CREATE OR REPLACE TABLE TABLE3 ( id integer, f_name string, l_name string, city string ) COMMENT = "Multi-row Insert for Structured Data using Explicitly Stated Val";

INSERT INTO TABLE3 ( id, f_name, l_name, city ) 
VALUES 
( 1, 'Anthony', 'Robinson', 'Atlanta' ), 
( 2, 'Peggy', 'Mathison', 'Birmingham' ); 

SELECT * FROM TABLE3;

+----+---------+----------+------------+
| ID | F_NAME  | L_NAME   | CITY       |
|----+---------+----------+------------|
|  1 | Anthony | Robinson | Atlanta    |
|  2 | Peggy   | Mathison | Birmingham |
+----+---------+----------+------------+

CREATE OR REPLACE TABLE TABLE4 ( id integer, f_name string, l_name string, city string ) COMMENT = "Multi-row Insert for Structured Data using Query, All Columns Same";

INSERT INTO TABLE4 ( id, f_name, l_name, city ) 
   SELECT * FROM TABLE3 WHERE CONTAINS (city, 'Atlanta'); 

SELECT * FROM TABLE4;

CREATE OR REPLACE TABLE TABLE5 ( id integer, f_name string, l_name string, city string ) COMMENT = "Multi-row Insert for Structured Data using Query, Fewer Columns";
 
INSERT INTO TABLE5 ( id, f_name, l_name ) SELECT * FROM TABLE3 WHERE CONTAINS ( city, 'Atlanta'); -- Error
002020 (21S01): SQL compilation error:
Insert value list does not match column list expecting 3 but got 4

INSERT INTO TABLE5 ( id, f_name, l_name ) 
SELECT id, f_name, l_name FROM TABLE3 WHERE CONTAINS ( city, 'Atlanta' ); 

SELECT * FROM TABLE5;
+----+---------+----------+------+
| ID | F_NAME  | L_NAME   | CITY |
|----+---------+----------+------|
|  1 | Anthony | Robinson | NULL |
+----+---------+----------+------+

CREATE OR REPLACE TABLE TABLE6 ( id integer, first_name string, last_name string, city_name string ) COMMENT = "Table to be used as part of next demo";

INSERT INTO TABLE6 ( id, first_name, last_name, city_name ) 
VALUES ( 1, 'Anthony', 'Robinson', 'Atlanta' ), 
( 2, 'Peggy', 'Mathison', 'Birmingham' );

CREATE OR REPLACE TABLE TABLE7 ( id integer, f_name string, l_name string, city string ) COMMENT = "Multi-row Insert for Structured Data using CTE";

INSERT INTO TABLE7 ( id, f_name, l_name, city )
 WITH CTE AS 
   (SELECT id, first_name as f_name, last_name as l_name, city_name as city FROM TABLE6) 
       SELECT id, f_name, l_name, city FROM CTE; 
   
SELECT * FROM TABLE7;
+----+---------+----------+------------+
| ID | F_NAME  | L_NAME   | CITY       |
|----+---------+----------+------------|
|  1 | Anthony | Robinson | Atlanta    |
|  2 | Peggy   | Mathison | Birmingham |
+----+---------+----------+------------+

common table expression (CTE), a named subquery defined in a WITH clause. A CTE functions as a tempo-rary view in a statement, except it does not store the definition in meta-data. CTEs are used to improve code readability, to make it easier tomaintain code, and to take advantage of recursive programming.

CREATE OR REPLACE TABLE TABLE8 ( id integer, f_name string, l_name string, zip_code string ) COMMENT = "Table to be used as part of next demo";

INSERT INTO TABLE8 ( id, f_name, l_name, zip_code ) 
VALUES 
( 1, 'Anthony', 'Robinson', '30301' ), ( 2, 'Peggy', 'Mathison', '35005' );

CREATE OR REPLACE TABLE TABLE9 ( id integer, zip_code string, city string, state string ) COMMENT = "Table to be used as part of next demo";
INSERT INTO TABLE9 ( id, zip_code, city, state ) 
VALUES 
( 1, '30301', 'Atlanta', 'Georgia' ), 
( 2, '35005', 'Birmingham', 'Alabama' );

insert records using an inner join from the two new tables

CREATE OR REPLACE TABLE TABLE10 ( id integer, f_name string, l_name string, city string, state string, zip_code string ) COMMENT = "Multi-row inserts from two tables using an Inner JOIN on zip_code";

INSERT INTO TABLE10 ( id, f_name, l_name, city, state, zip_code ) 
SELECT a.id, a.f_name, a.l_name, b.city, b.state, a.zip_code 
FROM TABLE8 a 
INNER JOIN TABLE9 b 
on a.zip_code = b.zip_code;

SELECT * FROM TABLE10;
+----+---------+----------+------------+---------+----------+
| ID | F_NAME  | L_NAME   | CITY       | STATE   | ZIP_CODE |
|----+---------+----------+------------+---------+----------|
|  1 | Anthony | Robinson | Atlanta    | Georgia | 30301    |
|  2 | Peggy   | Mathison | Birmingham | Alabama | 35005    |
+----+---------+----------+------------+---------+----------+

-- Semi structured data multi record insert 
CREATE OR REPLACE TABLE TABLE11 ( variant1 variant ) COMMENT = "Multi-row Insert for Semi-structured JSON Data" ;

INSERT INTO TABLE11
select parse_json(column1)
from
values
(
'{ "_id": "1",
"name": { "first": "Anthony", "last": "Robinson" },
"company": "Pascal",
"email": "anthony@pascal.com",
"phone": "+1 (999) 444-2222"}'
),
(
'{ "id": "2",
"name": { "first": "Peggy", "last": "Mathison" },
"company": "Ada",
"email": "Peggy@ada.com",
"phone": "+1 (999) 555-3333"}'
);

SELECT * FROM TABLE11 ;
+----------------------------------+
| VARIANT1                         |
|----------------------------------|
| {                                |
|   "_id": "1",                    |
|   "company": "Pascal",           |
|   "email": "anthony@pascal.com", |
|   "name": {                      |
|     "first": "Anthony",          |
|     "last": "Robinson"           |
|   },                             |
|   "phone": "+1 (999) 444-2222"   |
| }                                |
| {                                |
|   "company": "Ada",              |
|   "email": "Peggy@ada.com",      |
|   "id": "2",                     |
|   "name": {                      |
|     "first": "Peggy",            |
|     "last": "Mathison"           |
|   },                             |
|   "phone": "+1 (999) 555-3333"   |
| }                                |
+----------------------------------+

there are two different columns _id and id in those records.

Multitable inserts

It is possible to update multiple tables at a time by inserting one or morerows into the tables using a query statement. The
inserts can be uncondi-tional or conditional. Conditional multitable inserts create inserts by using WHEN-ELSE clauses to determine the table(s) into which each row willbe inserted.

unconditional multitable insert

CREATE OR REPLACE TABLE TABLE12 ( id integer, first_name string, last_name string, city_name string ) COMMENT = "Source Table to be used as part of next demo for Unconditional T Inserts";

INSERT INTO TABLE12 ( id, first_name, last_name, city_name ) 
VALUES 
( 1, 'Anthony', 'Robinson', 'Atlanta' ), 
( 2, 'Peggy', 'Mathison', 'Birmingham');

CREATE OR REPLACE TABLE TABLE13 ( id integer, f_name string, l_name string, city string ) COMMENT = "Unconditional Table Insert - Destination Table 1 for unconditional multi-table insert";

CREATE OR REPLACE TABLE TABLE14 ( id integer, f_name string, l_name string, city string ) COMMENT = "Unconditional Table Insert - Destination Table 2 for unconditional multi-table insert" ;

INSERT ALL 
  INTO TABLE13 
  INTO TABLE13 ( id, f_name, l_name, city ) VALUES ( id, last_name, first_name, default ) 
  INTO TABLE14 ( id, f_name, l_name, city ) 
  INTO TABLE14 VALUES ( id, city_name, last_name, first_name ) 
SELECT id, first_name, last_name, city_name FROM TABLE12 ;
+--------------------------------------+--------------------------------------+
| number of rows inserted into TABLE13 | number of rows inserted into TABLE14 |
|--------------------------------------+--------------------------------------|
|                                    4 |                                    4 |
+--------------------------------------+--------------------------------------+


SELECT * FROM TABLE12;
+----+------------+-----------+------------+
| ID | FIRST_NAME | LAST_NAME | CITY_NAME  |
|----+------------+-----------+------------|
|  1 | Anthony    | Robinson  | Atlanta    |
|  2 | Peggy      | Mathison  | Birmingham |
+----+------------+-----------+------------+

SELECT * FROM TABLE13;
+----+----------+----------+------------+
| ID | F_NAME   | L_NAME   | CITY       |
|----+----------+----------+------------|
|  1 | Anthony  | Robinson | Atlanta    |
|  2 | Peggy    | Mathison | Birmingham |
|  1 | Robinson | Anthony  | NULL       |
|  2 | Mathison | Peggy    | NULL       |
+----+----------+----------+------------+

SELECT * FROM TABLE14;
+----+------------+----------+------------+
| ID | F_NAME     | L_NAME   | CITY       |
|----+------------+----------+------------|
|  1 | Anthony    | Robinson | Atlanta    |
|  2 | Peggy      | Mathison | Birmingham |
|  1 | Atlanta    | Robinson | Anthony    |
|  2 | Birmingham | Mathison | Peggy      |
+----+------------+----------+------------+

-- conditional multitable insert example

CREATE OR REPLACE TABLE TABLE15 ( id integer, first_name string, last_name string, city_name string ) COMMENT = "Source Table to be used as part of next demo for Conditional multi-table Insert";

INSERT INTO TABLE15 ( id, first_name, last_name, city_name ) 
VALUES 
( 1, 'Anthony', 'Robinson', 'Atlanta' ), 
( 2, 'Peggy', 'Mathison', 'Birmingham' ), 
( 3, 'Marshall', 'Baker', 'Chicago' ),
( 4, 'Kevin', 'Cline', 'Denver' ), 
( 5, 'Amy', 'Ranger', 'Everly' ),
( 6, 'Andy', 'Murray', 'Fresno' );

CREATE OR REPLACE TABLE TABLE16 ( id integer, f_name string, l_name string, city string ) COMMENT = "Destination Table 1 for conditional multi-table insert";

CREATE OR REPLACE TABLE TABLE17 ( id integer, f_name string, l_name string, city string ) COMMENT = "Destination Table 2 for conditional multi-table insert" ;

INSERT ALL 
  WHEN id < 5 THEN 
    INTO TABLE16 
  WHEN id < 3 THEN 
    INTO TABLE16 
	INTO TABLE17 
  WHEN id = 1 THEN 
    INTO TABLE16 ( id, f_name ) VALUES ( id, first_name ) 
	  ELSE INTO TABLE17 
SELECT id, first_name, last_name, city_name FROM TABLE15 ;
+--------------------------------------+--------------------------------------+
| number of rows inserted into TABLE16 | number of rows inserted into TABLE17 |
|--------------------------------------+--------------------------------------|
|                                    7 |                                    4 |
+--------------------------------------+--------------------------------------+

SELECT * FROM TABLE16;
+----+----------+----------+------------+
| ID | F_NAME   | L_NAME   | CITY       |
|----+----------+----------+------------|
|  1 | Anthony  | Robinson | Atlanta    |
|  2 | Peggy    | Mathison | Birmingham |
|  3 | Marshall | Baker    | Chicago    |
|  4 | Kevin    | Cline    | Denver     |
|  1 | Anthony  | Robinson | Atlanta    |
|  2 | Peggy    | Mathison | Birmingham |
|  1 | Anthony  | NULL     | NULL       |
+----+----------+----------+------------+

SELECT * FROM TABLE17;
+----+---------+----------+------------+
| ID | F_NAME  | L_NAME   | CITY       |
|----+---------+----------+------------|
|  1 | Anthony | Robinson | Atlanta    |
|  2 | Peggy   | Mathison | Birmingham |
|  5 | Amy     | Ranger   | Everly     |
|  6 | Andy    | Murray   | Fresno     |
+----+---------+----------+------------+

ARRAY_INSERT

The ARRAY_INSERT Snowflake function provides us with a way to directlyinsert data into a table

CREATE OR REPLACE TABLE TABLE18 ( Array variant ) COMMENT = "Insert Array" ;

ARRAY_INSERT ( < array >, < position >, < new element > ) 
WHERE < array > INCLUDES ARRAY_CONSTRUCT ( < values > );

INSERT INTO TABLE18 
SELECT ARRAY_INSERT ( array_construct ( 0, 1, 2, 3 ), 4, 4 );

SELECT * FROM TABLE18;
+-------+
| ARRAY |
|-------|
| [     |
|   0,  |
|   1,  |
|   2,  |
|   3,  |
|   4   |
| ]     |
+-------+

INSERT INTO TABLE18 
SELECT ARRAY_INSERT(array_construct (0, 1, 2, 3), 7, 4);

SELECT * FROM TABLE18; 
+--------------+
| ARRAY        |
|--------------|
| [            |
|   0,         |
|   1,         |
|   2,         |
|   3,         |
|   4          |
| ]            |
| [            |
|   0,         |
|   1,         |
|   2,         |
|   3,         |
|   undefined, |
|   undefined, |
|   undefined, |
|   4          |
| ]            |
+--------------+

the number 4 was inserted into the seventh posi-tion. For positions four through six, there is an undefined value since no values were given for those positions.

OBJECT_INSERT

OBJECT_INSERT is a Snowflake semi-structured data function.

CREATE OR REPLACE TABLE TABLE19 ( Object variant ) COMMENT = "Insert Object" ;

OBJECT_INSERT ( <object> ,<key> ,<value> ) 
WHERE <object> 
INCLUDES OBJECT_CONSTRUCT (<key-value pairs>);

INSERT INTO TABLE19 
SELECT OBJECT_INSERT 
(OBJECT_CONSTRUCT('a', 1, 'b', 2, 'c', 3), 'd', 4 ); 

SELECT * FROM TABLE19 ;
+-----------+
| OBJECT    |
|-----------|
| {         |
|   "a": 1, |
|   "b": 2, |
|   "c": 3, |
|   "d": 4  |
| }         |
+-----------+

INSERT INTO TABLE19 SELECT OBJECT_INSERT ( object_construct ('a', 1, 'b', 2, 'c', 3), 'd', ' ' );    -- not NULL its ' '
INSERT INTO TABLE19 SELECT OBJECT_INSERT ( object_construct ('a', 1, 'b', 2, 'c', 3), 'd', 'null' ); -- not NULL its 'null'
INSERT INTO TABLE19 SELECT OBJECT_INSERT ( object_construct ('a', 1, 'b', 2, 'c', 3), 'd', null );   -- Value can be NULL
INSERT INTO TABLE19 SELECT OBJECT_INSERT ( object_construct ('a', 1, 'b', 2, 'c', 3), null, 'd' );   -- Key is NULL no entry

SELECT * FROM TABLE19 ;
+---------------+
| OBJECT        |
|---------------|
| {             |
|   "a": 1,     |
|   "b": 2,     |
|   "c": 3,     |
|   "d": 4      |
| }             |
| {             |
|   "a": 1,     |
|   "b": 2,     |
|   "c": 3,     |
|   "d": " "    |
| }             |
| {             |
|   "a": 1,     |
|   "b": 2,     |
|   "c": 3,     |
|   "d": "null" |
| }             |
| {             |
|   "a": 1,     |
|   "b": 2,     |
|   "c": 3      |
| }             |
| {             |
|   "a": 1,     |
|   "b": 2,     |
|   "c": 3      |
| }             |
+---------------+

Web UI Load Data Wizard

The wizardis intended only for manually loading a few small files at a time. It is important to adhere to the small file size recommendation when using theSnowflake web UI Load Data wizard. Doing so will help to ensure better perfor-mance. It will also prevent your browser from crashing, because as the file sizeincreases, more memory consumption will be required to encrypt the files. If youneed to load larger files use the Snowflake SnowSQL CLI.

the wizard indirectly loads the data into aSnowflake table by seamlessly combining two phases: one phase to stagethe files and the next phase to load the staged files into a table

To accomplish this, the wizard uses the PUT and COPY commands behindthe scenes. The PUT command places the file(s) in a stage and the COPY command moves the data from the Snowflake stage to the Snowflake ta-ble. When done loading the data into a Snowflake table from the stage,the wizard then deletes all the staged files.

In Snowsight -> Database -> click on the Table you want to load -> Load Data button on top right

Show SQL during file load

COPY INTO "DB1"."DB1_SCHEMA1"."TABLE20"
FROM (
    SELECT $1, $2, $3, $4
    FROM '@"DB1"."DB1_SCHEMA1"."__snowflake_temp_import_files__"'
)
FILES = ('2024-12-03T17:48:36.271Z/table20.txt')
FILE_FORMAT = '"DB1"."DB1_SCHEMA1"."temp_file_format_2024-12-03T17:50:50.631Z"'
ON_ERROR=ABORT_STATEMENT;
-- For more details, see: https://docs.snowflake.com/en/sql-reference/sql/copy-into-table


USE SCHEMA UI;
CREATE OR REPLACE TABLE TABLE20 ( id integer, f_name string, l_name string, city string ) 
COMMENT = "Load Structured Data file via the Web UI wizard" ;

Loading with SnowSQL CLI SQL PUT and COPY INTO Commands

JKAVILA2022 is my username and dx58224.us-central11.gcp is mySnowflake account name.

c:\>snowsql - a dx58224.us-central1.gcp -u JKAVILA2022 < enter > 
Password : ********* < enter >

Once you have created the table, you are ready to use the PUT commandto load the CSV file into the table stage:

@"DEMO6_DB"."SNOW".%"TABLE20" - this is the Table stage area for Table20 table

Put file:///C:\Users\table20.txt @"DB1"."DB1_SCHEMA1".%"TABLE20";

+-------------+----------------+-------------+-------------+--------------------+--------------------+----------+---------+
| source      | target         | source_size | target_size | source_compression | target_compression | status   | message |
|-------------+----------------+-------------+-------------+--------------------+--------------------+----------+---------|
| table20.txt | table20.txt.gz |          71 |         112 | NONE               | GZIP               | UPLOADED |         |
+-------------+----------------+-------------+-------------+--------------------+--------------------+----------+---------+

list @"DB1"."DB1_SCHEMA1".%"TABLE20";
+----------------+------+----------------------------------+------------------------------+
| name           | size | md5                              | last_modified                |
|----------------+------+----------------------------------+------------------------------|
| table20.txt.gz |  112 | 9decc9090f373378f9aa50ca37829e02 | Tue, 3 Dec 2024 18:23:18 GMT |
+----------------+------+----------------------------------+------------------------------+

nesanawsnew#VW3_WH@DB1.DB1_SCHEMA1>list @%TABLE20;
+----------------+------+----------------------------------+------------------------------+
| name           | size | md5                              | last_modified                |
|----------------+------+----------------------------------+------------------------------|
| table20.txt.gz |  112 | 9decc9090f373378f9aa50ca37829e02 | Tue, 3 Dec 2024 18:23:18 GMT |
+----------------+------+----------------------------------+------------------------------+
1 Row(s) produced. Time Elapsed: 0.429s

Next, we’ll copy the files from the table stage into the table

COPY INTO "TABLE20" 
FROM @"DB1"."DB1_SCHEMA1".%"TABLE20" 
file_format=(type = csv);

+----------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+
| file           | status | rows_parsed | rows_loaded | error_limit | errors_seen | first_error | first_error_line | first_error_character | first_error_column_name |
|----------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------|
| table20.txt.gz | LOADED |           4 |           4 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |
+----------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+

select * from TABLE20;  -- Loaded twice once from wizard and once from snowSQL CLI

+----+--------+--------+---------+
| ID | F_NAME | L_NAME | CITY    |
|----+--------+--------+---------|
|  1 | siva   | govind | chennai |
|  2 | mano   | NULL   | cbe     |
|  3 | seema  | p      | ooty    |
|  4 | gana   | baga   | nagar   |
|  1 | siva   | govind | chennai |
|  2 | mano   | NULL   | cbe     |
|  3 | seema  | p      | ooty    |
|  4 | gana   | baga   | nagar   |
+----+--------+--------+---------+

We could just as easily have created an inter-nal named stage and put the files there instead. It is recommended to usean internal named stage if you share files with multiple users or if filesare loaded into multiple tables. While using the COPY INTO command, it is possible to perform basictransformations such as reordering columns or performing casts using a SELECT command.

Loading using Data Pipelines

A data pipeline is a data transportation conduit that uses automated pro-cessing steps to move data to a target repository. A pipeline’s data origin, sometimes referred to as the data pipeline definition, is the final point ofdata collection before the data is moved through the pipeline. The move-ment of the data through the pipeline is known as the dataflow.Pipelinearchitecture can be of either a batch processing or a stream processingtype, with continuous loading architecture somewhere between the two.

TheELT approach is often the preferred approach when the destination is acloud native data warehouse. In an ELT approach, a Snowflake Snowpipe is the extract part. Sometransformations, such as changing column names or changing the orderof columns, can be accomplished, and then the COPY INTO commandloads the data into the target destination.

Semi-structured data types, such as JSON and Avro, are supported by Snowpipe.

To complete the ELT data pipeline using Snowpipe, we would also need touse Snowflake objects such as streams and tasks to transform data inwhich things such as string concatenation or number calculations arerequired. When using Snowflake’s Snowpipe, there are two different mechanismsfor detecting when the staged files are available: automate Snowpipe using cloud messaging and call Snowpipe REST endpoints.

One thing to note is that you can speed up Snowpipe by ingesting smallerdata files. Chunking large files into smaller ones not only allowsSnowflake to process the data more quickly but also makes the data avail-able sooner.

As a rule of thumb, the optimal file size for loading into Snowpipe is 100 to 250MB of compressed data. Try to stage the data within 60-second intervals if thedata arrives continuously. Remember, it is possible to create a Snowpipe in such away that latency can be decreased and throughput can be significantly increased,but the architectural design needs to be weighed against the increased Snowpipecosts that occur as more frequent file ingestions are triggered.

While Snowpipe latency reduction may be good enough for most near–real-time situations, there are times when Snowpipe might not be fastenough. Ecommerce and IoT are two examples in which actual real-timedata processing might be needed. In these cases, we’d need to utilize apure-play stream processing tool such as Confluent’s KSQL, which pro-cesses data directly in a Kafka Stream, or Apache Flink and ApacheFlume.

There are a few different considerations when deciding between the twoSnowpipe methods of auto-ingest or REST API. In situations where files arrivecontinuously, you can use Snowflake’s auto-ingest feature to create an event noti-fication. Snowpipe auto-ingest is the more scalable approach. REST API is the bet-ter option for use cases in which data arrives randomly and/or if preprocessingneeds require using an ETL or ELT tool, or in situations in which an external stageis unavailable.

Using Apache Kafka

Kafka was originally created to enable messaging queue workloads whichincluded producers and consumers in a publisher and subscriber model. The Kafka Producer and Consumer APIs areknown for their simplicity. Data is sent asynchronously, which results in acallback. Although the Kafka Producer API can be extended and builtupon, it isn’t recommended to use the Kafka Producer or Consumer APIfor an ETL approach. Instead, you should consider using the Kafka Connect source API.

The KafkaStreams API and KSQL can be used for applications wanting to consumefrom Kafka and produce back into Kafka you can use KSQL if you want to write a real-time SQL-likejob. Kafka Streams requires that custom coding be undertaken, and it doescome with joins and aggregations. Unlike the Kafka Connector, the KafkaStreams include exactly-once processing native capabilities. ApacheKafka also works with external processing systems such as Apache Flink,Apache Spark, and Apache NiFi for stream processing.

Another Kafka approach is the Kafka Connector source, typically used forbridging between Kafka and a datastore such as Twitter, and the KafkaConnector sink. The Kafka Connector, is useful for ingesting sreaming data in snowflake.

Kafka runs on a cluster of one or more servers, known as brokers, andpartitions all Kafka topics to be distributed across cluster nodes. A Kafkatopic is a category, or feed name, to which rows of records are stored. One Kafka topic processes a stream of messages, consisting of rows,which are then inserted into a Snowflake table. Within the Kafka configu-ration, a topic can be mapped to an existing Snowflake table. For any unmapped topics, the Kafka Connector creates anew Snowflake table using the topic name.

The Kafka Connector will use the following rules to convert a topic name to a Snowflake table name when creating a new table:
- Lowercase topic names are converted to uppercase table names.
- The Connector prepends an underscore to the table name unless thefirst character in the topic is an upper or lowercase letter, or an un-derscore character.
- An underscore character replaces any topic name character that is nota Snowflake table name legal character.

It is recommended that any Kafka topic names you create should follow the rulesfor Snowflake identifier names. That way, underscore characters won’t be neededto replace Snowflake table name illegal characters. It’s important to note that you con-figure the Kafka Connector by creating a file with specified parametersfor things such as the Snowflake login credential, topic name(s),Snowflake table name(s), and more.

A Kafka Connector can ingest messages from multiple topics, but the correspond-ing tables for connectors listed in one configuration file must all be stored in asingle database and schema.

Some important things to remember about Snowpipe billing is that Snowpipedoesn’t require a virtual warehouse, because it is a serverless model. In a server-less model, Snowflake provides and automatically manages compute resourcesbased on the load. Also, there is a utilization cost of 0.06 credits per 1,000 files no-tified. The utilization cost applies to both Snowpipe REST and SnowpipeAUTO_INGEST.

we can see that the Kafka Connect source API receives records from a source database, REST API, or application such as Twitter. The Kafka cluster receives those records and splits each topic into one or more Kafka partitions. It is important to note that, in our example, we’re including only one topic and sink connector, but a Kafka cluster could have multiple topics and, thus, would use multiple sink connectors

Once a certain time has passed or a certain number of records have beenreceived, the Kafka sink connector writes the messages to a Snowflake in-ternal named stage using the PUT command. The sink connector thentriggers Snowpipe to ingest the staged files. You’ll notice in Figure 6-35 that there are two Snowpipes because there are two Kafka topic partitions.

A Snowflake Snowpipe serverless compute cluster will use the COPY com-mand to load the data into a Snowflake table from the internal Snowflakestage. After confirming the data was successfully loaded, the files in theinternal stage will be deleted.

When the Snowflake table is created by the Kafka Connector, two col-umns of the VARIANT data type will be created. The first column is theRECORD_CONTENT column which contains the Kafka message. The second VARIANT column created is the RECORD_METADATA columnwhich has the metadata about the message. Among other things, themetadata includes the topic and the number of Kafka partitions withinthe topic.

Kafka messages are passed to Snowflake in either a JSON or Avro format and eachmessage is stored in one VARIANT column, without the data being parsed. Thefield names and values within the message are case sensitive. 

It is possible to create custom Kafka code for performing in-flight transformationsor for making data available more quickly. In those situations, the sink connectorwould be replaced by the custom code using JDBC batch insert statements.

Snowflake Snowpipe can be automated using cloud messaging from AWS,Azure, or GCP. The auto-ingestion method uses cloud message event noti-fication and a Snowflake external stage. The external stage is importantbecause an automated Snowpipe only works with a Snowflake externalstage object.

Optionally, in conjunction with automated Snowpipe using cloud messaging, it ispossible to utilize on-premises Kafka clusters to stream topics to one of the cloudproviders, where messages are collected and stored and are then auto-ingestedusing event notification.

For each of the three cloud providers upon which Snowflake is built,there is an associated event notification type (as shown in Figure 6-36) that can be used for cloud messaging. The cloud messaging service notifies Snowpipe of the arrival of new datafiles. In a continuous serverless fashion, Snowpipe loads the data into the target table

Let’s consider an example of how to use Snowpipe auto-ingestion forloading data into Snowflake. We’ll use Microsoft Azure as an example byconsidering, at a high level, the pieces of information we need, and thenwe’ll look at the steps required.

First, you’ll need several pieces of information that are assigned fromAzure:
 - Tenant ID
 - Storage queue notification URL, which will be in a form similar to     
      https://sflakesnowpipe.queue.core.windows.net/snowdata-queue
 - Application name, which will be in a form similar to SnowflakePACInt1025
 - Primary Azure blob storage endpoint, which will be in a form similar to https://sflakesnowpipe.blob.core.windows.net/
 - Blob services shared access signature, which will be in a form similar to  
      https://sflakesnowpipe.blob.core.windows.net/xxxx

Within Azure, you’ll need to create the following items: resource group,storage account name, container name, queue name, event subscription name, and system topic name. 
You’ll also need to obtain the Azure consent URL assigned in Snowflake. Itwill be in a form similar to
https://login.microsoftonline.com/xxxx.

In Snowflake, you’ll need to make sure you have a database, schema, ta-ble with fields, and stage created. You’ll also need to create an integrationand pipe in Snowflake. 

The stage link to a Snowflake integration is created using a hidden ID rather thanthe name of the integration. Therefore, the association between a Snowflake integration and any stage that references it will be broken if the integration is re-cre-ated, even with the same name.

Here are the steps you’d need to take to create a Snowpipe auto-ingestionapproach for loading data from Microsoft Azure:

1. Log in to your Azure account and obtain your tenant ID.
2. In Azure, create the following items if they do not already exist: re-source group, storage account within the resource group, container,queue, and event subscription.
3. In Azure, get the queue URI.
4. In Snowflake, create the following items if they do not already exist:database, schema, and table.
5. In Snowflake, set your role to ACCOUNTADMIN and create a notifica-tion integration:
CREATE NOTIFICATION INTEGRATION <integration name> ENABLED = TRUE TYPE = QUEUE NOTIFICATION_PROVIDER = AZURE_STORAGE_QUEUE AZURE_STORAGE_QUEUE_PRIMARY_URI = '<URI>' AZURE_TENANT_ID = '<tenant_id>' ;

6. In Snowflake, get the AZURE CONSENT URL
DESC NOTIFICATION INTEGRATION <integration name>;

7. In Azure, add the role assignment and get the endpoint.
8. In Azure, generate the shared access signature.
9. In Snowflake, create an external stage.
10. In Snowflake, create a Snowpipe:
CREATE PIPE <database.schema.pipe_name> AUTO_INGEST = TRUE INTEGRATION = <integration name> AS COPY INTO 
<database.schema.target_table_name> FROM @<database.schema.external_stage_name> FILE_FORMAT =(TYPE = 'JSON');

To resolve data load errors related to data issues, it is strongly recommended thatyou use the VALIDATE function as part of a post-load validation process to ensurethat all files have been successfully loaded into target tables .

It is important to note that cross-cloud sup-port is currently available only to accounts hosted on Amazon WebServices. - verify this.

In addition to auto-ingest Snowpipe, Snowflake provides a REST API op-tion to trigger data to Snowpipe. One important note is that SnowpipeREST APIs work with internal as well as external Snowflake stages.

Calling Snowpipe REST endpoints

Another way to trigger a data loading event is by using Snowflake’sSnowpipe REST API method. Using this method, a public REST endpointwith the name of a pipe and a list of filenames is called. If new data filesmatching the filenames list are discovered, they are queued for loading.Snowpipe serverless compute resources will then load the queued filesinto the Snowflake target table. Cloud storage service support for Snowpipe REST API calls fromSnowflake accounts allows for cross-cloud support for all accounts hostedon Amazon Web Services, Google Cloud Platform, or Microsoft Azure.

Differences between Snowpipe REST and Snowpipe AUTO_INGEST

Snowpipe REST						Snowpipe AUTO_INGEST
Available for use with internal		Available for use with externalstages only
and external stages

Manually call Snowpipe REST API		A notification is received from
endpoint, with the name of a pipe	cloud provider when a new file arrives
and a list of filenames

Pass a list of files in the 		Process new files when awakened
stagelocation


As we’ve learned, Snowpipe can add latency to your streaming data. Italso requires you to store your data in a stage prior to loading the datainto a table, which increases the storage costs. To help solve those prob-lems, Snowflake has created a new streaming API. This new API allowsyou to use a Streaming Ingest SDK, which can be implemented using Java.It is then possible to create a mapping of values and rows to be inserted.If you’re currently using Kafka as part of your solution, it is relativelyeasy to change the properties and begin taking advantage of the newSnowpipe Streaming functionality.

Alternatives to Loading Data

Queries can be executed on data that isn’t loaded into Snowflake. One ex-ample would be external tables created on data within a data lake. UsingSnowflake external tables allows you to query the existing data stored inexternal cloud storage, where the data remains as the source of truth,without needing to load the data into Snowflake. This solution is fre-quently used when large amounts of data exist in external cloud storagebut only a portion of the data is needed for analysis. In this case, we cancreate a materialized view on the external table so that only a subset ofthe data is used; this leads to improved query performance.

Another alternative to loading data into a table is to use Snowflake datasharing to access data that is owned by someone else and shared withyou. The Snowflake Marketplace includes many data offerings availableto the public.

One other alternative to data loading would be to clone a table. Cloning atable with the CREATE command results in a new table being createdwith the same data as the existing table. It is also possible to replicate atable with existing data.

USE ROLE SYSADMIN;USE SCHEMA WS;
CREATE TABLE DEMO_CLONE CLONE TABLE1 ;

Finally, one newer alternative to loading data is to use Snowflake’sSnowpark functionality. Snowpark offers the ability to query and process data in a data pipeline without moving data to the system where the ap-plication code runs.

Tools to Unload Data

The process for unloading data from Snowflake is the same as the dataloading process, except in reverse. When the target location is a local filesystem, data is first unloaded to a Snowflake internal stage using the COPY INTO command. The GET command is then used to download fromthe internal stage to the local file system

For data being unloaded to one of the three major cloud providers, thereare two options. Data files can be unloaded directly into a storage locationfor the cloud provider for whom the Snowflake account is built upon. Forexample, if you selected Google Cloud as your Snowflake provider, youcould unload files directly to a GCP container and then use Google CloudStorage utilities to download locally. Alternatively, you can use the COPYINTO command to extract files from a Snowflake table into a Snowflakeexternal stage and then use cloud provider tools to download intostorage.

Data Loading Best Practices forSnowflake Data Engineers

Select the Right Data Loading Tool and Considerthe Appropriate Data Type Options

Your choice of data loading tool is influenced by many factors: whetherthe load is intended to be a repeat process or a one-time load, whetherthe data is needed in real time or near-real time by end users, the datatype of the data to be loaded, the existing skill set of your team, and more.Keeping these factors in mind, general best practice recommendationscan still be made.

JDBC and ODBC connectors are good choices in some situations, but theyshould not be the tool of choice for large, regular data loads.

When considering the data being loaded, it is important to choose the ap-propriate data type when creating your Snowflake table. For example, date and timestamp data types are stored more efficiently in Snowflakethan the VARCHAR data type.

Data file size limitations and recommendations, especially for semi-struc-tured data, should be adhered to. Parquet
files greater than 3 GB couldtime out, so it is recommended they be split into 1 GB sizes or smaller.

Avoid Row-by-Row Data Processing

Query performance is greatly impacted in situations involving row-by-row data processing. Given that Snowflake is built to ingest and processbillions of rows with ease, the data process you design should focus onprocessing the entire data set, rather than one row at a time.

Choose the Right Snowflake Virtual WarehouseSize and Split Files as Needed

To prevent resource contention, be sure to isolate data load jobs fromqueries by dedicating separate virtual warehouses for each. Rather thanassuming a larger virtual warehouse will necessarily load massive datafiles any faster than a smaller virtual warehouse (it likely won’t), makesure to instead try splitting large files into smaller files of about 100 to250 MB in size. Remember that the number of files being loaded and thesize of each file influences performance more than the size of the virtualwarehouse.

That said, it’s important to note that choosing the right Snowflake virtualwarehouse size is necessary when using the COPY command. However, aserverless approach could be more ideal depending on the use case. If a serverless approach is desired, Snowpipe could be used. In that case,there is no need to worry about virtual warehouse sizing.

Transform Data in Steps and Use Transient Tablesfor Intermediate Results

When appropriate, transform data in multiple steps rather than havingmassive SQL statement(s). This should result in simpler code and makes itpossible to test intermediate results. For the intermediate results, it is rec-ommended that you use transient tables and truncate them prior to thenext data load so that the Time Travel storage fees will be reduced.

USE WAREHOUSE COMPUTE_WH;
DROP DATABASE DEMO6_DB;
DROP WAREHOUSE LOAD_WH ;

Chapter 7. Implementing DataGovernance, Account Security, and Data Protection and Recovery

USE ROLE USERADMIN;
CREATE OR REPLACE USER ADAM PASSWORD = '123' LOGIN_NAME = ADAM DISPLAY_NAME = ADAM EMAIL = 'sruthi.ask16@gMAIL.COM' MUST_CHANGE_PASSWORD = TRUE ;

Human Resources role, HR_ROLE, to test out our dynamic data masking. The two AREA roles willbe used later to demonstrate row-level security:

USE ROLE USERADMIN;
CREATE OR REPLACE ROLE HR_ROLE;
CREATE OR REPLACE ROLE AREA1_ROLE;
CREATE OR REPLACE ROLE AREA2_ROLE ;

USE ROLE SYSADMIN;
USE WAREHOUSE COMPUTE_WH;
CREATE OR REPLACE DATABASE DEMO7_DB;
CREATE OR REPLACE SCHEMA TAG_LIBRARY;
CREATE OR REPLACE SCHEMA HRDATA;
CREATE OR REPLACE SCHEMA CH7DATA;
CREATE OR REPLACE TABLE DEMO7_DB.CH7DATA.RATINGS ( EMP_ID integer, RATING integer, DEPT_ID varchar, AREA integer );

INSERT INTO DEMO7_DB.CH7DATA.RATINGS VALUES 
( 1, 77, '100', 1 ), 
( 2, 80, '100', 1 ), 
( 3, 72, '101', 1 ), 
( 4, 94, '200', 2 ), 
( 5, 88, '300', 3 ), 
( 6, 91, '400', 3 );

grant some permissions to roles and then assign those roles toour new user.

USE ROLE SECURITYADMIN;
GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE HR_ROLE;
GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE AREA1_ROLE;
GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE AREA2_ROLE;

We also need to grant those roles usage to the necessary objects

GRANT USAGE ON DATABASE DEMO7_DB TO ROLE HR_ROLE;
GRANT USAGE ON DATABASE DEMO7_DB TO ROLE AREA1_ROLE;
GRANT USAGE ON DATABASE DEMO7_DB TO ROLE AREA2_ROLE;
GRANT USAGE ON SCHEMA DEMO7_DB.CH7DATA TO ROLE HR_ROLE;
GRANT USAGE ON SCHEMA DEMO7_DB.HRDATA TO ROLE HR_ROLE;
GRANT USAGE ON SCHEMA DEMO7_DB.CH7DATA TO ROLE AREA1_ROLE;
GRANT USAGE ON SCHEMA DEMO7_DB.CH7DATA TO ROLE AREA2_ROLE;
GRANT SELECT ON ALL TABLES IN SCHEMA DEMO7_DB.CH7DATA TO ROLE HR_ROLE;
GRANT SELECT ON ALL TABLES IN SCHEMA DEMO7_DB.CH7DATA TO ROLE AREA1_ROLE;
GRANT SELECT ON ALL TABLES IN SCHEMA DEMO7_DB.CH7DATA TO ROLE AREA2_ROLE ;

finally, we’ll want to assign the three roles to our new user. HR_ROLE is not assigned back to the SYSADMIN because HR_ROLE will have access to sensitive data. 

GRANT ROLE HR_ROLE TO USER ADAM;
GRANT ROLE AREA1_ROLE TO USER ADAM;
GRANT ROLE AREA2_ROLE TO USER ADAM;
GRANT ROLE AREA1_ROLE TO ROLE SYSADMIN;
GRANT ROLE AREA2_ROLE TO ROLE SYSADMIN ;

use the ACCOUNTADMIN role to grant future select and insert privileges to the HR_ROLE

USE ROLE ACCOUNTADMIN;
GRANT SELECT ON FUTURE TABLES IN SCHEMA DEMO7_DB.HRDATA TO ROLE HR_ROLE;
GRANT INSERT ON FUTURE TABLES IN SCHEMA DEMO7_DB.HRDATA TO ROLE HR_ROLE;
USE ROLE SYSADMIN ;

Snowflake Security

Keeping your Snowflake account and data secure is a collaborative effortbetween Snowflake and you. Snowflake takes responsibility for the secu-rity of data in transit to and from the Data Cloud as well as the security ofdata at rest, stored in tables. This is accomplished by using strong encryp-tion methods to protect data from external parties. Snowflake providesSoc 1 Type II and Soc 2 Type II compliance security validations for all Snowflake accounts.

In addition, Snowflake performs many penetrationtests each year and offers support for HIPAA compliance, PCI DSS compli-ance, and FedRAMP Moderate compliance for Snowflake’s BusinessCritical Edition and higher in certain regions. Snowflake is FedRAMP au-thorized Moderate with agencies such as the Centers for Medicare &Medicaid Services, the GSA, and the Department of Health and HumanServices. Snowflake supports ITAR compliance, audited by a third-party organization.

Snowflake is in the process of obtaining FedRAMP High au-thorization, which is expected winter of 2022 and based on agency andPMO availability. In parallel, Snowflake is pursuing IL4, with the inten-tion of being IL4 authorized in 2023. Snowflake is also part of theHITRUST Shared Responsibility and Inheritance Program (SRM) and hasearned the HITRUST certification. The HITRUST CSF is the leading infor-mation security framework for the healthcare industry

Snowflake also provides the tools you need to keep your site access se-cure, protect your data, monitor user activity, and recover data, should itbecome necessary.

Controlling Account Access

Snowflake gives administrators the ability to create security policies to also limit the location from where users can log in.

Authentication and user management

Alternatively, users can indirectly access Snowflake. Indirect access toSnowflake through business intelligence (BI) tools is one example. Integration users can also be established to provide a more secure and au-ditable way to move data in and out of Snowflake without relying on anexisting user’s account. Creating an integration user for Snowpipe, for ex-ample, is a best practice. For integration users, key pair is the preferredauthentication method.

For users accessing Snowflake data through third-party applications,OAuth integration is the preferred choice because it enables access todata without sharing the user’s password and it decouples authentication and authorization. Credentials do not need to be stored, because a tokenis used and tied to a Snowflake role instead.

When users need direct access to work within Snowflake, they can au-thenticate using federated authentication. For federated authentication, auser is authenticated with single sign-on (SSO) using an identity provider(IdP) such as Okta or Microsoft Active Directory Federation Services (ADFS).

For Snowflake accounts not using SSO, Snowflake offers a multifactor au-thentication (MFA) native solution which supports connection via theweb UI, connections using the SnowSQL command-line interface (CLI),and JDBC and ODBC client connections. MFA is intended to be used in ad-dition to strong passwords. Snowflake offers self-service MFA to all ac-count levels so that users can enroll themselves.

Once users are authenticated, the data theycan see and the activities they can perform within Snowflake, such as cre-ating tables and using the SELECT command to view the data, need to becontrolled as well. its achived with RBAC.

One example of a best practicethat we recommended is that custom roles be granted back to the systemadministrator role. However, you may want to consider some exceptionsto that practice for roles used to access sensitive data, such as HR, finan-cial, or medical data.

Managing network security policies and firewall access

By default, users can connect to Snowflake from any location. However,to reduce security risks or to meet compliance requirements, you mayneed to restrict how and where users can connect to Snowflake. This canbe achieved by creating network policy objects. For example, networkpolicies can be created to allow traffic from certain IP addresses or your organization’s virtual private networks (VPNs) and virtual private clouds(VPCs). You can specify a list of IP ranges to allow or disallow.

Any defined blocked IP ranges should be a subset of the list of allowed IP ad-dresses because anything outside the list of allowed IP ranges would automati-cally be disallowed anyway.

There are two types of network policies: account-level network policiesand user-level network policies. Account-level network security policie scan be created via the web interface or by using SQL. In contrast, user-level network policies can only be formed using SQL.

Youfll need to use the account administrator role to set up a network policy. As an account administrator, you can click Admin -> Security from the Main menu. In the up-per-right corner, click the + Network Policy button. After an account-level network policy is created, it will need to be acti-vated by associating the policy to your Snowflake account. On the rightside of the screen is an Activate Policy button.

A SECURITYADMIN or ACCOUNTADMIN role can create an account-levelnetwork policy, but only one network policy can be activated at a time.Similarly, a user-level network policy also needs to be activated beforeSnowflake will enforce the policy, and only one user-level policy per usercan be activated at a time. For any user having both an account-level anduser-level network policy, the user-level policy will take precedence.

You can use the SHOW NETWORK POLICIES statement to see a list of all net-work policies in your Snowflake account. Make sure you use the ACCOUNTADMIN role to execute the statement. Executing the commanddoes not require a running warehouse.

For firewall egress traffic, you can allow your organization’s firewall toconnect client applications to Snowflake, if your network has a firewallfor egress traffic. If you’re allowing a public endpoint, you can executethe SELECT SYSTEM$WHITELIST() function. If you have an accessible pri-vate endpoint, you can execute the SELECTSYSTEM$WHITELIST_PRIVATELINK() function. Note that if you are using anetwork proxy to inspect egress traffic, you can set it up for SSLpassthrough.

SSL terminating proxies are not supported by Snowflake.

Diagnosing and troubleshooting network connection issues can be accom-plished with the Snowflake Connectivity Diagnostic Tool, SnowCD.SnowCD leverages the Snowflake hostname IP address and ports thatwere listed by the previous system functions to run a series of connectionchecks to evaluate and help troubleshoot the network connection toSnowflake.

Monitoring Activity with the Snowflake ACCESS_HISTORY Account Usage View

Designing and implementing access controls to limit access to certaindata is a critical piece of account security. The Snowflake ACCESS_HISTORY view monitors access to enable companies to satisfytheir compliance audits and comply with regulatory requirements. Whileperforming monitoring for security reasons, it is recommended to use theACCESS_HISTORY view to also identify any unused tables or columnswhich could be removed to optimize storage costs.

The Snowflake ACCESS_HISTORY view is used to query the access historyof Snowflake objects within the previous 365 days. It is accessible to theSnowflake ACCOUNTADMIN role by default. The ACCESS_HISTORY viewsupports SQL read and write operations. Columns in this view includeQUERY_ID, the name of the user who ran the query, the start time of thequery, and the objects that were accessed or modified by the user. Notice that the output for the accessed objects is given in JSON form, as shown in Figure 7-5.As a result, we’ll want to flatten the output.

The Snowflake ACCESS_HISTORY view supports most write operationcommands, including GET, PUT, DELETE, INSERT, MERGE, UPDATE, COPYINTO, and CREATE.The operations to populate views or streams, as wellas data movement resulting from replication, are not included in theACCESS_HISTORY view.

Let’s consider what specific information the ACCESS_HISTORY view canprovide for us. We can run a query to see the number of queries each user has run. We’ll list the user with the most frequent number of queries

USE ROLE ACCOUNTADMIN;
SELECT USER_NAME, COUNT (*) USES FROM SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY GROUP BY USER_NAME ORDER BY USES DESC ;
+---------------------+------+
| USER_NAME           | USES |
|---------------------+------|
| NESANAWSNEW         |  449 |
| WORKSHEETS_APP_USER |  116 |
| SYSTEM              |    9 |
| SNOWFLAKE           |    1 |
+---------------------+------+

We can also query the Snowflake ACCESS_HISTORY view to find out themost frequently used tables

SELECT OBJ.VALUE:objectName::STRING TABLENAME, COUNT (*) USES FROM SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY, 
TABLE ( FLATTEN ( BASE_OBJECTS_ACCESSED )) OBJ  sample(0.1) GROUP BY TABLENAME ORDER BY USES desc;

-------------------------------------------------------+------+
| TABLENAME                                             | USES |
|-------------------------------------------------------+------|
| NULL                                                  |   74 |
| LEARNING_SQL.PUBLIC.ORDERS                            |   73 |
| LEARNING_SQL.PUBLIC.REGION                            |   48 |
| LEARNING_SQL.PUBLIC.NATION                            |   44 |
| WORKSHEETS_APP.PUBLIC.BLOBS                           |   44 |
| LEARNING_SQL.PUBLIC.CUSTOMER                          |   25 |
| LEARNING_SQL.PUBLIC.EMPLOYEE                          |   24 |
| NESANAWSNEW                                           |   21 |
| DB1.DB1_SCHEMA1.TABLE16                               |    1 |
| SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.PART                   |    1 |
| DB1.DB1_SCHEMA1.TABLE15                               |    1 |
| DB1.DB1_SCHEMA1.TABLE10                               |    1 |
| DB1.DB1_SCHEMA1.TABLE4                                |    1 |
| DB1.DB1_SCHEMA1.TABLE8                                |    1 |
+-------------------------------------------------------+------+

SELECT OBJ.VALUE:objectName::STRING TABLENAME, COUNT (*) USES FROM SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY sample(20), 
TABLE ( FLATTEN ( BASE_OBJECTS_ACCESSED )) OBJ GROUP BY TABLENAME ORDER BY USES desc;

with sam as 
(SELECT OBJ.VALUE:objectName::STRING TABLENAME, COUNT (*) USES FROM SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY, 
TABLE ( FLATTEN ( BASE_OBJECTS_ACCESSED )) OBJ  GROUP BY TABLENAME ORDER BY USES desc)
select * from sam sample(30);
+---------------------------------------------------+------+
| TABLENAME                                         | USES |
|---------------------------------------------------+------|
| LEARNING_SQL.PUBLIC.REGION                        |   48 |
| LEARNING_SQL.PUBLIC.NATION                        |   44 |
| WORKSHEETS_APP.PUBLIC.BLOBS                       |   44 |
| LEARNING_SQL.PUBLIC.SUPPLIER                      |   19 |
| SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS             |   16 |
| SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.REGION             |   16 |
| DB1.DB1_SCHEMA1."__snowflake_temp_import_files__" |    5 |
| DB1.DB1_SCHEMA1.TABLE20                           |    4 |
| SNOWFLAKE.ACCOUNT_USAGE.VIEWS$V12                 |    3 |
| DB1.DB1_SCHEMA1.TABLE19                           |    3 |
| LEARNING_SQL.PUBLIC.PERSON                        |    3 |
| DB1.DB1_SCHEMA1.TABLE12                           |    2 |
| DEMO4_DB.SUBQUERIES.DERIVED                       |    2 |
| SNOWFLAKE.TELEMETRY.EVENTS                        |    2 |
| DB1.DB1_SCHEMA1.TABLE1                            |    1 |
| DB1.DB1_SCHEMA1.TABLE9                            |    1 |
| DB1.DB1_SCHEMA1.TABLE10                           |    1 |
| DB1.DB1_SCHEMA1.TABLE8                            |    1 |
| SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.PART               |    1 |
| DB1.DB1_SCHEMA1.TABLE7                            |    1 |
| DB1.DB1_SCHEMA1.TABLE2                            |    1 |
+---------------------------------------------------+------+

What we’d really liketo know is what tables are being accessed, by whom, and how frequently.

SELECT OBJ.VALUE:objectName::string TABLENAME, USER_NAME, COUNT ( * ) USES FROM SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY, TABLE ( FLATTEN ( BASE_OBJECTS_ACCESSED )) OBJ GROUP BY 1, 2 ORDER BY USES DESC ;

+-------------------------------------------------------+---------------------+------+
| TABLENAME                                             | USER_NAME           | USES |
|-------------------------------------------------------+---------------------+------|
| LEARNING_SQL.PUBLIC.ORDERS                            | NESANAWSNEW         |   73 |
| NULL                                                  | WORKSHEETS_APP_USER |   72 |
| LEARNING_SQL.PUBLIC.REGION                            | NESANAWSNEW         |   48 |
| LEARNING_SQL.PUBLIC.NATION                            | NESANAWSNEW         |   44 |
| WORKSHEETS_APP.PUBLIC.BLOBS                           | WORKSHEETS_APP_USER |   44 |
| LEARNING_SQL.PUBLIC.CUSTOMER                          | NESANAWSNEW         |   25 |
| LEARNING_SQL.PUBLIC.EMPLOYEE                          | NESANAWSNEW         |   24 |
| NESANAWSNEW                                           | NESANAWSNEW         |   21 |   -- there is table with username
| LEARNING_SQL.PUBLIC.SUPPLIER                          | NESANAWSNEW         |   19 |
| SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER               | NESANAWSNEW         |   16 |

The SNOWFLAKE ACCOUNT_USAGE schema includes other views usefulfor monitoring. The LOGIN_HISTORY view is a log of every connection es-tablished with Snowflake so that you can determine who logged in fromwhere and using what authentication method. The QUERY_HISTORY viewprovides a log of every query run in Snowflake, including queries against metadata such as users or roles as well as queries against data.

select top 20 * from SNOWFLAKE.ACCOUNT_USAGE.LOGIN_HISTORY;

select top 20 * from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY;

select distinct QUERY_TYPE, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct USER_NAME, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct ROLE_NAME, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct WAREHOUSE_ID, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct WAREHOUSE_NAME, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct WAREHOUSE_SIZE, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct WAREHOUSE_TYPE, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct CLUSTER_NUMBER, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct BYTES_SCANNED, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct PERCENTAGE_SCANNED_FROM_CACHE, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct PARTITIONS_SCANNED, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct PARTITIONS_TOTAL, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct BYTES_SPILLED_TO_LOCAL_STORAGE, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct BYTES_SPILLED_TO_REMOTE_STORAGE, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct BYTES_SENT_OVER_THE_NETWORK, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct COMPILATION_TIME, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct EXECUTION_TIME, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct OUTBOUND_DATA_TRANSFER_CLOUD, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct OUTBOUND_DATA_TRANSFER_REGION, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct OUTBOUND_DATA_TRANSFER_BYTES, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct INBOUND_DATA_TRANSFER_CLOUD, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct INBOUND_DATA_TRANSFER_REGION, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct INBOUND_DATA_TRANSFER_BYTES, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct CHILD_QUERIES_WAIT_TIME, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct ROLE_TYPE, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct QUERY_HASH, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct QUERY_HASH_VERSION, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct QUERY_PARAMETERIZED_HASH, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct QUERY_PARAMETERIZED_HASH_VERSION, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;
select distinct SECONDARY_ROLE_STATS, count(*) from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY group by 1 order by 2 desc;

Data Protection and Recovery

Snowflake offers several native data protection and high availability fea-tures. Many of those features, such as encryption and key management,are automatic and require no involvement from you to function fully. Other data protection and recovery features, such as Time Travel and fail-safe, exist within Snowflake automatically and are available for you to take advantage of them anytime you find it necessary. Lastly, there aresome Snowflake data protection and recovery native features, such asreplication and failover, that require some planning and implementationon your part to take advantage of them.

Encryption and key management

Snowflake automatically provides end-to-end encryption (E2EE) for allSnowflake editions, regardless of the edition type, so that not even thecloud provider on which your Snowflake account is deployed can accessyour data. When you store a Snowflake table, one or more files are cre-ated and stored using the cloud provider’s storage service. Snowflake en-crypts each file with a different key and then “wraps” each file and filekey together by encrypting them with a higher-level table master key. Onereason for this is to avoid having to store many individual file keys in asecure location. Table master keys, used to encrypt and decrypt the filekeys, are themselves encrypted with a higher-level account master keyand stored in the table’s metadata. Similarly, an account master key, usedto encrypt and decrypt table master keys, is encrypted with the root key.The root key, the only key stored in clear text, is used to encrypt and de-crypt an account master key.

To summarize, Snowflake’s hierarchical key model consists of the rootkey, account master keys, table master keys, and file keys. An account keyor table key is rotated by Snowflake when the key is more than 30 daysold. Note that result master keys and stage master keys also exist at the ta-ble master key level.

Snowflake’s root key is in a hardware security module (HSM). Snowflakeoffers the Tri-Secret Secure feature for organizations that need more con-trol over keys and have the Snowflake Business Critical Edition or higher.This feature allows you to generate your own root key using a cloudprovider’s key management service such that two account master keys are used instead of just one. Each master key is wrapped by a root key;your root key wraps one account master key and Snowflake’s root keywraps the other. both master keys are needed todecrypt the table, result, or stage master keys.

With the Business Critical edition and higher, Snowflake can encrypt alldata transmitted over the network within a VPC. Query statement encryp-tion is supported on Enterprise for Sensitive Data (ESD) accounts. Time Travel and fail-safe retention periods, described in the next section,are not affected by rekeying. Rekeying is transparent to both features.However, some additional storage charges are associated with rekeyingof data in fail-safe.

Time Travel and fail-safe

Snowflake provides native change data capture (CDC) features to ensure,for a specified period of time, the continued availability of important datathat has been changed or deleted. Historical data in permanent databasesand database objects can be queried, cloned, and restored for a maxi-mum of 90 days. Data in temporary and transient objects can be accessedup to a maximum of 24 hours. Beyond those Time Travel access periods,historical data in permanent databases and database objects can be re-covered by a Snowflake employee up to seven days later. The seven daysof recoverability are known as the fail-safe period.

Using the Time Travel feature, a Snowflake administrator can query datain the past that has since been updated or deleted, and can create clonesof entire tables, schemas, and databases at or before specific points in thepast. Having Time Travel offers protection from accidental data opera-tions such as dropping a column or table by mistake.

Time Travel is available for permanent, temporary, or transient tables,but not external tables. The following commands can be used for TimeTravel: SELECT At/Before, CLONE At/Before, and UNDROP. With TimeTravel, a user can use the SELECT command to query objects in one ofthree ways:

1) Querying can be accomplished using a timestamp to see what an ob-ject looked like before a given point in time.
2) Querying can be accomplished using a time offset to see what an ob-ject looked like previously based on a certain amount 
     of time that haspassed.
3) It is possible to use Time Travel to view what an object looked like be-fore running a specific query. To obtain any query ID, you can click theActivity ¨ Query History menu option where you will then be able tosee a listing of all queries, including the query ID and status

The Time Travel retention period is automatically enabled for allSnowflake accounts, and the default is 24 hours, or one day; however, itcan be set to zero at the account and object levels. For SnowflakeEnterprise Edition and higher orgs, the retention period can be set up to 90 days for permanent databases, schemas, and tables.

USE ROLE SYSADMIN;
ALTER DATABASE DEMO7_DB SET DATA_RETENTION_TIME_IN_DAYS = 90 ;

By setting the retention time as 90 days for the database, all the databaseobjects will also have a 90-day retention time period. We can confirm thisby taking a look at the database’s INFORMATION_SCHEMA TABLES view

USE ROLE SYSADMIN;
SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, RETENTION_TIME FROM DEMO7_DB.INFORMATION_SCHEMA.TABLES ;

While Time Travel cannot be disabled for an account, you can effectivelydisable Time Travel for specific databases, schemas, or tables by changingthe default data retention time from one day to zero days. Informationabout dropped objects can still be seen in Snowflake account usage butnot the information schema.

There are nuances to be aware of if you change the retention periods.Changing the data retention time at the schema level will result in all ta-bles in the schema inheriting the schema’s retention period unless the ta-ble is explicitly given a different retention period. Another thing youshould keep in mind is that the order in which you drop objects does af-fect the retention period if there are differences. When you drop aschema, all the existing tables will be available for the same time periodas the schema. If you want to make sure the data retention period for child objects is honored, you’ll need to drop the child objects prior todropping the parent object.

how Time Travel works. Let’s as-sume we make a mistake and update all the values in a column, ratherthan just updating one value.

USE ROLE SYSADMIN;
UPDATE DEMO7_DB.CH7DATA.RATINGS AREA SET AREA = 4 ;

all the area values have been changed to a value of 4 instead of justthe value for one employee

SELECT * FROM DEMO7_DB.CH7DATA.RATINGS ;

We have a couple of ways we can revert the AREA column back to its pre-vious values. We can use one of the two approaches based on time values,either by using a specific time in the past or by going back a certainamount of time from now, or we can use a query ID. Since we realizedour mistake right away, any of the three would be easy to implement. Let’sfirst confirm that going back in time five minutes will yield the result wewant

SELECT * FROM DEMO7_DB.CH7DATA.RATINGS at (offset => - 60 * 5);

If the table looks correct, go ahead and proceed with the next statement.If not, adjust the number of minutes until you find the correct timeneeded and then proceed with the next statement

CREATE OR REPLACE TABLE DEMO7_DB.CH7DATA.RATINGS AS SELECT * FROM DEMO7_DB.CH7DATA.RATINGS at (offset => - 60 * 1918);

If we check now, we’ll see that the table has the previous values beforewe made the update:

Rather than going back in time using a time offset, we could’ve accom-plished the same thing by using a query ID from our history.

SELECT * from DEMO7_DB.CH7DATA.RATINGS before (statement = > ' <Query ID from History> ') ;

Something else that could possibly happen is that we might drop a tableby mistake and then, after realizing our mistake, we’ll want to go back intime before the table was dropped. Let’s see how that would look.

DROP TABLE DEMO7_DB.CH7DATA.RATINGS;

Try running a SELECT * command and you’ll see that the table no longerexists. Now UNDROP the table and try running the SELECT * commandagain:

UNDROP TABLE DEMO7_DB.CH7DATA.RATINGS ;

It is important to remember that Time Travel works on permanent, tran-sient, and temporary objects, such as databases and database objects. Itdoesn’t work on users or warehouses. If you drop a user, for example,you’ll have to re-create that user.

Whereas Time Travel allows a Snowflake user to access the data for a cer-tain period of time, only a Snowflake employee may be able to recoverdata from fail-safe storage. Fail-safe storage exists as a best effort emer-gency measure that may be able to protect data loss in permanent tablesdue to a catastrophic event such as a security breach or system failure.

It is possible to increase the Time Travel retention period for permanentobjects. When that is done, the data that exists in the current Time Travelretention period will now be retained longer before moving to fail-safe.However, data that had previously exceeded the current retention periodand was already moved into the fail-safe seven-day period will not be af-fected. It is not possible for data to move from the fail-safe period back tothe Time Travel period.

When a Time Travel retention period is reduced for permanent objects,the data in fail-safe is not affected. The data in Time Travel will be af-fected. Any Time Travel data whose time exceeds the new retention pe-riod will be moved to fail-safe where it will be stored for seven days.

Snowflake transient objects are like permanent objects, except transientobjects have no fail-safe period. In addition, the maximum default TimeTravel period for transient objects is only one day, which is much lessthan the 90-day Time Travel period possible for permanent objects.

Temporary objects have the same Time Travel default and maximum val-ues as transient objects. Just like transient objects, there is no fail-safe fortemporary objects. The difference between temporary objects and tran-sient objects is that when the session ends, the data in transient objects isno longer accessible through the Time Travel feature.

Both Time Travel and fail-safe, along with active storage, are part of thetotal calculated storage that incurs storage costs. You can use the follow-ing statement to see how much storage, in bytes, is attributed to each ta-ble in each of the three sections:

USE ROLE ACCOUNTADMIN;
SELECT TABLE_NAME, ACTIVE_BYTES, TIME_TRAVEL_BYTES, FAILSAFE_BYTES FROM SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS ;

+--------------------------+--------------+-------------------+----------------+
| TABLE_NAME               | ACTIVE_BYTES | TIME_TRAVEL_BYTES | FAILSAFE_BYTES |
|--------------------------+--------------+-------------------+----------------|
| TAB                      |            0 |                 0 |              0 |
| _MEASUREMENT_TABLE       |            0 |                 0 |              0 |
| SCANNER_PACKAGE_REGISTRY |         6144 |                 0 |          18432 |
| THREAT_VULN_VAULT        |        20480 |                 0 |          80896 |
| TABLE7                   |         2048 |                 0 |              0 |
| TABLE11                  |         3584 |                 0 |              0 |
| CONFIGURATION_DEFAULTS   |         2048 |                 0 |          12288 |
| CONFIGURATION_REGISTRY   |         3072 |                 0 |           3072 |
| RATINGS                  |         1536 |              1536 |              0 |
+--------------------------+--------------+-------------------+----------------+

Replication and Failover

in maintaining 24/7 system and data availability. This is where Snowflake replication and failover can help.

Snowflake replication, supported across regions and across cloud plat-forms, enables an organization to replicate databases between Snowflakeaccounts. As such, replication and failover almost instantly synchronizeSnowflake data and, because cross-cloud and cross-region replication isasynchronous, there is little to no performance impact on primarydatabases.

Snowflake encrypts files for replication with a random unique key. It is important to be aware that cloned Snowflake objects are replicatedphysically, rather than logically, to secondary databases. This means thatthere will be additional data storage costs for any cloned objects.Replication charges also include data transfer costs and the cost of com-pute resources. Replication supports incremental refreshes, and cus-tomers can determine the frequency at which the replication runs. It isimportant to know that these charges will be billed on the target account.

Both permanent and transient primary databases can be replicated.When one of these primary database types is replicated, the followingitems are also replicated to the secondary database: automatic cluster ofclustered tables, views including materialized views, stored procedures,masking and row access policies, tags, user-defined functions (UDFs), fileformats, and sequences.

Snowflake replication is supported for databases, but if an external tableexists in the primary database, it’s not possible to create or refresh a sec-ondary database with an external table. In addition, the following indi-vidual objects cannot be replicated: warehouses, resource monitors,roles, users, pipes, and streams. Note that account-level replication andthe ability to create failover groups are available.

Democratizing Data with DataGovernance Controls

well-designed data governance controls provide the necessary framework tosupport data democratization, one of the most competitive advantages anorganization can achieve. With Snowflake’s many governance-related features, data canquickly and easily be secured to allow for open access to real-time data without need for replication of data. Because of effective data governance, data democratization canincrease access to data for all knowledge workers within an organization.If sensitive data exists within the data, we can simply identify it and re-strict access to the actual data but not the metadata.

The tangible benefits of all stakeholders in a data-driven organization be-ing able to make insightful data-driven decisions in real time cannot beoverstated. It all starts with embracing data governance as the frame-work for data democratization as well as the necessary vehicle for datasecurity, ensuring privacy, and adhering to regulatory and compliancerequirements.

more governance features including the INFORMATION_SCHEMA Data Dictionary, object tagging, data masking, row access policies, row-level se-curity, external tokenization, and secure views.

INFORMATION_SCHEMA Data Dictionary

The INFORMATION_SCHEMA, also known as the Snowflake DataDictionary, includes usage metrics and metadata information. INFORMATION_SCHEMA includes views at the account anddatabase levels.

Object Tagging

Protecting data in Snowflake begins with understanding the type of datathat is stored in an object and annotating that information so that thenecessary steps can be taken to secure and protect the data. To help withthe annotation process, Snowflake provides a native feature that can beused to create a library of custom tags. These tags can then be associatedto Snowflake objects.

A Snowflake tag is a schema-level object that can be associated to differ-ent object types, including columns as well as schema-, database-, and ac-count-level objects. The tag is stored as a key-value pair where the tag isthe key, and each tag can have many different string values. A tag identi-fier must be unique within each schema. Snowflake limits the number of tags to 10,000 per account.

If you enclose an identifier in double quotes, it is case sensitive and can includeblank spaces. If an identifier is unquoted, it cannot be case sensitive, cannot con-tain blank spaces, and must begin with a letter or underscore.

USE ROLE SYSADMIN;
CREATE OR REPLACE SCHEMA DEMO7_DB.TAG_LIBRARY ;

The first tag we want to set up is a tag associated at the table level to describe whether the table will be classified as confidential, restricted, in-ternal, or public. 

CREATE OR REPLACE TAG Classification; 
ALTER TAG Classification set comment = "Tag Tables or Views with one of the following classification values: 'Confidential', 'Restricted', 'Internal', 'Public'";

Next, we’ll create a tag associated at the column level that will identifyfields that contain personally identifiable information (PII)

CREATE OR REPLACE TAG PII; 
ALTER TAG PII set comment = "Tag Tables or Views with PII with one or more of the following values: 'Phone', 'Email', 'Address'";

CREATE OR REPLACE TAG SENSITIVE_PII; 
ALTER TAG SENSITIVE_PII set comment = "Tag Tables or Views with Sensitive P with one or more of the following values: 'SSN', 'DL', 'Passport', 'Financial', 'Medical'";

create a new EMPLOYEEStable. Remember that earlier we granted the HR role the ability to use the SELECT statement on future tables in the HRDATA schema so that thatrole will have access to this new table we are creating:

USE ROLE SYSADMIN;
CREATE OR REPLACE TABLE DEMO7_DB.HRDATA.EMPLOYEES 
(emp_id integer, fname varchar(50), lname varchar(50), ssn varchar(50), email varchar(50), dept_id integer, dept varchar(50)); 
INSERT INTO DEMO7_DB.HRDATA.EMPLOYEES (emp_id, fname, lname, ssn, email, dept_id, dept) 
VALUES (0, 'First', 'Last', '000-00-0000', 'email@email.com', '100', 'IT');

we’ll assign tags by using the ALTER command. First we assign the classification tag, with the value of "Confidential", to the entire table:

ALTER TABLE DEMO7_DB.HRDATA.EMPLOYEES set tag DEMO7_DB.TAG_LIBRARY.Classification = "Confidential" ;

we’ll assign two tags to two individual columns:

ALTER TABLE DEMO7_DB.HRDATA.EMPLOYEES MODIFY EMAIL set tag DEMO7_DB.TAG_LIBRARY.PII = "Email";
ALTER TABLE DEMO7_DB.HRDATA.EMPLOYEES MODIFY SSN SET TAG DEMO7_DB.TAG_LIBRARY.SENSITIVE_PII = "SSN" ;

In addition to tags directly assigned to an object, objects can also have inherited tags based on the Snowflake object hierarchy. For example, any tag that is appliedto a table is also applied to the columns in that table.

You can use the SHOW TAGS statement to see that all three tags were cre-ated.

SHOW TAGS;
+-------------------------------+----------------+---------------+-------------+----------+--------------------------------------------------------------------------------------------------------------------------------+----------------+-----------------+
| created_on                    | name           | database_name | schema_name | owner    | comment                                                                                                                        | allowed_values | owner_role_type |
|-------------------------------+----------------+---------------+-------------+----------+--------------------------------------------------------------------------------------------------------------------------------+----------------+-----------------|
| 2024-12-07 04:25:13.721 -0800 | CLASSIFICATION | DEMO7_DB      | TAG_LIBRARY | SYSADMIN | Tag Tables or Views with one of the following classification values: 'Confidential', 'Restricted', 'Internal', 'Public'        | NULL           | ROLE            |
| 2024-12-07 04:25:25.867 -0800 | PII            | DEMO7_DB      | TAG_LIBRARY | SYSADMIN | Tag Tables or Views with PII with one or more of the following values: 'Phone', 'Email', 'Address'                             | NULL           | ROLE            |
| 2024-12-07 04:25:34.829 -0800 | SENSITIVE_PII  | DEMO7_DB      | TAG_LIBRARY | SYSADMIN | Tag Tables or Views with Sensitive P with one or more of the following values: 'SSN', 'DL', 'Passport', 'Financial', 'Medical' | NULL           | ROLE            |
+-------------------------------+----------------+---------------+-------------+----------+--------------------------------------------------------------------------------------------------------------------------------+----------------+-----------------+

You can also query a table or column to see if a particular tag value is as-sociated with the table or column. First, let’s see if any Classification tag value is associated with our EMPLOYEES table

SELECT SYSTEM$GET_TAG( 'Classification', 'DEMO7_DB.HRDATA.EMPLOYEES', 'table');
+---------------------------------------------------------------------------+
| SYSTEM$GET_TAG( 'CLASSIFICATION', 'DEMO7_DB.HRDATA.EMPLOYEES', 'TABLE') |
|---------------------------------------------------------------------------|
| Confidential                                                              |
+---------------------------------------------------------------------------+

We see that the Confidential tag value is associated. Now let’s see if any SENSITIVE_PII tag is associated with a column. You’ll notice that we have to specify the column we want to check:

SELECT SYSTEM$GET_TAG ( 'SENSITIVE_PII', 'DEMO7_DB.HRDATA.EMPLOYEES.SSN', 'column' );

+---------------------------------------------------------------------------------+
| SYSTEM$GET_TAG ( 'SENSITIVE_PII', 'DEMO7_DB.HRDATA.EMPLOYEES.SSN', 'COLUMN' ) |
|---------------------------------------------------------------------------------|
| SSN                                                                             |
+---------------------------------------------------------------------------------+

SELECT SYSTEM$GET_TAG( 'SENSITIVE_PII', 'DEMO7_DB.HRDATA.EMPLOYEES.EMAIL', 'column' );

+----------------------------------------------------------------------------------+
| SYSTEM$GET_TAG( 'SENSITIVE_PII', 'DEMO7_DB.HRDATA.EMPLOYEES.EMAIL', 'COLUMN' ) |
|----------------------------------------------------------------------------------|
| NULL                                                                             |
+----------------------------------------------------------------------------------+

Appropriately setting tags for tables, views, and columns allows us toquery based on tags to discover which database objects and columns con-tain sensitive information. Then, data stewards can implement datamasking policies or row access policies as needed.

Let’s create a SQL statement to locate any object or column that has been tagged with a PII or SENSITIVE_PII tag but does not have a maskingpolicy. Let’s see what happens when we run the following statement

// Audit all columns with a sensitive tag without a masking policy 
// Note that Latency may be up to 2 hours for tag_references 
USE ROLE ACCOUNTADMIN;
WITH column_with_tag 
AS 
  (SELECT object_name table_name, column_name column_name, object_database db_name, object_schema schema_name 
  FROM snowflake.account_usage.tag_references WHERE tag_schema = 'TAG_LIBRARY' AND ( tag_name = 'SENSITIVE_PII' OR tag_name = 'PII' ) AND column_name is not null ), column_with_policy 
  AS 
  ( SELECT ref_entity_name table_name, ref_column_name column_name, ref_database_name db_name, ref_schema_name schema_name FROM snowflake.account_usage.policy_references WHERE policy_kind = 'MASKING POLICY' )
  SELECT * FROM column_with_tag 
   MINUS 
  SELECT * FROM column_with_policy;
  
+------------+-------------+----------+-------------+
| TABLE_NAME | COLUMN_NAME | DB_NAME  | SCHEMA_NAME |
|------------+-------------+----------+-------------|
| EMPLOYEES  | SSN         | DEMO7_DB | HRDATA      |
| EMPLOYEES  | EMAIL       | DEMO7_DB | HRDATA      |
+------------+-------------+----------+-------------+  

Most likely, the query was run successfully but returned no results. Thatis because there exists up to a two-hour latency before you are able toquery the Snowflake database for tag references.

The way Snowflake tags are designed allows for different managementapproaches. In a centralized approach, the administrator is responsiblefor creating and applying tags to Snowflake objects. The administratorcould create a TAG_ADMIN role to delegate the handling of that responsi-bility. Alternatively, a TAG_ADMIN role could be used in a decentralizedmanner such that individual teams apply tags to Snowflake objects.

when a Snowflake primary database is replicated,the tags are also replicated to the secondary database.

Snowflake classification is a new capability that automatically detects PIIin a table, view, or column and tags it. Once the tags produced by classification have been assigned, they can be used for data governance, datasharing, and data privacy
purposes.

Object tagging is also frequently used for tracking data and ware-house consumption for cost reporting purposes.

Classification

Snowflake’s Data Classification analyzes columns in structured data forpersonal information that may be considered sensitive. Classificationfunctionality is built into the Snowflake platform and includes predefinedsystem tags to assist with classifying the data. Once data has been classi-fied, it is discoverable by querying the INFORMATION_SCHEMA. Then, classified data access can be audited through the ACCESS_HISTORY viewto ensure that the proper role-based policies and anonymization have been put in place.

Classification category types

There are two category classification types: semantic and privacy. A se-mantic category identifies a column that is storing personal attributes,such as name, address, age, or gender. Any column for which a semanticcategory is identified is further classified with a privacy category. 

Classification is available to all Enterprise Edition accounts and above. To classifydata in a table or view, the role used to classify data must have the IMPORTEDPRIVILEGES privileges on the SNOWFLAKE database. Additionally, the USAGEprivilege on the database and schema is needed along with at least the SELECT privilege on the table or view. Because the classification feature uses tags, it isalso necessary to have the APPLY TAG global privilege.

Snowflake classification supports three privacy categories as follows:
Identifier
   Also known as a direct identifier. A direct identifier identifies an individual.Examples of direct identifiers include name, Social Security number, and phonenumber.
Quasi-identifier
   Also known an indirect identifier. When combined with other attributes, quasi-identifiers can be used to uniquely identify an individual. An example in whichan individual could be uniquely identified using quasi-identifiers is if a person’sage, gender, and zip code were available.
Sensitive information
   Includes personal attributes that aren’t directly or indirectly identifying, but areprivate and thus shouldn’t be disclosed. Examples include salary details and medical status.

At this time, data in Snowflake can only be classified as sensitive or as a direct/indirect identifier, but not both. Knowing this could affect how you design rules for governed access.

Once a Snowflake table or view has been analyzed, the values for the sys-tem tags are returned. To analyze columns in a table or view, generate se-mantic and privacy categories using Snowflake’s EXTRACT_SEMANTIC_CATEGORIES function. Next, the columns with seman-tic or privacy categories will need to be automatically or manuallytagged. We learned about manual tagging in the previous section. To au-tomatically tag columns that have semantic or privacy categories associ-ated with them, you can use the ASSOCIATE_SEMANTIC_CATEGORY_TAGS stored procedure.

Classification can be performed on all table and view types, including external tables, materialized views, and secure views. Snowflake supportsclassifying data for all data types except the GEOGRAPHY, BINARY, and VARIANT data types.

Data Masking

Revealing certain data to all users could have consequences, especially insituations where there are legal and compliance requirements for keep-ing the data private and secure. Restricting internal access to private orsensitive data reduces the risk that the data could be leaked intentionally or unintentionally. Using object tagging is a necessary first step to identifydata that needs to be secured. We can then use Snowflake’s native gover-nance controls such as data masking, column-level security, and row-levelpolicies. Row-level access controls

Snowflake masking policies are schema-level objects applied to individ-ual columns in tables or views. As such, a database and schema must ex-ist before a masking policy can be applied to a column. This column-levelsecurity feature uses the masking policy to selectively mask plain-textdata with either a partially masked value or a fully masked value.

Dynamic data masking

Using dynamic data masking, the masking policy is applied to the columnat query runtime. The masking policy can incorporate role hierarchyand/or be based on things like network security policies. Also, dynamicdata masking can be used with data sharing whereas external tokeniza-tion cannot.

we are going to create a role-basedmasking policy. We want to allow the HR role to view the plain-text datawhile all others will see a fully masked value. Remember that we’ll needto use the account administrator role to create a masking policy:

USE ROLE ACCOUNTADMIN;
CREATE OR REPLACE masking policy DEMO7_DB.HRDATA.emailmask 
AS 
( val string ) 
   returns string -> CASE WHEN current_role () in ( 'HR_ROLE' ) 
      THEN val 
	  ELSE '**MASKED**' END;
	  
ALTER TABLE DEMO7_DB.HRDATA.EMPLOYEES modify column EMAIL set masking policy DEMO7_DB.HRDATA.emailmask;

we’ll want to create asecond masking policy for Social Security numbers

CREATE OR REPLACE masking policy DEMO7_DB.HRDATA.SSNmask 
AS 
( val string ) 
   returns string -> CASE WHEN current_role () in ( 'HR_ROLE' ) 
      THEN val 
	  ELSE '**MASKED**' END;
	  
ALTER TABLE DEMO7_DB.HRDATA.EMPLOYEES modify column SSN set masking policy DEMO7_DB.HRDATA.SSNmask;

If we view the data with the ACCOUNTADMIN role, which has not beengranted the HR_ROLE, we’ll see masked data. This is intentional becauseSnowflake has removed the concept of a Snowflake super user who hasaccess to all data. This unique feature greatly reduces risk because it al-lows for thoughtful consideration of who should be granted access to dif-ferent pieces of sensitive information:

USE ROLE ACCOUNTADMIN;
SELECT * FROM DEMO7_DB.HRDATA.EMPLOYEES ;

Let’s confirm that the HR role can actually see the data. Be sure to log inas Adam, the new user, before issuing the following SQL statements. Ifyou do not log in as Adam, you’ll receive an error message if you try toexecute the command. That is because the user you are logged in as wasnot granted the HR_ROLE and, thus, you cannot use that role:

USE ROLE HR_ROLE;
USE WAREHOUSE COMPUTE_WH;SELECT * FROM DEMO7_DB.HRDATA.EMPLOYEES ;

INSERT INTO DEMO7_DB.HRDATA.EMPLOYEES ( emp_id, fname, lname, ssn, email, dept_id, dept ) 
VALUES ( 1, 'Harry', 'Smith', '111-11-1111', 'harry@coemail.com', '100', 'IT' ), 
( 2, 'Marsha', 'Addison', '222-22-2222', 'marsha@coemail.com', '100', 'IT' ), 
( 3, 'Javier', 'Sanchez', '333-33-3333', 'javier@coemail.com', '101', 'Marketing' ), 
( 4, 'Alicia', 'Rodriguez', '444-44-4444', 'alicia@coemail.com', '200', 'Finance' ), 
( 5, 'Marco', 'Anderson', '555-55-5555', 'marco@coemail.com', '300', 'HR' ), 
( 6, 'Barbara', 'Francis', '666-66-6666', 'barbara@coemail.com', '400', 'Executive');
 
SELECT * FROM DEMO7_DB.HRDATA.EMPLOYEES ;

A role that can view an unmasked data column has the ability to insert unmaskeddata into another column which might not be protected by data masking. 

Conditional masking

Conditional masking specifies two arguments, using one column to deter-mine whether data in another column should be masked. The first argu-ment indicates which column will be masked whereas the second and subsequent arguments are used as conditional columns.

we’ll want the employee ID to always be seen by the HRrole, but the employee ID for IT personnel should be the only employeeIDs available to everyone. All employee ID numbers other than IT shouldbe masked to everyone except HR:

USE ROLE ACCOUNTADMIN;
CREATE OR REPLACE masking policy DEMO7_DB.HRDATA.namemask 
   AS ( EMP_ID integer, DEPT_ID integer ) returns integer -> 
   CASE WHEN current_role () = 'HR_ROLE' 
      then EMP_ID WHEN DEPT_ID = '100' 
	  then EMP_ID ELSE '**MASKED**' END ;

Use the SELECT * command in both the main Snowflake account and asthe user Adam to see the difference in how the employee ID numbers are seen.

Static masking

Static masking is another way to mask Snowflake data. Rather than creat-ing the masking dynamically, we use ETL tools to transform a set of datato create one or more tables that always have certain columns masked.We’re then able to grant access to each table, built upon the same set ofunderlying data, based on specific roles. Static masking allows for a clearseparation of roles, but there is an increased cost, in terms of time andmoney, due to additional table maintenance and the need to create multi-ple, different tables.

Row Access Policies and Row-Level Security

Whereas data masking is column-based security, row access policies pro-vide dynamic row-level security. It is important to design these policies intandem because a column cannot have both a masking policy and a rowaccess policy at the same time. That is precisely why we want to keep the examples separate by using two different schemas and tables to demon-strate masking policies and row access policies.

Added to a table or view when created or after the object is created, rowaccess policies restrict the rows returned in the query result based on in-formation such as role and/or geographic region. While row access poli-cies can prevent rows from being returned in a query, they cannot pre-vent existing rows from being updated or new rows from being inserted.

One policy can be set on different tables and views, and row access poli-cies can range from simple to complex. An example of a complex row ac-cess policy would be a mapping table. To illustrate how row policieswork, we’ll create a row-level security example based on how two differ-ent roles would be given access to the same data. First we’ll create a map-ping table:

USE ROLE SYSADMIN;
USE DATABASE DEMO7_DB;
USE SCHEMA CH7DATA;
CREATE OR REPLACE TABLE AreaFiltering ( role_name text, area_id integer ); 
INSERT INTO AreaFiltering ( role_name, area_id ) 
VALUES 
( 'AREA1_ROLE', 1 ), 
( 'AREA2_ROLE', 2 ); 

SELECT * FROM AreaFiltering ;

Next, we’ll create a secure view to hold the data from the RATINGS tableso that that role can access specific roles for which it is allowed access

USE ROLE SYSADMIN;
CREATE OR REPLACE SECURE VIEW V_RATINGS_SUMMARY 
AS 
  SELECT emp_id, rating, dept_id, area 
  FROM RATINGS 
  WHERE area = ( SELECT area_id FROM AreaFiltering WHERE role_name = CURRENT_ROLE ());

We are still using the SYSADMIN role, so we won’t see the data in the se-cure view. To confirm

SELECT * FROM v_ratings_summary ;

Now let’s give the AREA1_ROLE the ability to see data in the secure view. The AREA1-ROLE will be able to see the data when the value in the areacolumn from the DEMO7_DB.CH7DATA.RATINGS table is equal to AREA1:

GRANT SELECT ON ALL TABLES IN SCHEMA CH7DATA TO ROLE AREA1_ROLE;
GRANT SELECT ON AreaFiltering TO ROLE AREA1_ROLE;
GRANT SELECT ON v_ratings_summary TO ROLE AREA1_ROLE ;

As the new user Adam, let’s try viewing the data using the AREA1 role

USE ROLE AREA1_ROLE;
USE DATABASE DEMO7_DB;
USE SCHEMA CH7DATA;
SELECT * FROM v_ratings_summary ;

Because there are three rows of data associated with AREA1, we expectthat Adam will be able to see three rows of data by using theAREA1_ROLE

Now let’s switch back to the main Snowflake user and grant privileges tothe AREA2 role

USE ROLE SYSADMIN;
GRANT SELECT ON ALL TABLES IN SCHEMA CH7DATA TO ROLE AREA2_ROLE;
GRANT SELECT ON AreaFiltering TO ROLE AREA2_ROLE;
GRANT SELECT ON v_ratings_summary TO ROLE AREA2_ROLE ;

We can once again switch back to Adam’s user account and confirm thatwe can use the AREA2 role to view the data associated with AREA2

USE ROLE AREA2_ROLE;
USE DATABASE DEMO7_DB;
USE SCHEMA CH7DATA;
SELECT * FROM v_ratings_summary ;

While it is possible for Adam to use the two area roles to view the data inthe secure view, it would be better if Adam could see at one time all thedata that he should be able to see. Fortunately, we have a way inSnowflake to make that happen. We’ll replace the original secure viewwith a new secure view that allows for a multicondition query. Be sureyou are in the main Snowflake user account and run the followingstatement.

USE ROLE SYSADMIN;
DROP VIEW v_ratings_summary ;

USE ROLE SYSADMIN;
USE DATABASE DEMO7_DB;
USE SCHEMA CH7DATA;

CREATE SECURE VIEW v_ratings_summary  
AS 
  SELECT emp_id, rating, dept_id, area FROM RATINGS 
  WHERE area IN (SELECT area_id FROM AreaFiltering 
                     WHERE role_name IN 
					 ( SELECT value FROM TABLE ( flatten( input => parse_json (CURRENT_AVAILABLE_ROLES ())))) );

we’ll give the necessary privileges to the roles:

GRANT SELECT ON AreaFiltering TO ROLE AREA1_ROLE;
GRANT SELECT ON v_ratings_summary TO ROLE AREA1_ROLE;
GRANT SELECT ON AreaFiltering TO ROLE AREA2_ROLE;
GRANT SELECT ON v_ratings_summary TO ROLE AREA2_ROLE ;

Now for the interesting part! Go to Adam’s user account and see whathappens if we use either of the AREA roles

USE ROLE AREA1_ROLE;
USE DATABASE DEMO7_DB;
USE SCHEMA CH7DATA;
SELECT * FROM v_ratings_summary ;

Using either of the AREA roles, Adam will be able to see the combined results from both the AREA1 and AREA2 query results

Let’s see what happens if we use Adam’s other role to query the secureview

USE ROLE HR_ROLE;
USE DATABASE DEMO7_DB;
USE SCHEMA CH7DATA;
SELECT * FROM v_ratings_summary ;

Adam receives the message that the view does not exist, or he is not au-thorized. This confirms for us that Adam can get full access to the resultsif he uses a role that has access to the view. If, instead, Adam uses a rolewhich does not have access to the view, he will not be able to see any results.

External Tokenization

External tokenization functionality, which uses external functions, isavailable in the Snowflake Standard Edition and above. If you want to in-tegrate your tokenization provider with Snowflake external tokenization,you’ll need to have Snowflake Enterprise Edition or higher.

Secure Views and UDFs

It is a best practice to use secure views and secure UDFs instead of di-rectly sharing Snowflake tables if sensitive data exists in a database. a secure SQL UDF was explained in Chapter 3 in market basket analysis. Secure data sharing approaches are where we use a mapping table to share data in a base table with many different accounts where we want to share spe-cific rows in the table with specific accounts.

We’ve previously been introduced to the PUBLIC schema available in allSnowflake databases. It is a best practice to use a public schema for se-cure views and a private schema for base tables.

Object Dependencies

The Snowflake object dependencies feature allows you to identify dependencies among Snowflake objects and is frequently used for impact analysis, data integrity assurance, and compliance purposes. An object dependency is established when an existing object has to refer-ence some metadata on its behalf or for at least one other object. A depen-dency can be triggered by an object’s name, its ID value, or both.

One example of an object dependency is a Snowflake external stage where theID is used as a reference to the external storage location.

It is important for data engineers to be aware of the object dependenciesbefore making changes to Snowflake architecture. Before an engineer re-names a table or drops a table, for example, they could notify users ofdownstream views which are dependent on those tables. In this way, theengineer could get the feedback necessary to assess the impact of modifi-cations before actually making changes.

Additionally, data analysts and data scientists can look upstream to seethe lineage of a view. Being able to do so now, with object dependenciesfunctionality, gives them the confidence that the view is backed by atrustworthy object source. Finally, the new object dependencies feature isimportant for compliance officers and auditors who need to be able totrace data from a given object to its original data source to meet compli-ance and regulatory requirements.

Object dependencies appears as a view in the Snowflake Account Usageschema. A Snowflake Account Administrator can query theOBJECT_DEPENDENCIES view and the output can include the object namepath, the referencing object name and domain, along with the referencedobject name and domain.

Snowflake’s object dependencies feature is one of the many comprehen-sive data governance–native features provided by Snowflake. This suiteof features makes it easy to efficiently know and protect your data. This isimportant because governing your data is critical to ensure that yourdata is secure, trustworthy, and compliant with regulatory requirements.

USE ROLE ACCOUNTADMIN;
DROP DATABASE DEMO7_DB;
DROP USER ADAM ;

Chapter 8. Managing SnowflakeAccount Costs

Snowflake includes tools, such as resource monitors, which allow you tocap actual costs based on your initial plan.
One of the greatest benefits of usage-based pricing is that it provides youan opportunity to transparently see and understand how your organiza-tion is charged for storing and using data. This often leads to changes thatwill immediately begin to lower costs. Snowflake’s usage-based billingand Snowflake object tagging make it easy to assign appropriate costs tothe appropriate accounts or departments.

Snowflake’s pricing model is straightforward, transparent, and easy tounderstand. Snowflake has unique func-tionality which can result in decreased costs. One example is zero-copy cloning, which offers the ability to duplicate an object without actuallycreating a physical copy. This means no storage costs are assessed for theclone. Snowflake’s native functionality allows you to setlimits on your credit consumption costs. These Snowflake resource moni-tors can be set to take actions such as notifying or suspending consump-tion. Additionally, Snowflake offers a lot of usage data for companies tounderstand their costs, and this makes customers on Snowflake more cost-effective.

A best practice is to use theSYSADMIN role to create the new virtual warehouses:

USE ROLE SYSADMIN;
USE WAREHOUSE COMPUTE_WH;
CREATE OR REPLACE WAREHOUSE VW2_WH WITH WAREHOUSE_SIZE=MEDIUM AUTO_SUSPEND=300 AUTO_RESUME=true, INITIALLY_SUSPENDED=true;
CREATE OR REPLACE WAREHOUSE VW3_WH WITH WAREHOUSE_SIZE=SMALL AUTO_SUSPEND=300 AUTO_RESUME=true, INITIALLY_SUSPENDED=true;
CREATE OR REPLACE WAREHOUSE VW4_WH WITH WAREHOUSE_SIZE=MEDIUM AUTO_SUSPEND=300 AUTO_RESUME=true, INITIALLY_SUSPENDED=true;
CREATE OR REPLACE WAREHOUSE VW5_WH WITH WAREHOUSE_SIZE=SMALL AUTO_SUSPEND=300 AUTO_RESUME=true, INITIALLY_SUSPENDED=true;
CREATE OR REPLACE WAREHOUSE VW6_WH WITH WAREHOUSE_SIZE=MEDIUM AUTO_SUSPEND=300 AUTO_RESUME=true, INITIALLY_SUSPENDED=true ;

Organizations can create and manage multiple Snowflake accounts. EachSnowflake account incurs fees that are summarized in the organization’smonthly bill. Your total Snowflake monthly bill includes three distinctcharges per account: storage fees, data transfer costs, and credits consumed. 

Storage Fees

Snowflake monthly storage fees are calculated based on the average ter-abytes used per month, based on periodic snapshots of all data stored inan account. You’ll notice that capacity pricing offers steep discounts forupfront payments.

Note that while storage pricing is a consideration of which region to se-lect for your Snowflake account, you’ll also want to consider which re-gion could minimize latency and give you access to any features you mayrequire. It’s important to note that if you decide later to move your datato a different region, data transfer costs will be
incurred.

Try to make more use of transient tables as they are not maintained in the historytables, which in turn reduces the data storage costs for history tables.

Data Transfer Costs

Data transfer costs are assessed when data stored in Snowflake is trans-ferred from one region to another, within the same cloud platform, orwhen data is transferred out of the cloud.

Snowflake does not charge data ingress fees. The data egress fees you pay are a re-imbursement to Snowflake coming from the three cloud data providers applyingdata egress charges when data is transferred.

Data egress charges apply only when unloading data from Snowflake,when external functions are used, or when replicating data to aSnowflake account in a region or cloud platform that is different fromwhere your primary Snowflake account is hosted.

Compute Credits Consumed

The consumption of resources on Snowflake is paid for with Snowflake credits, which are a unit of measure. That measure is the equivalent of one hour of usage by a single compute node.

While one credit is equivalent to one hour, the actual billing for virtual ware-houses is on a per-second basis with a one-minute minimum.

You can purchase credits as you go, on demand. The price of on-demandcredits is dependent on which cloud provider, region, and service editionis selected. Alternatively, you can prepurchase credits at a negotiated dis-counted rate.

A Snowflake credit is consumed only when you use resources. Snowflakeresources are used, or consumed, in one of three ways: running virtualwarehouses, when the cloud services layer is performing work, or usingserverless features such as automatic clustering or maintaining material-ized views.

The credits consumed for Snowflake running virtual warehouses arebased on the size of the warehouse. For example, the credit consumptionfor an X-Small virtual warehouse is billed at one Snowflake credit perhour of usage.

The credits consumed for the cloud services layer are free for up to 10%of the consumed daily virtual warehouse compute. Anything above the10% daily threshold for cloud services layer consumption is billed and ap-pears in your Snowflake account as a charge to theCLOUD_SERVICES_ONLY virtual warehouse.

The credits consumed for using serverless features are billed at a dis-tinctly different rate, depending on the serverless feature.

Snowflake’s many native features, such as micro-partitions and automaticcompute clustering, lessen the need for credit consumption, which inturn decreases overall costs. Snowflake’s highly elastic compute and per-second billing model also helps keep credit consumption charges low.

Creating Resource Monitors to ManageVirtual Warehouse Usage and ReduceCosts

A Snowflake virtual warehouse consumes credits when it’s in use. Thenumber of Snowflake credits consumed by a virtual warehouse dependson two factors: the size of the virtual warehouse and how long it runs. Virtual warehouse consumption is billed on a per-second basis, with a 60-secondminimum. A resource monitor is a first-class Snowflake object that can be applied toboth user-managed virtual warehouses and virtual warehouses used bycloud services.

A Snowflake resource monitor has the following properties:
  Credit Quota (Number of Snowflake credits)
  Credit Usage (Total Snowflake credits consumed during each billingperiod)
    Monitor Level: account or specific virtual warehouse(s)
    Schedule
      Frequency: daily, weekly, monthly, yearly, never
      Start and End, although it is uncommon to have an end value
  Triggers (Actions)
    Notify
    Notify and Suspend
    Notify and Suspend Immediately




Resource Monitor Credit Quota

An account-level resource monitor credit quota, stated in terms ofSnowflake credits, is based on the entire amount of cloud service usage.However, the warehouse-level credit quota does not consider the daily10% adjustment for cloud services. Only an account-level resource monitor can monitor credit usage for virtual warehouses that provide cloudservices. Resource monitors, at the virtual warehouse level, can only be created for user-created and user-managed virtual warehouses.

When a credit quota threshold is reached for a virtual warehouse whose resourcemonitor action is set to Suspend Immediately, additional Snowflake credits mightstill be consumed. Thus, you might want to consider utilizing buffers in quotathresholds for trigger actions if you want to strictly enforce your quotes. For ex-ample, set your threshold for 95% instead of 100% to ensure that your credit us-age won’t exceed your actual quota.

Resource Monitor Credit Usage

As stated, resource monitors can be created at the account level or thevirtual warehouse level.

Account-level resource monitors only control the credit usage of virtual ware-houses created in your account; not Snowflake-provided virtual warehouses for serverless features such as those used for Snowpipe, automatic clustering, or materialized views.

Snowflake resource monitors can be created to impose limits on the num-ber of Snowflake credits that are consumed, either for a specified intervalor for a date range. For a default schedule, a resource monitor starts tracking the assignedvirtual warehouse as soon as it is created, and the credit usage trackingresets to 0 at the beginning of each calendar month. Customizing the schedule so that the resource monitor resets at a specificinterval, such as daily or weekly, requires that you select both a fre-quency and a start time. Selecting an end time is optional.

Resource Monitor Notifications and Other Actions

Resource monitors can be created to trigger specific actions, such as send-ing alert notifications and/or suspending a virtual warehouse. There is a limit of five Notify actions, one Suspend action, and one SuspendImmediately action for each resource monitor.

By default, resource monitor notifications are not enabled.ACCOUNTADMIN role users can receive notifications only if they providea verified valid email address and enable notifications.

To enable notifications via the web interface, make sure your role is set to ACCOUNTADMIN. You can select Preferences and then Notifications.You’ll need to choose your notification preference as Web, Email, or All. Ifyou select Email or All, you’ll be prompted to enter a valid email address.

The difference between the Suspend action and the Suspend Immediately action has to do with whether statements being executed by the virtualwarehouse(s) will be allowed to complete or not.

When a trigger causes a virtual warehouse to suspend, the suspended vir-tual warehouse cannot be resumed until one of the following conditions occurs:

The resource monitor is no longer assigned to the virtual warehouseor is dropped.
Either the credit quota for the monitor or the credit threshold for thetrigger is increased.
A new monthly billing cycle starts.

Resource Monitor Rules for Assignments

1) An account-level resource monitor can be set at the account level tocontrol credit usage for all virtual warehouses in an account; how-ever, there can be only one account-level resource monitor.
2) There is a one-to-many relationship between virtual warehouses andvirtual warehouse–level resource monitors. Each virtual warehousecan be assigned to only one virtual warehouse–level resource monitor;one resource monitor can be assigned to one or more virtualwarehouses.
 2a) Virtual warehouses that share the same resource monitor also share the same thresholds.
3) The credit quota and credit usage properties are both required to cre-ate a resource monitor. If a schedule is not explicitly stated, the defaultschedule applies. By default, the credit quota resets at the beginning of each calendar month. Setting triggers is optional.
4) Before it can perform monitoring actions, a resource monitor must beassigned to a virtual warehouse or account after it is created.

create aresource monitor plan for a large enterprise withabout 10,000 employees. Assume we want to limit our credit consumption to 5,000 credits. We’llneed to create an account-level resource monitor in which the credit quota equals 5,000, and we’ll want to be notified when that credit quotahas been reached. We’ll elect to use the default schedule.

Out of the six virtual warehouses in our account, we’ve identified two pri-ority virtual warehouses for which we want to allocate half our creditbudget. This means 2,500 credits are being reserved from the 5,000 totalcredits.

The other four virtual warehouses will need to split the remain-ing 2,500 credits, and we’ll create virtual warehouse–level resource moni-tors to notify and suspend whenever the credit quota has been reached.Additionally, we’ve decided to be notified when the second of our priorityvirtual warehouses reaches a credit quota of 1,500.

DDL Commands for Creating and Managing Resource Monitors

By default, Snowflake resource monitors can only be created, viewed, andmaintained by the ACCOUNTADMIN role. Although the creation ofSnowflake resource monitors is exclusive to the ACCOUNTADMIN role, a Snowflake account administrator can enable other roles to view and modify resource monitors. If you’re using a role other than the ACCOUNTADMIN role, that role willneed to have two privileges (MODIFY and MONITOR) in order for you to view and edit resource monitors:

CREATE RESOURCE MONITOR, ALTER RESOURCE MONITOR, SHOW RESOURCE MONITOR, DROP RESOURCE MONITOR - SQL statements 

To create a resource monitor, you need to assign at least one virtual ware-house to the resource monitor, unless you are setting the monitor at theaccount level. using SQL you’ll have to create the re-source monitor first and then use the ALTER WAREHOUSE command to as-sign one or more virtual warehouses to the resource monitor.

Let’s first create an account-level resource monitor.

USE ROLE ACCOUNTADMIN;
CREATE OR REPLACE RESOURCE MONITOR MONITOR1_RM 
  WITH CREDIT_QUOTA = 5000 
  TRIGGERS on 50 percent do notify 
           on 75 percent do notify 
		   on 100 percent do notify 
		   on 110 percent do notify 
		   on 125 percent do notify ;

After we create the re-source monitor, we’ll need to assign it to our Snowflake account:

USE ROLE ACCOUNTADMIN;
ALTER ACCOUNT SET RESOURCE_MONITOR = MONITOR1_RM ;

USE ROLE ACCOUNTADMIN;
CREATE OR REPLACE RESOURCE MONITOR MONITOR5_RM 
  WITH CREDIT_QUOTA = 1500 
  TRIGGERS on 50 percent do notify 
           on 75 percent do notify 
		   on 100 percent do notify 
		   on 110 percent do notify on 
		   125 percent do notify;

ALTER WAREHOUSE VW2_WH SET RESOURCE_MONITOR = MONITOR5_RM ;

USE ROLE ACCOUNTADMIN;
SHOW RESOURCE MONITORS ;

USE ROLE ACCOUNTADMIN;
CREATE OR REPLACE RESOURCE MONITOR MONITOR2_RM 
  WITH CREDIT_QUOTA = 500 
  TRIGGERS on 50 percent do notify 
           on 75 percent do notify 
		   on 100 percent do notify 
		   on 100 percent do suspend 
		   on 110 percent do notify on 110 percent do suspend_immediate;  -- Suspend option

ALTER WAREHOUSE VW3_WH SET RESOURCE_MONITOR = MONITOR2_RM ;

We know that a virtual warehouse canonly be assigned to one resource monitor, so we expect that our secondattempt at assigning a resource monitor to the same virtual warehouse will result in either a failed command or the second resource monitor as-signment overriding the first assignment.

USE ROLE ACCOUNTADMIN;
CREATE OR REPLACE RESOURCE MONITOR MONITOR6_RM 
  WITH CREDIT_QUOTA = 500 
  TRIGGERS on 50 percent do notify 
           on 75 percent do notify 
		   on 100 percent do notify 
		   on 100 percent do suspend 
		   on 110 percent do notify on 110 percent do suspend_immediate;
		   
ALTER WAREHOUSE VW6_WH SET RESOURCE_MONITOR = MONITOR6_RM ;

USE ROLE ACCOUNTADMIN;
CREATE OR REPLACE RESOURCE MONITOR MONITOR4_RM 
  WITH CREDIT_QUOTA = 500 
  TRIGGERS on 50 percent do notify 
           on 75 percent do notify 
		   on 100 percent do notify 
		   on 100 percent do suspend 
		   on 110 percent do notify on 110 percent do suspend_immediate;

ALTER WAREHOUSE VW6_WH SET RESOURCE_MONITOR = MONITOR4_RM;   -- This overrides previous assignment for VW6_WH

USE ROLE ACCOUNTADMIN;
SHOW RESOURCE MONITORS ;

We can see that the sixth resource monitor appears to have nullifiedwhen we assigned the fourth resource monitor to the virtual warehouse. We can confirm that by using the SHOW WAREHOUSES
command:

USE ROLE ACCOUNTADMIN;
SHOW WAREHOUSES ;

We can now get rid of the resource monitor we created in error:

DROP RESOURCE MONITOR MONITOR6_RM ;

USE ROLE ACCOUNTADMIN;
CREATE OR REPLACE RESOURCE MONITOR MONITOR3_RM 
  WITH CREDIT_QUOTA = 1500 
  TRIGGERS on 50 percent do notify 
           on 75 percent do notify 
		   on 100 percent do notify 
		   on 100 percent do suspend 
		   on 110 percent do notify on 110 percent do suspend_immediate;

ALTER WAREHOUSE VW4_WH SET RESOURCE_MONITOR = MONITOR3_RM;
ALTER WAREHOUSE VW5_WH SET RESOURCE_MONITOR = MONITOR3_RM ;

USE ROLE ACCOUNTADMIN;
SHOW RESOURCE MONITORS ;
SHOW WAREHOUSES ;

We can also take it a step further to get a list ofany of our virtual warehouses that have not been assigned to a virtualwarehouse–level resource monitor. 

SHOW WAREHOUSES ;
SELECT "name", "size" FROM TABLE ( RESULT_SCAN ( LAST_QUERY_ID ())) WHERE "resource_monitor" = 'null' ;

One final note about resource monitors is that for Snowflake provider ac-counts that have created reader accounts, there exists aRESOURCE_MONITORS view which can be used to query resource moni-tor usage of the reader account.

Using Object Tagging for Cost Centers

In addition to using object tags for security reasons, Snowflake tags canbe created for objects to help track resource usage. For example, cost cen-ter tags could have values such as accounting, finance, marketing, and en-gineering. In that case, cost center object tagging would allow for moregranular insight, at a department level. Additionally, tags can be used fortracking and analyzing resource usage by specific short-term or long-term projects.

Querying the ACCOUNT_USAGE View

To get more granular detail about virtual warehouse costs, we can lever-age queries on the Snowflake ACCOUNT_USAGE view 
This query provides the individual details of cost, based on virtual ware-house start times and assuming $3.00 as the credit price:

USE ROLE ACCOUNTADMIN;
SET CREDIT_PRICE = 3.00;
USE DATABASE SNOWFLAKE;
USE SCHEMA ACCOUNT_USAGE;
SELECT WAREHOUSE_NAME, START_TIME, END_TIME, CREDITS_USED, ($CREDIT_PRICE * CREDITS_USED) AS DOLLARS_USED 
  FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY ORDER BY START_TIME DESC;

This query summarizes the costs of each virtual warehouse for the past30 days, assuming $3.00 as the credit price as set in the previous query

SELECT WAREHOUSE_NAME, SUM ( CREDITS_USED_COMPUTE ) AS CREDITS_USED_COMPUTE_30DAYS, 
   ($CREDIT_PRICE * CREDITS_USED_COMPUTE_30DAYS) AS DOLLARS_USED_30DAYS 
     FROM ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY 
	 WHERE START_TIME >= DATEADD ( DAY, - 30, CURRENT_TIMESTAMP ()) 
	 GROUP BY 1 
	 ORDER BY 2 DESC ;

The previous code samples provided an example of how you can querythe ACCOUNT_USAGE view to monitor virtual warehouse metering forcompute warehouse credits consumed. Similar queries can be created tomonitor compute credits consumed from Snowpipe, clustering, material-ized views, search optimization, and replication.

Using BI Partner Dashboards to Monitor Snowflake Usage and Costs

If you have access to the Snowflake business intelligence (BI) and analyt-ics partners tools, you’ll be able to access one or more plug-and-play dash-boards to help you monitor your Snowflake usage. Here is a list of someof the prebuilt dashboards that are currently available

Tableau Dashboards : Compute cost overview
Sigma : Compute cost, storage cost, and Snowflake cost (reader accounts)
Looker : Snowflake cost and usage analysis
Microsoft Power BI :  Snowflake usage report
Qlik : Snowflake usage dashboard

Snowflake Agile Software Delivery

DevOps is a term used to describe the set of practices that are establishedwith the goals of high software quality and helping software developersand IT operations shorten the systems development lifecycle and providecontinuous delivery. Database change management (DCM) is a subset ofDevOps that specifically focuses on the changes made to databases. DCMhas unique challenges, but the good news is that Snowflake features, suchas zero-copy cloning and Time Travel, make it considerably easier forteams to use an agile approach for DevOps.

Why Do We Need DevOps?

It is crucial that an audit trail of software and application changes bemaintained, because of regulatory and compliance requirements applica-ble to an organization’s software systems and data. For many reasons, itis also important that any bug fixes be resolved quickly. New features andrequested enhancements should also be released frequently to give orga-nizations a competitive advantage in today’s fast-paced environment, es-pecially given the vast number of powerful software application develop-ment tools available. Achieving these things requires careful considera-tion. And most importantly, it requires automation.

Automation of software development, testing, and release is the mostlikely way to innovate more quickly, deliver improved applications ingreater numbers, adhere to regulatory requirements, and obtain greaterreturn on investment. It is the objective of DevOps to minimize the man-ual aspects of software development, testing, and deployment.

Continuous Data Integration, Continuous Delivery, and Continuous Deployment

In general terms, continuous integration is a DevOps software develop-ment practice whereby developers merge their code regularly into a cen-tral repository. Continuous delivery occurs when application changes areautomatically tested and uploaded to a source control repository, such asGitHub, where the operations team can then deploy them to production.  Continuous deployment automates the next stage in the pipeline by auto-matically releasing changes from the repository to production.

What Is Database Change Management?

Database DevOps is often referred to as database change management(DCM). Database DevOps, especially for virtual warehouses, have someunique challenges as compared to the difficulties of DevOps for softwareapplications. Snowflake’s unique architecture solves for many of theseproblems. Specific DCM tools are also available which work easily withSnowflake.

Overcoming the unique challenges of database changes

Making changes to databases presents its own unique set of challenges,especially for traditional on-premises databases, which have to deal withthe problems of downtime for hardware upgrades and with explicitlyhaving to back up databases beforehand so that they will be available forrestoration, if needed. In the event a rollback is needed, an operationsfreeze with no work happening during the restoration process is re-quired. In addition, there is the problem of drift whereby developerscould be working on a different version of the database than the versionin production, and there is also the problem of load balancing.

With Snowflake, these top challenges no longer exist for users. Hardwarefixes and upgrades are handled by the Snowflake team, with no impacton users. For rollback and version control, Snowflake has Time Traveland fail-safe, which eliminate the need to make traditional data backups.Time Travel and fail-safe are part of Snowflake’s continuous data protec-tion lifecycle. To alleviate the problem of drift, Snowflake users can uti-lize zero-copy cloning, described in the previous section, to simplify theloading and seeding of databases in different environments.

Given all this, and because most everything in Snowflake can be auto-mated via SQL, the DCM process could be handled quite simply inSnowflake by creating a lightweight DevOps framework for small deploy-ments. For a more heavyweight Snowflake DevOps framework, however,there are DCM tools available.

DCM tools for a heavyweight Snowflake DevOps framework

DCM refers to a set of processes and tools used to manage objects within adatabase. The most important tool to consider for Snowflake DCM is asource control repository and pipeline tool. Examples of hosted optionsfor DCM include AWS CodeCommit and CodePipeline, Azure DevOps, GitHub Actions, and Bitbucket.

In addition to a source control repository and pipeline tool, you’ll needdatabase migration tools to deploy the changes to the database. SeveralDCM tools can be used with Snowflake. Some are primarily SaaS tools ordata transformation tools, and many of them are community developed.Examples of tools that can be used as a database migration tool includeSqitch, Flyway, snowchange, schemachange, SqlDBM, Jenkins, and dbt.

Lastly, a testing framework such as JUnit, pytest, or DbFit is needed.

How Zero-Copy Cloning Can Be Used to SupportDev/Test Environments

Because zero-copycloning is a metadata-only operation, there is no additional storagecharge for Snowflake cloned objects unless and until changes are made. Zero-copy clones are frequently used to support working in-developmentand test environments as part of the development lifecycle. Changes to a Snowflake pro-duction environment can take the form of new development. Creating anew table is one example. Changes could also be in the form of enhance-ments to an existing object in production.

In our example, let’s assume that Table A already exists in our productionenvironment, and we want to add an additional table in production.What we’ll do first is create a clone of the production database in whichwe can develop. That is where we’ll create the new table. We’ll also wantto create a clone of the development database in which we can performour testing, or quality assurance activities

Let’s first create some specific roles for our production, development, andQA environments. As a best practice, we’ll assign the new custom rolesback to the SYSADMIN role:

USE ROLE SECURITYADMIN;
CREATE OR REPLACE ROLE PROD_ADMIN;
CREATE OR REPLACE ROLE DEV_ADMIN;
CREATE OR REPLACE ROLE QA_ADMIN;
GRANT ROLE PROD_ADMIN TO ROLE SYSADMIN;
GRANT ROLE DEV_ADMIN TO ROLE SYSADMIN;
GRANT ROLE QA_ADMIN TO ROLE SYSADMIN ;

Next, we’ll want to give some privileges to the new roles so that they cancreate databases on the account

USE ROLE SYSADMIN;
GRANT CREATE DATABASE ON ACCOUNT TO ROLE PROD_ADMIN;
GRANT CREATE DATABASE ON ACCOUNT TO ROLE DEV_ADMIN;
GRANT CREATE DATABASE ON ACCOUNT TO ROLE QA_ADMIN ;

Let’s assume that we want to assign a separate virtual warehouse to eachadministrator. We can rename three of the virtual warehouses that wecreated earlier:

USE ROLE SYSADMIN;
ALTER WAREHOUSE IF EXISTS VW2_WH RENAME TO WH_PROD;
ALTER WAREHOUSE IF EXISTS VW3_WH RENAME TO WH_DEV;
ALTER WAREHOUSE IF EXISTS VW4_WH RENAME TO WH_QA;
SHOW WAREHOUSES ;

Let’s grant usage for these virtual warehouses toeach associated role:

USE ROLE ACCOUNTADMIN;
USE WAREHOUSE COMPUTE_WH;
GRANT USAGE ON WAREHOUSE WH_PROD TO ROLE PROD_ADMIN;
GRANT USAGE ON WAREHOUSE WH_DEV TO ROLE DEV_ADMIN;
GRANT USAGE ON WAREHOUSE WH_QA TO ROLE QA_ADMIN ;

We need to create a production environment database, schema, and ta-ble. We’ll also grant usage on the database to the development administrator

USE ROLE PROD_ADMIN;
USE WAREHOUSE WH_PROD;
CREATE OR REPLACE DATABASE PROD_DB;
CREATE OR REPLACE SCHEMA CH8_SCHEMA;
CREATE OR REPLACE TABLE TABLE_A ( Customer_Account int, Amount int, transaction_ts timestamp ); GRANT USAGE ON DATABASE PROD_DB TO ROLE DEV_ADMIN ;

Now the development administrator can create a clone of the productiondatabase and grant usage of the cloned database to the QA administratorrole

USE ROLE DEV_ADMIN;
USE WAREHOUSE WH_DEV;
CREATE OR REPLACE DATABASE DEV_DB CLONE PROD_DB;
GRANT USAGE ON DATABASE DEV_DB TO ROLE QA_ADMIN ;

The QA administrator now creates a clone of the development databaseand grants usage to the production administrator role

USE ROLE QA_ADMIN;
USE WAREHOUSE WH_QA;
CREATE OR REPLACE DATABASE QA_DB CLONE DEV_DB;
GRANT USAGE ON DATABASE QA_DB TO ROLE PROD_ADMIN ;

Now we’re ready for the development work. Back in the developmentdatabase, we use the DEV_ADMIN role to create a new developmentschema and a new table

USE ROLE DEV_ADMIN;
USE WAREHOUSE WH_DEV;
USE DATABASE DEV_DB;
CREATE OR REPLACE SCHEMA DEVELOPMENT;
CREATE OR REPLACE TABLE TABLE_B ( Vendor_Account int, Amount int, transaction_ts timestamp );
GRANT USAGE ON SCHEMA DEVELOPMENT TO ROLE QA_ADMIN;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA DEVELOPMENT TO ROLE QA_ADMIN;

Once development is done, the QA team can do their testing by creating anew table in the testing environment from the newly created table in the development environment:

USE ROLE QA_ADMIN;
USE WAREHOUSE WH_QA;
USE DATABASE QA_DB;
CREATE OR REPLACE SCHEMA TEST;
CREATE OR REPLACE TABLE QA_DB.TEST.TABLE_B AS SELECT * FROM DEV_DB.DEVELOPMENT.TABLE_B;
GRANT USAGE ON SCHEMA TEST TO ROLE PROD_ADMIN;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA TEST TO ROLE PROD_ADMIN;

The new table in the QA environment could be created using either the CREATETABLE AS SELECT (also known as CTAS) command, or the CREATE TABLE LIKE command. The difference between the two is that the latter creates an empty copyof the existing table whereas the former creates a populated table:

When the QA team has completed their testing, the production adminis-trator can then copy the table into production:

USE ROLE PROD_ADMIN;
USE WAREHOUSE WH_PROD;
USE DATABASE PROD_DB;
USE SCHEMA CH8_SCHEMA;
CREATE OR REPLACE TABLE TABLE_B AS SELECT * FROM QA_DB.TEST.TABLE_B ;

For development that involves enhancing an existing object, you’ll mostlikely want to use a combination of zero-copy cloning and Time Travel.You can clone the production database as it was a certain number of daysprior, make the enhancements to the tables using scripts, and then rerunthe data loads for the prior number of days. When you compare the re-sults, the only difference between the two should be the enhancements.

Code Cleanup

USE ROLE ACCOUNTADMIN;
DROP DATABASE DEV_DB;
DROP DATABASE PROD_DB;
DROP DATABASE QA_DB;
DROP ROLE PROD_ADMIN;
DROP ROLE DEV_ADMIN;
DROP ROLE QA_ADMIN;
DROP RESOURCE MONITOR MONITOR1_RM;
DROP RESOURCE MONITOR MONITOR2_RM;
DROP RESOURCE MONITOR MONITOR3_RM;
DROP RESOURCE MONITOR MONITOR4_RM;
DROP RESOURCE MONITOR MONITOR5_RM;
DROP WAREHOUSE WH_PROD;
DROP WAREHOUSE WH_DEV;
DROP WAREHOUSE WH_QA;
DROP WAREHOUSE VW5_WH;
DROP WAREHOUSE VW6_WH ;

Chapter 9. Analyzing and Improving Snowflake Query Performance

Features such as micro-partitions, a searchoptimization service, and materialized views are examples of uniqueways Snowflake works in the background to improve performance. Snowflake also makes it possible to easily analyze query performancethrough a variety of different methods. We’ll learn about some of themore common approaches to analyzing Snowflake query performance,such as query history profiling, the hash function, and the Query Profile tool.

Analyzing Query Performance

There are many different ways toanalyze Snowflake query performance. In this section, we’ll look at three of them: QUERY_HISTORY profiling, the HASH() function, and using the web UI’s history.

QUERY_HISTORY Profiling

The QUERY_HISTORY table function used on theINFORMATION_SCHEMA provides very useful information about thequeries executed against a particular database.

You can execute the following query to obtain details about all queriesrun by the current user in the past day, with the records returned in de-scending order of the total elapsed time:

USE ROLE ACCOUNTADMIN;
USE DATABASE LEARNING_SQL;
SELECT * FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY (dateadd ('days', - 1, current_timestamp()), current_timestamp())) ORDER BY TOTAL_ELAPSED_TIME DESC;

The results of this query give you what you need in order to discover long-running queries that are run frequently.

It is possible to narrow the focus of the query history by selecting a par-ticular user, session, or virtual warehouse you wish to know more about.Those functions are as follows:

QUERY_HISTORY_BY_USER
QUERY_HISTORY_BY_SESSION
QUERY_HISTORY_BY_WAREHOUSE

HASH() Function

Snowflake’s hash function is a utility function that returns information,such as descriptions of queries. It is not a cryptographic hash function. The following query was written to return the queries executed for a par-ticular database, in order of frequency and average compilation time. Inaddition to average compilation time, the average execution time is also included to compare the two times:

USE ROLE ACCOUNTADMIN;
USE DATABASE LEARNING_SQL;
SELECT HASH (query_text), QUERY_TEXT, COUNT(*), AVG (compilation_time), AVG (execution_time) FROM TABLE (INFORMATION_SCHEMA.QUERY_HISTORY (dateadd ('days', - 1, current_timestamp ()), current_timestamp ())) 
GROUP BY HASH (query_text), QUERY_TEXT 
ORDER BY COUNT (*) DESC, AVG (compilation_time) DESC ;

Web UI History

We can easily review individual queries using the Snowflake web inter-face. The graphical representation Snowflake provides allows us to viewthe main components of the query’s processing plan and the statistics foreach component. We can also see the overall statistics of the query.

Using Snowflake’s Query Profile tool

Snowflake’s Query Profile tool provides the execution details of a query toassist you in spotting common mistakes in SQL query expressions. Youcan use Query Profile to learn more about the performance or behaviorof the query, which will help you locate potential performance bottle-necks and identify opportunities for improvement.

One of the things you’ll notice is that statements such as USE and DESCRIBE and GRANT do not have a query profile it is Only Metadata operation.

Understanding Snowflake Micro-Partitions and Data Clustering

Data stored in Snowflake tables is divided into micro-partitions dynami-cally. There is no need to manually define or maintain partitions, as is thecase in traditional partitioning. Snowflake automatically takes care ofthat for you. At the time that the data is loaded into a Snowflake table, the metadata about each micro-partition is stored so that it can be leveraged to avoid unnecessary scanning of micro-partitions when querying the data.

Partitions Explained

Partitioning a relational database will result in large tables divided intomany smaller parts, or partitions. Databases are most often partitioned toimprove scalability, assist in backup and recovery, improve security, andimprove query performance. For a partitioned database, the query onlyhas to scan relevant partitions, not all the data. This reduces the response time needed to read and load data for SQL operations.

There are two traditional methods by which relational databases can bepartitioned: horizontal and vertical. horizontal partitioning example. with an original table consisting of eight columns and eightrows representing customers’ purchases. Horizontal partitioning, also known as sharding, stores table rows in dif-ferent database clusters. Each partition has the same schema and col-umns but contains different rows of data (as shown in Figure 9-6). The most important benefit of sharding is that it allows an otherwise too-largedatabase the ability to scale out horizontally.

Snowflake is column based andhorizontally partitioned, and the Snowflake multicluster virtual ware-house feature means that sharding isn’t needed to scale out a database. 

The second type of partitioning is vertical partitioning, also known as col-umn-based partitioning. Vertical partitioning is undertaken by creatingtables with fewer columns and storing the remaining columns in addi-tional tables; a practice commonly known as row splitting. Vertical partitioning is a physical optimization technique, but it can becombined with a conceptual optimization technique known as normalization. Normalization removes redundant columns from a table and putsthem into another table while linking them with a foreign key relation-ship. In our examples, the foreign keys will be the customer ID (CID) andproduct ID (PID) fields.

Continuing with the same example, we’ll create two vertical partitions:one for customer data and one for transaction data.

Probably the most common vertical partitioning method is to split staticdata from dynamic data. The intent is to reduce performance costs basedon how frequently the data is accessed and how often the table changes.

One way the data can be modeled is to store the static or slowly changinginformation in one table and the dynamic or frequently changing infor-mation in a different table. Modeling data in thisway is especially beneficial when using Snowflake because you’ll be ableto take advantage of Snowflake features, such as caching, to help improveperformance and lower costs.

To summarize, horizontal partitioning decomposes tables row-wise.Horizontal partitions include all fields and hold a specific subset of thedata. In contrast, vertical partitioning decomposes tables column-wise.Vertical partitions hold all the relevant data relating to a subset of fields.Vertical partitions that include a foreign key, linking them to another par-tition, are taking advantage of normalization.

Snowflake Micro-Partitions Explained

All Snowflake data is stored in database tables. Logically, tables are struc-tured as collections of rows and columns. This logical structure of rowsand columns is mapped to the Snowflake physical table structures knownas micro-partitions. Snowflake’s unique micro-partitioning approach istransparent to users, unlike traditional data warehouse partitioning approaches in which the user has to independently manipulate partitionsby using Data Definition Language (DDL) commands.

Snowflake users cannot partition their own tables. Instead, all Snowflake tablesare automatically divided into micro-partitions using the ordering of the data as itis inserted or loaded.

A Snowflake micro-partition is a small physical cloud storage block thatstores between 50 MB and 500 MB of uncompressed data. However, theactual storage block size is even smaller because Snowflake data is al-ways stored compressed, and Snowflake automatically determines themost efficient compression algorithm for columns in each micro-parti-tion. As a result, a Snowflake micro-partition holds about 16 MB of com-pressed data.

There are many benefits of having a small micro-partition size in whicheach block represents a set of rows from the table. Snowflake’s micro-par-titions allow for efficient scanning of columns, effective finer-grain querypruning of large tables, and prevention of skews. In addition, Snowflakestores the metadata about all the rows stored in micro-partitions. Amongother things, this metadata includes the range of values and the numberof distinct values for each column.

Snowflake is well designed for bulk inserts and updates. As previouslystated, all data in Snowflake is stored in tables and is transparently parti-tioned as data is inserted or loaded. Data Manipulation Language (DML)operation changes intended for these micro-partition blocks cannot resultin updates within each micro-partition file. Instead, DML operations willadd a new partition block and/or remove an existing partition block.These micro-partition additions and deletions are tracked in the Snowflake metadata.

Now let’s turn to an example. The customer and transaction tables fromthe previous section will be used to demonstrate how micro-partitionswork. In our example, we’ll assume the micro-partitions each hold sixrecords. Note that this is an arbitrary number I selected to make our ex-ample easy to demonstrate. In practice, Snowflake automatically deter-mines the number of records per micro-partition.

If three new records are added the following day, they will not be addedto the second micro-partition; instead, a third micro-partition will be cre-ated

Now let’s take a look at the customer table and how the initial data loadwould be stored. You’ll notice that while we have eight transactions, wehave seven customers, so seven customer records will be loaded. Theseven customers will be stored in two micro-partitions

For our customer table, we’d expect there to be new additions but proba-bly no deletions. We’d also expect there could be updates to the table be-cause we’re storing the customer’s zip code, which would need to be up-dated if the customer moved. What happens to the stored data when there is an update to the record? As depicted in Figure 9-13, Snowflake deletes the existing partition fileand replaces it with a new file when an update or delete operation isperformed.

Indeed, micro-partitions are inherently small in size and are stored ascolumnarized sets of rows. Storing the data in this way, where columnsare stored independently within the micro-partitions, allows for efficientcolumn scanning and filtering. This is especially beneficial for very largetables that could have many millions of micro-partitions. Using micro-partitions allows for granular pruning on large tables. Effective and effi-cient pruning is important because scanning entire tables is not optimalfor most queries.

Snowflake’sunique micro-partitions and the use of metadata about therows in each micro-partition, including the range and distinct values foreach column, lays the foundation for optimized queries and highly effi-cient query processing. Additionally, the data in Snowflake tables isstored in its natural ingestion order, which works well for data such astransaction data, which is rarely updated or deleted and is often queriedby date. For other situations, natural ingestion may not be the optimalphysical order, especially if there are frequent updates to the table’s data.For those use cases, we can use clustering.

Snowflake Data Clustering Explained

When data is loaded into Snowflake, Snowflake creates micro-partitions automatically based on the order in which the data is loaded.This works well, for the most part, as Snowflake generally produces well-clustered data in tables using the micro-partitioning approach. However,sometimes Snowflake tables are not optimally clustered, especially ifDML actions occur frequently on very large tables containing multipleterabytes of data. When this happens, we should consider defining ourown clustering key(s).

A clustering key is a subset of columns in a table or expressions on a ta-ble. The purpose of creating a clustering key is to co-locate data in thesame micro-partitions in the table.

Clustering keys are not meant to be created for all tables. Before deciding whetherto cluster a table, it is recommended that you test a representative set of querieson the table to establish a query performance baseline. The query performance ofthe table, along with the size of the table, should determine whether to define aclustering key. In general, it may be time to create a clustering key when queriesrun slower than expected and/or have noticeably degraded over time. A largeclustering depth is another indication that the table needs to be clustered, but theclustering depth is not a precise measure. The best indicator of a well-clustered ta-ble is its query performance.

Snowflake can determine how well clustered a table is by consideringhow many partitions overlap. Micro-partitions can overlap with one an-other in a partition; the number of overlaps is known as the clusteringwidth. Micro-partitions can overlap at a specific point in the partition; thenumber of overlaps at that specific point determines the clustering depth.

In the first example, we have six partitions in the micro-partition, all ofwhich are overlapping on the one specific value and with one another (as shown in Figure 9-14). This is an example of a least well-clustered micro-partitioned column.

The next example is of a perfectly clustered micro-partition. One partitionoverlaps the specific value, and no partitions overlap one another (as shown in Figure 9-15).

In the third example, three partitions overlap the specific value and fourpartitions overlap one another (as shown in Figure 9-16).

The smaller the depth, the better the table is clustered. A table with no micro-par-titions has a clustering depth of zero, which indicates that the table has not yetbeen populated with data.

Changing the specific value for which we are querying could change the clustering depth, where as the clustering width would not change (as shown in Figure 9-17).

Figure 9-17. The depth of a micro-partitioned column changes when a specific value has changed

If we were to look at all three micro-partitions, assuming they were theonly three micro-partitions in the table, we’d be able to see the full rangeof values within the table (as shown in Figure 9-18).

We can see from the examples that the width of the table gives us therange of values. If the range of table values spanned 40 values, for exam-ple, we could get the average clustering depth by calculating the averageof all 40 depths.

On tables within Snowflake, you are able to see the clustering depth,among other details, by using the SYSTEM$CLUSTERING_INFORMATION function. If a table has been clustered, you don’t need to specify the column. Ifa table has not been clustered, you’ll need to include the column(s) inyour statement. Here is an example of a column whose details you can view. Navigate back to the worksheet and execute the following statements:

USE ROLE ACCOUNTADMIN;

USE DATABASE SNOWFLAKE_SAMPLE_DATA;

USE SCHEMA TPCH_SF100;

SELECT SYSTEM$CLUSTERING_INFORMATION ( 'CUSTOMER', '(C_NATIONKEY )' );

From our examples in this section, we see that loading data into a Snowflake table creates micro-partitions based on the order in which therecords are loaded. When we use SQL to query the data, the WHERE clauseis used to prune the search space of the partitions that need to bescanned. If the way the data is currently loaded into a table is not effi-cient for searching, we can create a clustering key that allows us to re-order the records so that the data is co-located with the same micro-parti-tion. Doing so often results in significant performance improvements, es-pecially if the queries make use of the clustering key as a filter in the WHERE clause.

Choosing a Clustering Key

When deciding whether or not to cluster a table, it is important to re-member that the main goal in clustering a table is to help the query opti-mizer achieve better pruning efficiency. As such, it is recommended that,by default, clustering keys be defined on tables larger than 1 TB.

A clustering key can include one or more columns or expressions from atable. When deciding which column(s) to use for the clustering key, thedata characteristics of the table should be evaluated first. It’s important to remember that the degree to which pruning efficiency can be achieveddepends, in part, on the particular queries in users’ workloads.

Table data characteristics and workload considerations

When deciding on a clustering key, a good rule of thumb is to select a keythat has enough distinct values for effective pruning, yet not so many val-ues that Snowflake is unable to effectively group rows in the same micro-partitions. The number of distinct values in a table is its cardinality. A table’s cardinality is part of the formula used to calculate selectivity; themeasure of how much variety there is in the values of the table column.Selectivity is a value between 0 and 1. A value closer to 0 indicates less va-riety in a column’s values. A value of 1 indicates that every value in thetable column is unique.

We’ll use a Snowflake sample data table to demonstrate cardinality andselectivity. Let’s first run a few queries to get the distinct count of NATIONKEY values as well as the total count of records:

USE ROLE ACCOUNTADMIN;
USE DATABASE SNOWFLAKE_SAMPLE_DATA;
USE SCHEMA TPCH_SF100;
SELECT COUNT ( DISTINCT C_NATIONKEY ) FROM CUSTOMER;
SELECT COUNT ( C_NATIONKEY ) FROM CUSTOMER ;

The results indicate that there are 25 distinct values for theC_NATIONKEY column in the CUSTOMER table, which has 15 millionrecords. The cardinality is 25. To calculate the selectivity, we can use thefollowing statement, which gives us a value of 0.000002; a very low selec-tivity value:

USE ROLE ACCOUNTADMIN;
USE DATABASE SNOWFLAKE_SAMPLE_DATA;
USE SCHEMA TPCH_SF100;
SELECT COUNT ( DISTINCT C_NATIONKEY ) / Count ( C_NATIONKEY ) FROM CUSTOMER ;

If we consider that this column could be a good candidate for a clusteringkey, we’d want to take a closer look at the data distribution before deciding:

SELECT C_NATIONKEY, count (*) FROM CUSTOMER group by C_NATIONKEY ;

+-------------+-----------+
| C_NATIONKEY | COUNT (*) |
|-------------+-----------|
|           5 |      2619 |
|           1 |      2638 |
|          10 |      2658 |
|          23 |      2654 |
|          19 |      2702 |
|          17 |      2625 |
|          21 |      2653 |
|          11 |      2625 |
|          20 |      2532 |
|          24 |      2603 |
|           0 |      2619 |
+-------------+-----------+

The results of the query, indicate that the records are pretty evenly distributed between the NATIONKEY.

For comparison, let’s look at the selectivity value of the C_NAME columnin the customer table:

USE ROLE ACCOUNTADMIN;
USE DATABASE SNOWFLAKE_SAMPLE_DATA;
USE SCHEMA TPCH_SF100;
SELECT COUNT ( DISTINCT C_NAME ) / Count ( C_NAME ) FROM CUSTOMER ;
+----------------------------------------------+
| COUNT ( DISTINCT C_NAME ) / COUNT ( C_NAME ) |
|----------------------------------------------|
|                                     1.000000 |
+----------------------------------------------+

The result is exactly 1, which means that every one of the 15 millionrecords has a unique name.

A column with very high cardinality, such as one containing customername or nanosecond timestamp values, has such a high selectivity valuethat it would probably yield poor pruning results. At the other extreme, afield with very low cardinality, such as gender or a binary column with Yes/No values, would likely not be a good candidate to use as a clustering key.

A column with very high cardinality can be used as a clustering key more effec-tively if it is defined as an expression on the column rather than on the column di-rectly. In this way, an expression can preserve the original ordering of the col-umn. An example would be using a timestamp column as a clustering key by cast-ing the values to dates instead of timestamps. If the column name wereTIMESTAMP_NTZ, the statement would be similar to the following:

CREATE OR REPLACE TABLE < table name > 
   CLUSTER BY (to_date(TIMESTAMP_NTZ));

The effect would be to reduce the number of distinct values to the number of daysinstead of the number of timestamps. This would likely yield better pruning results.

A clustering key can be defined at the time of table creation by adding thecommand CLUSTER BY ( column name(s) ) to the CREATE statement

ALTER TABLE <table name> CLUSTER BY (column name(s)) ;

A clustering key can be added or changed after table creation and can bedropped at any time.

When creating the clustering key, it is recommended that you choose no morethan three or four columns or expressions. While the number of columns is an important consideration, it is most important to focus on selecting the right columns or expressions for the clustering key and putting them in the best order.

If more than one column or expression is used to cluster a table, it is rec-ommended that the columns most actively used in selective filters be prioritized. For tables in which date-based queries are used, it makes senseto choose the date column for the cluster key.

If you select a string data type column in which the leading characters are thesame, you can use the substr expression to truncate the common leading characters.

It could be a good idea to include columns that are frequently used in join predicates, if there is room for additional cluster keys after considering other, higher-priority columns. The balance between query workload and DML workload should be carefully considered when choosing the columns for a clustering key.

That said, as a general rule, Snowflake recommends ordering selected col-umns from lowest to highest cardinality assuming evenly distributeddata. Doing so will allow columns with fewer distinct values to come ear-lier in the clustering key order. Of course, there could be exceptions tothis general rule, so don’t let it override what would otherwise be a better selection order.

Reclustering

Over time, data in a table might become less clustered as DML operationsare performed on the table. To maintain optimal performance, it might benecessary to recluster a table periodically. No manual intervention ormaintenance is needed for reclustering; Snowflake automatically reclus-ters tables as needed.

Reclustering consumes credits and also results in storage costs. The amount of data and the size of the table determine the number of credits that will be consumed to perform the recluster. The additional storage costs are the result of the deleted data from reclustering that remains available for the Time Travel retention period and the subsequent failsafe period.

Clustering is just one tool we can use to improve query performance. Another option is to create materialized views.

Performance Benefits of Materialized Views

Materialized viewscontain an up-to-date copy of a subset of data in a table. As such, materi-alized views can be used to improve query performance. It’s important to remember that similar to other Snowflake securable ob-jects, materialized views are owned by a role. When materialized viewsare created, however, they do not automatically inherit the privileges of the base table.

Altering the base table does not change a materialized view; instead, a new materialized view will need to be created. More specifically, adding a new column to the base table will not cause a new column to be added to the materialized view. If changes are made to an existing column in the base table, such as changing the label or dropping the column, all materialized views on that base table will be suspended.

1) Snowflake materialized views can improve the performance ofqueries that use the same subquery results repeatedly.
2) Querying materialized views is typically slower than using cached re-sults but faster than querying tables.
3) Materialized views are especially useful when a query is meant for external table data, in which data sets are stored in files in an external stage. The reason is because query performance is generally faster when querying internal tables as compared to querying external tables.

A materialized view supports clustering. Importantly, you can cluster amaterialized view on different columns than the columns used to clusterthe base table upon which the materialized view is defined.

As a general rule of thumb, a materialized view is recommended over a regularview when the query results are used often and the results change infrequently, especially if the query consumes a lot of resources.

The performance benefits of creating materialized views need to beweighed against cost considerations. Unlike regular views that do notcache data, materialized views do use storage. Thus, there are storagefees associated with materialized views. Materialized views also consumecredits for maintenance. 

While materialized views have many benefits, there are a few constraints. A materialized view doesn’t support DML operations, can querybut one table alone, and cannot query another view or a user-defined function. Additionally, a materialized view does not support joins and cannot include any of the following clauses: HAVING, ORDER BY, LIMIT or GROUP BY.

Exploring Other Query Optimization Techniques

Snowflake’s search optimization service is another way to optimize queryperformance. There is an important distinction between Snowflake’ssearch optimization service and the other two query optimization tech-niques. Materialized views and clustered tables can speed up both rangesearches and equality searches; the search optimization service can onlyspeed up equality searches.

Equality searches use equality predicates such as <column name> = <constant value> whereas range searches query ranges in time.

In addition to equality searches, the search optimization service supportspredicate searches within lists using the IN conditional expression forthe following data types:

Fixed-point numbers such as INTEGER and NUMERIC
DATE, TIME, and TIMESTAMP
VARCHAR
BINARY

Search Optimization Service

The goal of Snowflake’s search optimization service is to improve the per-formance of selective point lookup queries, which are queries that returnonly one or a small number of distinct rows. In Snowflake, we’ll mostlikely run more aggregation queries than queries that return only a fewrows out of a large volume of rows in a table. Thus, there likely won’t be many use cases in which the search optimization service will be the opti-mal choice. However, for the very specific use cases that could benefitfrom Snowflake’s search optimization service, the query performance im-provement could be significant.

The Snowflake search optimization service is available in Snowflake Enterpriseand higher editions.

To take advantage of the search optimization service, you’ll need to register the table with the service. When search optimization is added to aSnowflake table, a maintenance service runs in the background. We know that the search optimization service is meant for finding one or asmall number of records based on using = in the WHERE clause but, for-tunately, we don’t have to decide when it makes sense to use the service.When a query is executed, the Snowflake optimizer automatically decides when to use the search optimization service.

Before deciding whether to register a table with the service, you may want to consider estimating the cost of adding search optimization to a table. You can do so by using the function SYSTEM$ESTIMATE_SEARCH_OPTIMIZATION_COSTS.

Once you’ve determined that it makes sense to add search optimization toa table, you can use the ALTER TABLE command to do so:

ALTER TABLE [ IF EXISTS ] <table name> ADD SEARCH OPTIMIZATION ;

The search optimization service is a serverless feature that incurs com-pute costs. Because the search optimization service maintains search ac-cess paths, there are also storage costs associated with the service. The costs are directly proportional to the number of tables on which the fea-ture is enabled, the number of distinct values in those tables, and the amount of data that changes in the tables.

Query Optimization Techniques Compared

Table 9-1. Snowflake query optimization techniques compared

In most cases, it would be redundant to use Snowflake’s search optimization ser-vice on a clustered table unless the query is on columns other than the primarycluster key.

Chapter 10. Configuring and Managing Secure Data Sharing

However, there are impedi-ments to effective and efficient data sharing when using traditional datasharing options. For one thing, traditional data sharing options, such asFTP transfers, API calls, sending and receiving CSV files, and ETL toolsand processes, often require building complex infrastructure. Similarly, traditional data sharing approaches, which involve transferringdata copies and reconstructing and storing duplicate data, are expensive.There is often no single source of truth with traditional data sharing op-tions, and fewer actionable insights result from having delayed access tostale and sometimes incomplete data. Overcoming many of these sharingchallenges, however, can be accomplished by using Snowflake’s SecureData Sharing technology.

Secure Data Sharing makes it possible for data to be accessible via a liveconnection so that updated data is automatically available to data con-sumers in real time. The business logic, along with the data, can also beshared. In addition to being secure, efficient, and a great way to monetizedata, Snowflake Secure Data Sharing comes with the peace of mind ofknowing you can revoke access at any time.

Snowflake Architecture Data Sharing Support

When you separate compute from storage and you share data access con-trol, as Snowflake does, you can have multiple virtual warehouses work-ing on the same data at the same time. As such, Snowflake’s architecturesupports the ability to have one single source of truth.

If you share data within the same cloud provider and region, there is noneed to copy the data to share it. If you share data to a different region ordifferent cloud provider, Snowflake replicates the data. However, it doesso via auto-fulfillment, so you don’t need to do any manual work. Snowgrid is the term to describe data replication for the purpose of facili-tating data sharing.

The Power of Snowgrid

Snowgrid is global, seamlessly connecting Snowflake users who may beseparated by region or cloud. Snowgrid achieves this through replicationvia auto-fulfillment. Even when sharing data across clouds and regions,the shares are transactionally  consistent, meaning the source of truth isstill maintained. Within Snowgrid are all the native cross-cloud gover-nance controls that serve as the foundational building blocks for enablingfederated governance.

With the power of Secure Data Sharing and Snowgrid, Snowflakeproviders can create and share any number of data shares with data consumers worldwide. Each share encapsulates the necessary access control,so it is secure. Once a provider’s outbound share appears as an inboundshare in the data consumer’s account, the consumer is then able to createa database from that share. The metadata points back to the original datasource, so the data continues to exist only in the provider account.

Data Sharing Use Cases

In fact, payersstoring petabytes of protected health information (PHI) and personallyidentifiable information (PII) that needs to be shared with insurance bro-kers, providers, and other vendors are turning to Snowflake as a solutionbecause data sharing in the healthcare industry is mandated and regulated. Healthcare providers that own massive amounts of data often col-laborate with healthcare consortiums or are affiliated with universities;thus, sharing data in real time, rather than in batches, is absolutelyessential.

Trucking companies that can optimize fleet routing by using IoT data andretailers that are able to improve the supply chain by sharing data withlogistics partners are two more examples of industries that incorporateSnowflake’s live data sharing capabilities. Also, some enterprise companies use different Snowflake accounts for development, testing, and pro-duction purposes. Data sharing provides the ability to incorporate strongaudit controls and move data in a controlled manner, rather than having to prepare flat files and reload them into a different environment.

Publishers, advertising agencies, and brands need to develop a new data sharing paradigm quickly as we are fast approaching a world without theexistence of third-party cookies. As third-party cookies are being depre-cated, marketers will need to find new ways to identify people online sothat they can optimize marketing campaigns and continue to personalizemessages.

Snowflake Support for Unified ID 2.0

Cookies, developed in 1994 as a tiny piece of software code installed onyour web browser when you visit a website to facilitate revisiting andtracking, are an outdated technology. Today, internet usage occurs morefrequently in mobile apps and on connected TV devices where cookiesare mostly irrelevant. Cookies often contain a wealth of personal datathat could potentially identify you without your consent; thus, they aresubject to the European Union’s General Data Protection Regulation(GDPR) and the ePrivacy Directive (EPD). The EPD will eventually be re-placed by the ePrivacy Regulation (EPR). Major platforms such as Apple,Firefox, and Google have begun limiting the use of third-party cookies. Inthe near future, changes to internet platforms’ privacy policies couldeven render cookies, the internet’s first universal identifier, obsolete.
 
Concerns over user privacy and more regulation have necessitated theconsideration of new approaches. Unified ID 2.0, developed by The TradeDesk, is an open source, industry-governed identity solution to provideanonymization, greater transparency, and better control for users. Aperson’s UID 2.0 is a random string of numbers and letters generated from an email address that cannot be reverse engineered to an email ad-dress or any other form of identification. Currently, Unified 2.0 is ac-cepted by The Washington Post, Oracle, Nielsen, and Tubi TV, amongothers.

Snowflake supports Unified ID 2.0 for use with Secure Data Sharing, mostnotably with data clean rooms, As such, Snowflake customers can join first-party andthird-party data directly in the Snowflake platform in a more privacy-conscious way. The power of the Snowflake Data Cloud and Unified ID 2.0provides for a single view of the customer.

Snowflake Secure Data Sharing Approaches

There are four different ways to approach Snowflake’s Secure DataSharing. 

1) Direct Share : The simplest way is to engage in  account-to-account data shar-ing, whereby you can share data directly with another account and haveyour data appear in their Snowflake account without having to move or copy it. The other account could belong to a different internal businessunit, for example, or to a different organization altogether.

2) Snowflake’s Marketplace : a public Secure Data Sharing ap-proach that connects global providers of data with consumers around the world.

3) Data Exchange : You can even create your own Snowflake Private Data Exchange to collaborate with others; you’ll get to control who can join the data exchange and which members can provide data, consume data, or both.

4) Data Clean Room : Alternatively, you can use a data clean room, a framework for sharing data between two or more parties; the data is brought together underspecific guidelines so that PII is anonymized and processed, and can bestored in a way that allows compliance with privacy regulations.

USE ROLE ACCOUNTADMIN;
USE WAREHOUSE COMPUTE_WH;
CREATE OR REPLACE DATABASE DEMO10_DB;
USE SCHEMA DEMO10_DB.PUBLIC;
CREATE OR REPLACE TABLE SHARINGDATA ( i integer );

Data products -> Private Sharing Menu

Snowflake’s Direct Secure Data Sharing Approach

Direct sharing, Snowflake’s simplest form of data sharing, results when aSnowflake provider creates an outbound data share which then becomesan inbound share in a consumer’s Snowflake account. A provider canshare a Snowflake database table, a secure view, or a secure user-definedfunction (UDF) via a share object. It is important to realize that the shareis not created from objects, but rather is granted privileges to the data-base, schema, table, view, or UDF.

Creating Outbound Shares

When you are creating outbound shares your account is the Snowflake provider account, whereas the account to which you are sharing yourdata is the Snowflake consumer account. It’s important to know that ashare can contain only a single database.

To include multiple databases in a share, you can create a secure view which canbe shared. This is possible provided the objects referenced in the share reside inthe same Snowflake account.

It’s important to note that the secure share identifier must be unique for the account in which theshare is created, and that the identifier is case sensitive. Also notice thatyou could share the object with a specific consumer by entering the accounts in your region by name.

Another way to create an outbound share is to use SQL commands withina worksheet.

USE ROLE ACCOUNTADMIN;
CREATE OR REPLACE SHARE DEMO10_SHARE2;
GRANT USAGE ON DATABASE DEMO10_DB TO SHARE DEMO10_SHARE2;
GRANT USAGE ON SCHEMA DEMO10_DB.PUBLIC TO SHARE DEMO10_SHARE2;
GRANT SELECT ON TABLE DEMO10_DB.PUBLIC.SHARINGDATA TO SHARE DEMO10_SHARE2

ALTER SHARE <name_of_share> ADD ACCOUNTS = <name_of_consumer_account>;

You can add multiple consumer accounts by separating names with a comma.

One of the things you’ll notice is that you are assigning access privileges to the shares that will be used by the data consumers.

Data providers’ role in creating and managing shares

Snowflake data providers create an outbound share object and assignprivileges on one or more database tables, secure views, or secure UDFsusing data stored in their account. As such, the data provider is responsi-ble for bearing the cost of data storage. The consumer is not billed for anydata storage costs because the data share, and objects accessed via theshare, are not copied or moved to the consumer’s account.

A Snowflake provider can share an outbound share with a virtually unlimitednumber of accounts, and there is never a cost for the act of sharing data across ac-counts within the same cloud provider region. The Snowflake data provider canalso set terms, through a contract, to limit the resharing of data.

Remember, of course, that you can restrict certain data from being acces-sible either by excluding the data altogether or by masking the data thatcan be seen. Some examples are dynamic data masking and row-level se-curity,

What if you want to share specific records within a data share with dif-ferent consumers? One way to achieve that goal is to separate the storageof those records and then create many different data shares, but that istime-consuming and prone to errors. Fortunately, there is an easier andbetter way. A provider can maintain one data share and, usingSnowflake’s Current Account capability, provide access to custom slices ofthe data to the current account. It’s also possible to provide specific slicesof the data to each consumer account external to the current account.

Let’s see this in action. For this example, we’ll have different countrieswith data, and we’ll associate the countries to a specific region. We wantthis regional data to be accessible to different accounts. We need to createthe table and insert the values for that base table in our Snowflake in-stance. We’ll designate the ID as the region.

USE ROLE ACCOUNTADMIN;
USE DATABASE DEMO10_DB;
CREATE OR REPLACE SCHEMA PRIVATE;
CREATE OR REPLACE TABLE DEMO10_DB.PRIVATE.SENSITIVE_DATA ( nation string, price float, size int, id string ); 

INSERT INTO DEMO10_DB.PRIVATE.SENSITIVE_DATA 
values ( 'USA', 123.5, 10, 'REGION1' ), 
( 'USA', 89.2, 14, 'REGION1' ), 
( 'CAN', 99.0, 35, 'REGION2' ), 
( 'CAN', 58.0, 22, 'REGION2' ), 
( 'MEX', 112.6, 18, 'REGION2' ), 
( 'MEX', 144.2, 15, 'REGION2' ), 
( 'IRE', 96.8, 22, 'REGION3' ), 
( 'IRE', 107.4, 19, 'REGION3' );

Next, we’ll create a table that will hold the mapping to individual accounts:

CREATE OR REPLACE TABLE DEMO10_DB.PRIVATE.SHARING_ACCESS ( id string, snowflake_account string );

Let’s give our current account access to the data in REGION1, which in-cludes the USA data, by inserting those details into the SHARING_ACCESSmapping table

INSERT INTO SHARING_ACCESS values ('REGION1', current_account());

Now we’ll assign REGION2 and REGION3 the values associated with theirrespective accounts

INSERT INTO SHARING_ACCESS values ( 'REGION2', 'ACCT2' ); 
INSERT INTO SHARING_ACCESS values ( 'REGION3', 'ACCT3' ); 
SELECT * FROM SHARING_ACCESS ;

Our next action is to create a secure view where we’ll join all the data inthe SENSITIVE_DATA base table with the SHARING_ACCESS mappingtable

CREATE OR REPLACE SECURE VIEW DEMO10_DB.PUBLIC.PAID_SENSITIVE_DATA 
as 
SELECT nation, price, size FROM DEMO10_DB.PRIVATE.SENSITIVE_DATA sd 
JOIN DEMO10_DB.PRIVATE.SHARING_ACCESS sa 
on sd.id = sa.id AND sa.snowflake_account = current_account ();

We’ll want to grant the SELECT privilege to the secure view for all roles,which we can do by granting this privilege to the PUBLIC role:

GRANT SELECT ON DEMO10_DB.PUBLIC.PAID_SENSITIVE_DATA to PUBLIC ;

SELECT * FROM DEMO10_DB.PRIVATE.SENSITIVE_DATA;
 -- List all 8 records

SELECT * FROM DEMO10_DB.PUBLIC.PAID_SENSITIVE_DATA;
-- List only two records as current_user has Region1 access only

Let’s use a session variable to simulate our Snowflake account being ACCT2, and take a look at what we’ll be able to see in the secure view. if this session parameter simulated_data_sharing_consumer is set to user name, Data sharing views will return rows as if executed in the specified consumer account.

ALTER SESSION SET simulated_data_sharing_consumer = 'ACCT2';

SELECT * FROM DEMO10_DB.PUBLIC.PAID_SENSITIVE_DATA;
 -- List only 4 records as ACCT2 has Region2 access only

ALTER SESSION SET simulated_data_sharing_consumer = 'ACCT3';

SELECT * FROM DEMO10_DB.PUBLIC.PAID_SENSITIVE_DATA;
 -- List only 2 records as ACCT3 has Region3 access only

ALTER SESSION UNSET simulated_data_sharing_consumer ;

we are ready to create a new share:

USE ROLE ACCOUNTADMIN;
USE DATABASE DEMO10_DB;
USE SCHEMA DEMO10_DB.PUBLIC;
CREATE OR REPLACE SHARE NATIONS_SHARED;
SHOW SHARES ;

But we are not done yet. We need to grant privileges to the newshare:

GRANT USAGE ON DATABASE DEMO10_DB TO SHARE NATIONS_SHARED;
GRANT USAGE ON SCHEMA DEMO10_DB.PUBLIC TO SHARE NATIONS_SHARED;
GRANT SELECT ON DEMO10_DB.PUBLIC.PAID_SENSITIVE_DATA TO SHARE NATIONS_SHARED;

SHOW GRANTS TO SHARE NATIONS_SHARED ;

Now all that is left to do from the provider perspective is to add accounts to the share:

// This is pseudocode ; you need to replace with actual Snowflake accounts 
ALTER SHARE NATIONS_SHARED ADD ACCOUNTS = ACCT2, ACCT3;

If you’d like to retrieve a list of all the Snowflake consumer accounts thathave a database created from a share, you can use the following

SHOW GRANTS OF SHARE NATIONS_SHARED ;

It is important to remember that provider accounts pay for the data stor-age costs of the shared data and consumer accounts pay for the virtualwarehouse cost of compute for querying the data. This assumes that boththe data provider and data consumer each has their own Snowflakeaccount.

A situation could arise in which a data provider would like to share datawith a consumer who does not currently have a Snowflake account. In that case, the provider can establish and manage a Snowflake reader account. When a provider establishes a reader account for the consumer,the reader account will offer read-only capability for the consumer.

Setting up a reader account

Data -> Private sharing -> Reader accounts 

The dialog page asks you to provide the reader account name, and theuser name and password for the person who will be given admin privileges. The reader account created will be the same edition, region, and cloud as yourown Snowflake account, and the admin login information is what you will use tomanage the reader account.

Once you are logged in to the reader account as the data provider admin,you’ll want to create a database from an inbound share and set up a dataconsumer user account to assign to the person you want to have access tothe Snowflake reader account.

As the data provider, you are responsible for the compute charges incurred when the data is queried, so you may want to consider setting up a resource monitor tolimit the total costs or to be notified when a specified number of credits have been consumed.

How Inbound Shares Are Used by Snowflake Data Consumers

Using the ACCOUNTADMIN role, a data consumer who is the owner of aSnowflake account can create a database from the provider’s outboundshare, which is now the consumer’s inbound share.

CREATE DATABASE <name_of_new_database> FROM SHARE <name_of_inbound_share>;

As previously discussed, it is not possible to edit shared data. Shared data-bases are read-only; thus, the data cannot be updated and no new objectswithin the database can be created by the data consumer. There are someadditional limitations for this newly created database. One unique prop-erty of the shared database is that the comments cannot be edited.

The data consumer is unable to clone a shared database or the objectswithin it. However, it is possible to copy the shared data into a new table.While it is technically possible to make a copy of the shared database, itmight be a violation of the terms of the contract if both parties enteredinto such an agreement. For data consumers who are the owners of aSnowflake account where the database resides, there is no limit on thenumber of times the data can be queried. It is important to keep in mind,though, that data consumers bear the compute cost of querying the data.

Understanding consumer accounts: reader accounts versusfull accounts

Both reader accounts and full accounts are types of consumer accountsused in Snowflake data sharing. Reader accounts pay none of the com-pute costs for querying the data. Those costs are paid by the provider ac-count that created the reader account. As such, providers may choose tolimit the queries of the reader accounts by using resource monitors.Reader accounts can be useful as an intermediate step for partners whoare not yet ready to transition from their current data processes. Settingup and maintaining reader accounts for their partners’ use does requireadditional effort by providers.

Full consumer accounts also have the ability to join their data with the shareddata directly in Snowflake, while a reader account can only view the data sharedwith them in Snowflake.

How the ACCOUNT_USAGE share is different from all otherinbound shares

when we compared the ACCOUNT_USAGE share from theSNOWFLAKE database to the INFORMATION_SCHEMA, a schema that isprovided for each database. The SNOWFLAKE database, viewable only bythe ACCOUNTADMIN by default, is similar to the INFORMATION_SCHEMAin that they both provide information about an account’s object metadataand usage metrics. However, there are three major differences betweenthe two:

The ACCOUNT_USAGE share, with multiple views, includes records fordropped objects.
The ACCOUNT_USAGE share also a longer retention time than theINFORMATION_SCHEMA.
The ACCOUNT_USAGE share has an average latency of about twohours, whereas there is no latency when querying the INFORMATION_SCHEMA.

Share privileges are imported, and most inbound shares allow you to cre-ate and rename a database as well as drop the database from which youcreated the inbound share. However, the ACCOUNT_USAGE share is dif-ferent. You cannot create, rename, or drop the database associated withthe ACCOUNT_USAGE share. You also cannot add a comment. TheACCOUNT_USAGE share is managed differently because it is a way inwhich Snowflake communicates with you about your account.

Unlike the ACCOUNT_USAGE share, inbound shares can be re-moved, or a new database can be created with a different name than the inbound share name.

Comparison between databases on inbound shares and regular databases

Table 10-1. Differences between an inbound share and a regular database

One important note is that, unlike most traditional databases, Snowflakesupports cross-database joins, including joins with databases built from inbound shares. Consumers, other than those withreader accounts, can also create views in their own database which cancombine their own data with data from the shared database. This ishighly beneficial, because the consumer can enrich their own data tomake it even more valuable.

When new records are created within the provider’s Snowflake accountas part of the shared database, the records are almost instantly availablein a data consumer’s inbound shared database. However, when theprovider creates new objects within the shared database, those objectsare not automatically shared with the data consumer. The provider has togrant authorization to the data share before the records in those objectsare viewable by the consumer.

How to List and Shop on the Public Snowflake Marketplace

Whether the access to the datais free or the provider charges you a fee, that arrangement between you
and the data provider is separate from your arrangement withSnowflake. You’ll be billed by Snowflake only for the compute you use toquery the shared data. The data storage charges are the responsibility ofthe data provider.

Snowflake Marketplace for Providers

For data providers, the Snowflake Marketplace provides an opportunityto monetize data in ways never before possible. You can provide secure,personalized data views using your own customized business model.Snowflake isn’t a party to the transaction between a data provider anddata consumer, and there is no cost for the act of sharing data on the DataCloud. Instead, Snowflake enables data sharing on its platform and earnsrevenue when data consumers use virtual warehouses to query the data.

To become an approved Snowflake data provider, you’ll need to submit arequest, sign the Snowflake provider agreement, and agree to operatewithin the Snowflake provider policies. Importantly, your data will haveto meet certain requirements.

One requirement is that your data must be fresh and nonstatic, meaningthat the historical data you’ll provide must be relatively recent and thatyou have a plan to continue updating the data going forward. Another requirement is that the data cannot be sample, mocked-up data. Finally, you must possess the right to make the data available and ensurethat the data doesn’t include sensitive private information.

Provider Studio

Provider Studio offers the ability to perform required actions to get list-ings approved and to review data requests from consumers. Additionally,Provider Studio makes it possible for Snowflake providers to view key an-alytics relating to Marketplace listings by selecting the Analytics tab.Access to Provider Studio is available only to Snowflake data providers.

































Snowpark is the set of libraries and code execution environments that run Python and other programming languages next to your data in Snowflake. Snowpark can be used to build data pipelines, ML models, apps, and other data processing tasks.


GRANT ROLE ACCOUNTADMIN TO USER nesanreader;


ALTER SESSION SET simulated_data_sharing_consumer = 'nesanreader' ;



alter session set autocommit=FALSE;
show parameters like 'simulated_data_sharing_consumer';














