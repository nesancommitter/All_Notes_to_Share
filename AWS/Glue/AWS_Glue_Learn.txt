Glue intro
~~~~~~~~~~~~~~~~

With AWS Glue, you can discover and connect to more than 70 diverse data sources and manage your data in a centralized data catalog. You can visually create, run, and monitor extract, transform, and load (ETL) pipelines to load data into your data lakes. Also, you can immediately search and query cataloged data using Amazon Athena, Amazon EMR, and Amazon Redshift Spectrum.

These include data discovery, modern ETL, cleansing, transforming, and centralized cataloging. It's also serverless, which means there's no infrastructure to manage. With flexible support for all workloads like ETL, ELT, and streaming in one service. Also, AWS Glue makes it easy to integrate data across your architecture. It integrates with AWS analytics services and Amazon S3 data lakes. It scales for any data size, and supports all data types and schema variances. To increase agility and optimize costs, AWS Glue provides built-in high availability and pay-as-you-go billing.

AWS Glue Studio

AWS Glue Studio is a graphical interface that makes it easy to create, run, and monitor data integration jobs in AWS Glue. You can visually compose data transformation workflows and seamlessly run them on the Apache Spark–based serverless ETL engine in AWS Glue. With AWS Glue Studio, you can create and manage jobs that gather, transform, and clean data. You can also use AWS Glue Studio to troubleshoot and edit job scripts.

AWS Glue features fall into three major categories:
● Discover and organize data
  -- Unify and search across multiple data stores (Store, index, catalog your data then you can search and use your data)
  -- Automatically discover data (Use AWS Glue crawlers to automatically infer schema information and integrate it into your AWS Glue Data Catalog)
  -- Manage schemas and permissions (Control access to database and tables)
  -- Connect to a wide variety of data sources (using AWS Glue connections to build your data lake, connect to on permises and any source)
● Transform, prepare, and clean data for analysis
  -- Visually transform data with a job canvas interface (Define your ETL process in the visual job editor)
  -- Build complex ETL pipelines with simple job scheduling (Invoke AWS Glue jobs on a schedule, on demand, or based on an event)
  -- Clean and transform streaming data in transit ()
  -- Deduplicate and cleanse data with built-in machine learning (using the FindMatches feature)
  -- Built-in job notebooks (AWS Glue job notebooks provide serverless notebooks)
  -- Edit, debug, and test ETL code 
  -- Define, detect, and remediate sensitive data (AWS Glue sensitive data detection lets you define, identify, and process sensitive data in your data pipeline and in your data lake)
● Build and monitor data pipelines
  -- Automatically scale based on workload (This assigns workers to jobs only when needed)
  -- Automate jobs with event-based triggers (Start crawlers or AWS Glue jobs with event-based triggers, and design a chain of dependent jobs and crawlers)
  -- Run and monitor jobs (Run AWS Glue jobs with your choice of engine, Spark or Ray. Improve your monitoring of Spark-backed jobs with the Apache Spark UI)
  -- Define workflows for ETL and integration activities 

other related services with Glue

● AWS Lake Formation –- A service that is an authorization layer that provides fine-grained access control to resources in the AWS Glue Data Catalog.
● AWS Glue DataBrew  –- A visual data preparation tool that you can use to clean and normalize data without writing any code.
 
Accessing AWS Glue
● AWS Glue console - create, view, and manage your AWS Glue jobs.
● AWS Glue Studio  - create and edit your AWS Glue jobs visually.
● AWS CLI glue subcommand 
● AWS Glue API     - Can access glue from programming interface

AWS Glue API - Higlevel category of classes
● Security APIs in AWS Glue
● Catalog objects API
  - Catalogs API
  - Database API
  - Table API
  - Partition API
  - Connections API
     -- Connection API
	 -- Connection Types API
	 -- Connection Metadata and Preview API
  - User-defined Function API
  - Importing an Athena catalog to AWS Glue
● Table optimizer API
● Crawlers and classifiers API
  - Classifier API
  - Crawler API
  - Column statistics API
  - Crawler scheduler API
● Autogenerating ETL Scripts API
● Visual job API
● Jobs API
  - Jobs
  - Job runs
  - Triggers
● Integration APIs in AWS Glue
● Interactive sessions API
● Development endpoints API
● Schema registry
● Workflows
● Usage profiles
● Machine learning API
● Data Quality API
● Sensitive data detection API
● Tagging APIs in AWS Glue
● Common data types
● Exceptions

aws Glue CLI Commands
~~~~~~~~~~~~~~~~~~~~~~~
batch-create-partition
batch-delete-connection
batch-delete-partition
batch-delete-table
batch-delete-table-version
batch-get-blueprints
batch-get-crawlers
batch-get-custom-entity-types
batch-get-data-quality-result
batch-get-dev-endpoints
batch-get-jobs
batch-get-partition
batch-get-table-optimizer
batch-get-triggers
batch-get-workflows
batch-put-data-quality-statistic-annotation
batch-stop-job-run
batch-update-partition
cancel-data-quality-rule-recommendation-run
cancel-data-quality-ruleset-evaluation-run
cancel-ml-task-run
cancel-statement
check-schema-version-validity
create-blueprint
create-catalog
create-classifier
create-column-statistics-task-settings
create-connection
create-crawler
create-custom-entity-type
create-data-quality-ruleset
create-database
create-dev-endpoint
create-glue-identity-center-configuration
create-integration
create-integration-resource-property
create-integration-table-properties
create-job
create-ml-transform
create-partition
create-partition-index
create-registry
create-schema
create-script
create-security-configuration
create-session
create-table
create-table-optimizer
create-trigger
create-usage-profile
create-user-defined-function
create-workflow
delete-blueprint
delete-catalog
delete-classifier
delete-column-statistics-for-partition
delete-column-statistics-for-table
delete-column-statistics-task-settings
delete-connection
delete-crawler
delete-custom-entity-type
delete-data-quality-ruleset
delete-database
delete-dev-endpoint
delete-glue-identity-center-configuration
delete-integration
delete-integration-resource-property
delete-integration-table-properties
delete-job
delete-ml-transform
delete-partition
delete-partition-index
delete-registry
delete-resource-policy
delete-schema
delete-schema-versions
delete-security-configuration
delete-session
delete-table
delete-table-optimizer
delete-table-version
delete-trigger
delete-usage-profile
delete-user-defined-function
delete-workflow
describe-connection-type
describe-entity
describe-inbound-integrations
describe-integrations
get-blueprint
get-blueprint-run
get-blueprint-runs
get-catalog
get-catalog-import-status
get-catalogs
get-classifier
get-classifiers
get-column-statistics-for-partition
get-column-statistics-for-table
get-column-statistics-task-run
get-column-statistics-task-runs
get-column-statistics-task-settings
get-connection
get-connections
get-crawler
get-crawler-metrics
get-crawlers
get-custom-entity-type
get-data-catalog-encryption-settings
get-data-quality-model
get-data-quality-model-result
get-data-quality-result
get-data-quality-rule-recommendation-run
get-data-quality-ruleset
get-data-quality-ruleset-evaluation-run
get-database
get-databases
get-dataflow-graph
get-dev-endpoint
get-dev-endpoints
get-entity-records
get-glue-identity-center-configuration
get-integration-resource-property
get-integration-table-properties
get-job
get-job-bookmark
get-job-run
get-job-runs
get-jobs
get-mapping
get-ml-task-run
get-ml-task-runs
get-ml-transform
get-ml-transforms
get-partition
get-partition-indexes
get-partitions
get-plan
get-registry
get-resource-policies
get-resource-policy
get-schema
get-schema-by-definition
get-schema-version
get-schema-versions-diff
get-security-configuration
get-security-configurations
get-session
get-statement
get-table
get-table-optimizer
get-table-version
get-table-versions
get-tables
get-tags
get-trigger
get-triggers
get-unfiltered-partition-metadata
get-unfiltered-partitions-metadata
get-unfiltered-table-metadata
get-usage-profile
get-user-defined-function
get-user-defined-functions
get-workflow
get-workflow-run
get-workflow-run-properties
get-workflow-runs
import-catalog-to-glue
list-blueprints
list-column-statistics-task-runs
list-connection-types
list-crawlers
list-crawls
list-custom-entity-types
list-data-quality-results
list-data-quality-rule-recommendation-runs
list-data-quality-ruleset-evaluation-runs
list-data-quality-rulesets
list-data-quality-statistic-annotations
list-data-quality-statistics
list-dev-endpoints
list-entities
list-integration-resource-properties
list-jobs
list-ml-transforms
list-registries
list-schema-versions
list-schemas
list-sessions
list-statements
list-table-optimizer-runs
list-triggers
list-usage-profiles
list-workflows
modify-integration
put-data-catalog-encryption-settings
put-data-quality-profile-annotation
put-resource-policy
put-schema-version-metadata
put-workflow-run-properties
query-schema-version-metadata
register-schema-version
remove-schema-version-metadata
reset-job-bookmark
resume-workflow-run
run-statement
search-tables
start-blueprint-run
start-column-statistics-task-run
start-column-statistics-task-run-schedule
start-crawler
start-crawler-schedule
start-data-quality-rule-recommendation-run
start-data-quality-ruleset-evaluation-run
start-export-labels-task-run
start-import-labels-task-run
start-job-run
start-ml-evaluation-task-run
start-ml-labeling-set-generation-task-run
start-trigger
start-workflow-run
stop-column-statistics-task-run
stop-column-statistics-task-run-schedule
stop-crawler
stop-crawler-schedule
stop-session
stop-trigger
stop-workflow-run
tag-resource
test-connection
untag-resource
update-blueprint
update-catalog
update-classifier
update-column-statistics-for-partition
update-column-statistics-for-table
update-column-statistics-task-settings
update-connection
update-crawler
update-crawler-schedule
update-data-quality-ruleset
update-database
update-dev-endpoint
update-glue-identity-center-configuration
update-integration-resource-property
update-integration-table-properties
update-job
update-job-from-source-control
update-ml-transform
update-partition
update-registry
update-schema
update-source-control-from-job
update-table
update-table-optimizer
update-trigger
update-usage-profile
update-user-defined-function
update-workflow

AWS Glue uses other AWS services to orchestrate your ETL (extract, transform, and load) jobs to build data warehouses and data lakes and generate output streams. AWS Glue calls API operations to transform your data, create runtime logs, store your job logic, and create notifications to help you monitor your job runs. You supply credentials and other properties to AWS Glue to access your data sources and write to your data targets. 

AWS Glue takes care of provisioning and managing the resources that are required to run your workload. You don't need to create the infrastructure for an ETL tool because AWS Glue does it for you. When resources are required, to reduce startup time, AWS Glue uses an instance from its warm pool of instances to run your workload. you create jobs using table definitions in your Data Catalog. Jobs consist of scripts that contain the instructions that execute the desired data transformation tasks. You use triggers to initiate jobs either on a schedule or as a result of a specified event. You determine where your target data resides and which source data populates your target. 

Serverless ETL jobs run in isolation

AWS Glue runs your ETL jobs in a serverless environment with your choice of engine, Spark or Ray. AWS Glue runs these jobs on virtual resources that it provisions and manages in its own service account. During provisioning of an ETL job, you provide input data sources and output data targets in your virtual private cloud (VPC). In addition, you provide the IAM role, VPC ID, subnet ID, and security group that are needed to access data sources and targets.  For each tuple (customer account ID, IAM role, subnet ID, and security group), AWS Glue creates a new environment that is isolated at the network and management level from all other environments inside your AWS Glue service account.

You create and configure AWS Glue resources, such as Data Catalogs, Jobs, and Crawlers within your AWS account. These resources are then associated with the IAM role and network settings (subnet and security group) you specify during the creation process. AWS Glue creates elastic network interfaces in your subnet using private IP addresses. Jobs use these elastic network interfaces to access your data sources and data targets. Traffic in, out, and within the job run environment is governed by your VPC and networking policies

The key components of Glue are:
● Data Catalog: A metadata store containing table definitions, job definitions, and other control information for your ETL workflows.
● Crawlers: Programs that connect to data sources, infer data schemas, and create metadata table definitions in the Data Catalog.
● ETL Jobs: The business logic to extract data from sources, transform it using Apache Spark scripts, and load it into targets.
● Triggers: Mechanisms to initiate job runs based on schedules or events.

Steps typically done for ETL pipeline in Glue

● In addition to table definitions, the AWS Glue Data Catalog contains other metadata that is required to define ETL jobs. You use this metadata when you define a job to transform your data.For data store sources, you define a crawler to populate your AWS Glue Data Catalog with metadata table definitions. You point your crawler at a data store, and the crawler creates table definitions in the Data Catalog. For streaming sources, you manually define Data Catalog tables and specify data stream properties. In addition to table definitions, the AWS Glue Data Catalog contains other metadata that is required to define ETL jobs. You use this metadata when you define a job to transform your data.
● AWS Glue can generate a script to transform your data. Or, you can provide the script in the AWS Glue console or API.
● You can run your job on demand, or you can set it up to start when a specified trigger occurs. The trigger can be a time-based schedule or an event.

AWS Glue Data Catalog : Each AWS account has one AWS Glue Data Catalog per region. Each Data Catalog is a highly scalable collection of tables organized into databases. 
Classifier            : Determines the schema of your data. AWS Glue provides classifiers for common file types, such as CSV, JSON, AVRO, XML, and others. It also provides classifiers for common relational database management systems using a JDBC connection. 
Connection            : A Data Catalog object that contains the properties that are required to connect to a particular data store.
Crawler               : A program that connects to a data store (source or target), progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog.
Database              : A set of associated Data Catalog table definitions organized into a logical group.
Data store, data source, data target : A data store is a repository for persistently storing your data. Examples include Amazon S3 buckets and relational databases. Any data store can be source or target based on ETL job
Development endpoint  : An environment that you can use to develop and test your AWS Glue ETL scripts.
Dynamic Frame         : A distributed table that supports nested data such as structures and arrays. Each record is self-describing, designed for schema flexibility with semi-structured data. Each record contains both data and the schema that describes that data. You can use both dynamic frames and Apache Spark DataFrames in your ETL scripts, and convert between them. Dynamic frames provide a set of advanced transformations for data cleaning and ETL.
Job                   : The business logic that is required to perform ETL work. It is composed of a transformation script.
Job performance dashboard : The dashboard displays information about job runs from a specific time frame.
Script                : Code that extracts data from sources, transforms it, and loads it into targets. AWS Glue generates PySpark or Scala scripts.
Notebook interface    : You can use the notebook interface based on Jupyter Notebook to interactively develop, debug, and deploy scripts and workflows using AWS Glue serverless Apache Spark ETL infrastructure. 
Table                 : The metadata definition that represents your data. A table defines the schema of your data in S3 or any other source. A table in the AWS Glue Data Catalog consists of the names of columns, data type definitions, partition information, and other metadata about a base dataset. 
Visual job editor     : The visual job editor is a graphical interface that makes it easy to create, run, and monitor extract, transform, and load (ETL) jobs in AWS Glue.
Worker                : With AWS Glue, you only pay for the time your ETL job takes to run. There are no resources to manage, no upfront costs, and you are not charged for startup or shutdown time. You are charged an hourly rate based on the number of Data Processing Units (or DPUs) used to run your ETL job. A single Data Processing Unit (DPU) is also referred to as a worker. AWS Glue comes with multiple worker types to help you select the configuration that meets your job latency and cost requirements. Workers come in Standard, G.1X, G.2X, G.4X, G.8X, G.12X, G.16X, G.025X, and memory-optimized R.1X, R.2X, R.4X, R.8X configurations.

The Data Catalog, along with CloudTrail and Lake Formation, also provides you with comprehensive audit and governance capabilities, with schema change tracking and data access controls. This helps ensure that data is not inappropriately modified or inadvertently shared.

AWS Glue also lets you set up crawlers that can scan data in all kinds of repositories, classify it, extract schema information from it, and store the metadata automatically in the AWS Glue Data Catalog. The AWS Glue Data Catalog can then be used to guide ETL operations.

AWS Glue enables you to perform ETL operations on streaming data using continuously-running jobs. AWS Glue streaming ETL is built on the Apache Spark Structured Streaming engine, and can ingest streams from Amazon Kinesis Data Streams, Apache Kafka, and Amazon Managed Streaming for Apache Kafka (Amazon MSK). Streaming ETL can clean and transform streaming data and load it into Amazon S3 or JDBC data stores. If you know the schema of the streaming data source, you can specify it in a Data Catalog table. If not, you can enable schema detection in the streaming ETL job. The job then automatically determines the schema from the incoming data. The streaming ETL job can use both AWS Glue built-in transforms and transforms that are native to Apache Spark Structured Streaming. 

The AWS Glue jobs system : The AWS Glue Jobs system provides managed infrastructure to orchestrate your ETL workflow.   

Visual ETL components    : AWS Glue allows you to create ETL jobs through a visual canvas that you can manipulate.
Visual ETL panels
When you work in the canvas, several panels are available to help you configure your nodes, or help you to preview your data and view the output schema.
Properties     – The Properties panel appears when you choose a node on your canvas.
Data preview   – The Data preview panel provides a preview of the data output so that you can make decisions before you run your job and examine your output.
Output schema  – The Output schema tab allows you to view and edit the schema of your transform nodes.

Resource panel 

The resource panel contains all of the data sources, transform actions, and connections available to you. Open the resource panel on the canvas by clicking the "+" icon. This will open the resource panel.

AWS Glue for Spark and AWS Glue for Ray

In AWS Glue, you can use Python shell jobs to run native Python data integrations. These jobs run on a single Amazon EC2 instance and are limited by the capacity of that instance. This restricts the throughput of the data you can process, and becomes expensive to maintain when dealing with big data.

Converting semi-structured schemas to relational schemas with AWS Glue

It's common to want to convert semi-structured data into relational tables. Conceptually, you are flattening a hierarchical schema to a relational schema. AWS Glue can perform this conversion for you on-the-fly. Semi-structured data typically contains mark-up to identify entities within the data. It can have nested data structures with no fixed schema. 

AWS Glue uses crawlers to infer schemas for semi-structured data. It then transforms the data to a relational schema using an ETL (extract, transform, and load) job. For example, you might want to parse JSON data from Amazon Simple Storage Service (Amazon S3) source files to Amazon Relational Database Service (Amazon RDS) tables.

AWS Glue type systems

AWS Glue uses multiple type systems to provide a versatile interface over data systems that store data in very different ways. This document disambiguates AWS Glue type systems and data standards.

AWS Glue Data Catalog Types

The Data Catalog is a registry of tables and fields stored in various data systems, a metastore. When AWS Glue components, such as AWS Glue crawlers and AWS Glue with Spark jobs, write to the Data Catalog, they do so with an internal type system for tracking the types of fields. These values are shown in the Data type column of the table schema in the AWS Glue Console. This type system is based on Apache Hive's type system. 

Validation, compatibility and other uses

The Data Catalog does not validate types written to type fields. When AWS Glue components read and write to the Data Catalog, they will be compatible with each other. AWS Glue components also aim to preserve a high degree of compatibility with the Hive types. However, AWS Glue components do not guarantee compatibility with all Hive types. Since the Data Catalog does not validate types, other services may use the Data Catalog to track types using systems that strictly conform to the Hive type system, or any other system.

Types in AWS Glue with Spark scripts

When a AWS Glue with Spark script interprets or transforms a dataset, we provide DynamicFrame, an in-memory representation of your dataset as it is used in your script. The goal of a DynamicFrame is similar to that of the Spark DataFrame– it models your dataset so that Spark can schedule and execute transforms on your data. We guarantee that the type representation of DynamicFrame is intercompatible with DataFrame by providing the toDF and fromDF methods.

The Choice Type

DynamicFrames provide a mechanism for modeling fields in a dataset whose value may have inconsistent types on disk across rows. For instance, a field may hold a number stored as a string in certain rows, and an integer in others. This mechanism is an in-memory type called Choice. We provide transforms such as the ResolveChoice method, to resolve Choice columns to a concrete type. AWS Glue ETL will not write the Choice type to the Data Catalog in the normal course of operation; Choice types only exist in the context of DynamicFrame memory models of datasets. 

AWS Glue Crawler Types

Crawlers aim to produce a consistent, usable schema for your dataset, then store it in Data Catalog for use in other AWS Glue components and Athena. Crawlers deal with types as described in the previous section on the Data Catalog, AWS Glue Data Catalog Types. To produce a usable type in "Choice" type scenarios, where a column contains values of two or more types, Crawlers will create a struct type that models the potential types.

Overview of using AWS Glue

With AWS Glue, you store metadata in the AWS Glue Data Catalog. You use this metadata to orchestrate ETL jobs that transform data sources and load your data warehouse or data lake. The following steps describe the general workflow and some of the choices that you make when working with AWS Glue.

1) Populate the AWS Glue Data Catalog with table definitions.
In the console, for persistent data stores, you can add a crawler to populate the AWS Glue Data Catalog. You can start the Add crawler wizard from the list of tables or the list of crawlers. You choose one or more data stores for your crawler to access. You can also create a schedule to determine the frequency of running your crawler. For data streams, you can manually create the table definition, and define stream properties. Optionally, you can provide a custom classifier that infers the schema of your data. You can create custom classifiers using a grok pattern. However, AWS Glue provides built-in classifiers that are automatically used by crawlers if a custom classifier does not recognize your data. When you define a crawler, you don't have to select a classifier. 

Crawling some types of data stores requires a connection that provides authentication and location information. If needed, you can create a connection that provides this required information in the AWS Glue console. The crawler reads your data store and creates data definitions and named tables in the AWS Glue Data Catalog. These tables are organized into a database of your choosing. You can also populate the Data Catalog with manually created tables. With this method, you provide the schema and other metadata to create table definitions in the Data Catalog. 

2) Define a job that describes the transformation of data from source to target.

Generally, to create a job, you have to make the following choices:
● Choose a table from the AWS Glue Data Catalog to be the source of the job. Your job uses this table definition to access your data source and interpret the format of your data.
● Choose a table or location from the AWS Glue Data Catalog to be the target of the job. Your job uses this information to access your data store.
● Tell AWS Glue to generate a script to transform your source to target. AWS Glue generates the code to call built-in transforms to convert data from its source schema to target schema format. These transforms perform operations such as copy data, rename columns, and filter data to transform data as necessary. You can modify this script in the AWS Glue console.

3) Run your job to transform your data.
You can run your job on demand, or start it based on a one of these trigger types:
● A trigger that is based on a cron schedule.
● A trigger that is event-based; for example, the successful completion of another job can start an AWS Glue job.
● A trigger that starts a job on demand.

4) Monitor your scheduled crawlers and triggered jobs.
Use the AWS Glue console to view the following:
● Job run details and errors.
● Crawler run details and errors.
● Any notifications about AWS Glue activities

Review IAM permissions needed for the AWS Glue Studio user

To use AWS Glue Studio, the user must have access to various AWS resources. The user must be able to view and select Amazon S3 buckets, IAM policies and roles, and AWS Glue Data Catalog objects.

AWS Glue service permissions : AWS Glue Studio uses the actions and resources of the AWS Glue service. Your user needs permissions on these actions and resources to effectively use AWS Glue Studio. You can grant the AWS Glue Studio user the AWSGlueConsoleFullAccess managed policy, or create a custom policy with a smaller set of permissions.

Creating Custom IAM Policies for AWS Glue Studio

Accessing AWS Glue Studio APIs : To access AWS Glue Studio, add glue:UseGlueStudio in the actions policy list in the IAM permissions.
Notebook and data preview permissions : To ensure data previews and notebook commands work correctly, use a role that has a name that starts with the string AWSGlueServiceRole
Amazon CloudWatch permissions  : You can monitor your AWS Glue Studio jobs using Amazon CloudWatch, which collects and processes raw data from AWS Glue into readable, near-real-time metrics. By default, AWS Glue metrics data is sent to CloudWatch automatically. To access CloudWatch dashboards, the user accessing AWS Glue Studio needs one of the following: The AdministratorAccess policy The CloudWatchFullAccess policy

Review IAM permissions needed for ETL jobs

When you create a job using AWS Glue Studio, the job assumes the permissions of the IAM role that you specify when you create it. This IAM role must have permission to extract data from your data source, write data to your target, and access AWS Glue resources. The name of the role that you create for the job must start with the string AWSGlueServiceRole for it to be used correctly by AWS Glue Studio. For example, you might name your role AWSGlueServiceRole-FlightDataJob.

Data source and data target permissions

An AWS Glue Studio job must have access to Amazon S3 for any sources, targets, scripts, and temporary directories that you use in your job. 

Data sources require s3:ListBucket and s3:GetObject permissions.
Data targets require s3:ListBucket, s3:PutObject, and s3:DeleteObject permissions.

Your IAM policy needs to allow s3:GetObject for the specific buckets used for hosting AWS Glue transforms.
The following buckets are owned by the AWS service account and is worldwide readable. These buckets serve as a repository for the source code pertinent to a subset of transformations accessible via the AWS Glue Studio visual editor. Permissions on the bucket are set up to deny any other API action on the bucket. Anybody can read those scripts we provide for the transformations, but nobody outside our service team can "put" anything in them. When your AWS Glue job runs, that file is pulled in as a local import so the file is downloaded to the local container. After that, there is no further communication with that account.

ap-south-1: aws-glue-studio-transforms-584702181950-prod-ap-south-1
ap-south-2: aws-glue-studio-transforms-380279651983-prod-ap-south-2
many more in each region and AZ

If you choose Amazon Redshift as your data source, you can provide a role for cluster permissions. Jobs that run against a Amazon Redshift cluster issue commands that access Amazon S3 for temporary storage using temporary credentials. If your job runs for more than an hour, these credentials will expire causing the job to fail. To avoid this problem, you can assign a role to the Amazon Redshift cluster itself that grants the necessary permissions to jobs using temporary credentials.

If the job uses data sources or targets other than Amazon S3, then you must attach the necessary permissions to the IAM role used by the job to access these data sources and targets. 

If you're using connectors and connections for your data store, you need additional permissions, 

Permissions required for using connectors : If you're using an AWS Glue Custom Connector and connection to access a data store, the role used to run the AWS Glue ETL job needs additional permissions attached:

● The AWS managed policy AmazonEC2ContainerRegistryReadOnly for accessing connectors purchased from AWS Marketplace.
● The glue:GetJob and glue:GetJobs permissions.
● AWS Secrets Manager permissions for accessing secrets that are used with connections. Refer to Example: Permission to retrieve secret values for example IAM policies. 

Permissions required for deleting jobs : To perform this action, you must have the glue:BatchDeleteJob permission. This is different from the AWS Glue console, which requires the glue:DeleteJob permission for deleting jobs.

AWS Key Management Service permissions

If you plan to access Amazon S3 sources and targets that use server-side encryption with AWS Key Management Service (AWS KMS), then attach a policy to the AWS Glue Studio role used by the job that enables the job to decrypt the data. The job role needs the kms:ReEncrypt, kms:GenerateDataKey, and kms:DescribeKey permissions. Additionally, the job role needs the kms:Decrypt permission to upload or download an Amazon S3 object that is encrypted with an AWS KMS customer master key (CMK).

Configure a VPC for your ETL job

You can use Amazon Virtual Private Cloud (Amazon VPC) to define a virtual network in your own logically isolated area within the AWS Cloud, known as a virtual private cloud (VPC). You can launch your AWS resources, such as instances, into your VPC. Your VPC closely resembles a traditional network that you might operate in your own data center, with the benefits of using the scalable infrastructure of AWS. You can configure your VPC; you can select its IP address range, create subnets, and configure route tables, network gateways, and security settings. You can connect instances in your VPC to the internet. You can connect your VPC to your own corporate data center, making the AWS Cloud an extension of your data center. To protect the resources in each subnet, you can use multiple layers of security, including security groups and network access control lists. 

You can configure your AWS Glue ETL jobs to run within a VPC when using connectors. You must configure your VPC for the following, as needed:
● Public network access for data stores not in AWS. All data stores that are accessed by the job must be available from the VPC subnet.
● If your job needs to access both VPC resources and the public internet, the VPC needs to have a network address translation (NAT) gateway inside the VPC.

Getting started with notebooks in AWS Glue Studio

When you start a notebook through AWS Glue Studio, all the configuration steps are done for you so that you can explore your data and start developing your job script after only a few seconds. The following sections describe how to create a role and grant the appropriate permissions to use notebooks in AWS Glue Studio for ETL jobs.

Granting permissions for the IAM role

Setting up AWS Glue Studio is a pre-requisite to using notebooks. To use notebooks in AWS Glue, your role requires the following:
● A trust relationship with AWS Glue for the sts:AssumeRole action and, if you want tagging then sts:TagSession.
● An IAM policy containing all the permissions for notebooks, AWS Glue, and interactive sessions.
● An IAM policy for a pass role since the role needs to be able to pass itself from the notebook to interactive sessions.

For example, when you create a new role, you can add a standard AWS managed policy like AWSGlueConsoleFullAccessRole to the role, and then add a new policy for the notebook operations and another for the IAM PassRole policy.

Actions needed for a trust relationship with AWS Glue

When starting a notebook session, you must add the sts:AssumeRole to the trust relationship of the role that is passed to the notebook. If your session includes tags, you must also pass the sts:TagSession action. Without these actions, the notebook session cannot start.

Policies containing IAM permissions for notebooks

The following sample policy describes the required AWS IAM permissions for notebooks. If you are creating a new role, create a policy that contains the following:

        "glue:StartNotebook",
        "glue:TerminateNotebook",
        "glue:GlueNotebookRefreshCredentials",
        "glue:DeregisterDataPreview",
        "glue:GetNotebookInstanceStatus",
        "glue:GlueNotebookAuthorize"
		
IAM policy to pass a role

When you create a notebook with a role, that role is then passed to interactive sessions so that the same role can be used in both places. As such, the iam:PassRole permission needs to be part of the role's policy. Create a new policy for your role using the following example. Replace the account number with your own and the role name.

      "Effect": "Allow",
      "Action": "iam:PassRole",
      "Resource": "arn:aws:iam::111122223333:role/<role_name>"

Setting up AWS Glue usage profiles

However, with this ease of creating compute resources comes a risk of spiraling cloud costs when left unmanaged and without guardrails. As a result, admins need to balance avoiding high infrastructure costs while at the same time allowing users to work without unnecessary friction. With AWS Glue usage profiles, admins can create different profiles for various classes of users within the account, such as developers, testers, and product teams. Each profile is a unique set of parameters that can be assigned to different types of users. For example, developers may need more workers and can have a higher number of maximum workers while product teams may need fewer workers and a lower timeout or idle timeout value.

Suppose that a job is created by user A with profile A. The job is saved with certain parameter values. User B with profile B will try to run the job. When user A authored the job, if he didn’t set a specific number of workers, the default set in user A's profile was applied and was saved with the job's definitions. When user B runs the job, it run with whatever values were saved for it. If user B's own profile is more restrictive and not allowed to run with that many workers, the job run will fail.

Usage profile as a resource

An AWS Glue usage profile is a resource identified by an Amazon Resource Name (ARN). 

Admins should create usage profiles and then assign them to the various users. When creating a usage profile, you specify default values as well as a range of allowed values for various job and session parameters. You must configure at least one parameter for jobs or interactive sessions. You can customize the default value to be used when a parameter value is not provided for the job, and/or set up a range limit or a set of allowed values for validation if a user provides a parameter value when using this profile. 

Defaults are a best practice set by the admin to assist job authors. When a user creates a new job and doesn't set a timeout value, the usage profile's default timeout will apply. If the author doesn’t have a profile, then the AWS Glue service defaults would apply and be saved in the job's definition. At runtime, AWS Glue enforces the limits set in the profile (min, max, allowed workers).

Once a parameter is configured, all other parameters are optional. Parameters that can be customized for jobs or interactive sessions are:

Number of workers 
Worker type
Timeout 
Idle timeout

The admin can assign an AWS Glue usage profile to users/roles who create AWS Glue resources. Assigning a profile is a combination of two actions:

Usage profiles and jobs

While authoring jobs, the limits and defaults set in your usage profile will apply. Your profile will be assigned to the job upon save.

Running jobs with usage profiles

When you start a job run, AWS Glue enforces the limits set in your caller's profile. If there is no direct caller, Glue will then apply the limits from the profile assigned to the job by its author.
When a job is ran on a schedule (by AWS Glue workflows or AWS Glue triggers), the profile assigned to the job the author will apply.
When a job is ran by an external service (Step Functions, MWAA) or a StartJobRun API, the caller's profile limit will be enforced.

For AWS Glue workflows or AWS Glue triggers: pre-existing jobs need to be updated to save the new profile name so that profile's limits (min, max, and allowed workers) will be enforced at runtime for scheduled runs.

Viewing a usage profile assigned for jobs

To view the profile assigned to your jobs (that will be used at runtime with scheduled AWS Glue workflows or AWS Glue triggers), you may look at the job Details tab. You may also look at the profile used in past runs in the job runs details tab.

Getting started with the AWS Glue Data Catalog

The AWS Glue Data Catalog is your persistent technical metadata store. It is a managed service that you can use to store, annotate, and share metadata in the AWS Cloud. 

Step 1: Create a database
























● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 


































